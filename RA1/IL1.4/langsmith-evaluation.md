# Evaluaci√≥n de Sistemas RAG con LangSmith

## üìã Introducci√≥n

LangSmith es una plataforma de observabilidad y evaluaci√≥n desarrollada por LangChain para monitorear, debuggear y evaluar aplicaciones basadas en modelos de lenguaje. Es especialmente √∫til para sistemas RAG (Retrieval-Augmented Generation) donde necesitamos evaluar tanto la recuperaci√≥n de informaci√≥n como la generaci√≥n de respuestas.

## üéØ ¬øQu√© es LangSmith?

LangSmith proporciona:
- **Trazabilidad completa**: Seguimiento detallado de cada paso en tu pipeline RAG
- **Evaluaciones autom√°ticas**: M√©tricas predefinidas y personalizadas
- **Datasets de evaluaci√≥n**: Gesti√≥n de casos de prueba y ground truth
- **Debugging visual**: Interfaz para entender qu√© est√° pasando en cada paso
- **Comparaci√≥n de modelos**: An√°lisis lado a lado de diferentes configuraciones

## üöÄ Configuraci√≥n Inicial

### 1. Instalaci√≥n

```bash
pip install langsmith langchain
```

### 2. Configuraci√≥n de API Keys

```python
import os
from langsmith import Client

# Configurar variables de entorno
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "tu_api_key_aqui"
os.environ["LANGCHAIN_PROJECT"] = "rag-evaluation-project"

# Inicializar cliente
client = Client()
```

### 3. Obtener API Key

1. Ir a [smith.langchain.com](https://smith.langchain.com)
2. Crear cuenta o iniciar sesi√≥n
3. Navegar a Settings ‚Üí API Keys
4. Crear nueva API key
5. Copiar y guardar de forma segura

## üìä Configuraci√≥n B√°sica para RAG

### 1. Instrumentar tu Sistema RAG

```python
from langchain.schema import Document
from langchain.callbacks import LangChainTracer
from langsmith import traceable

@traceable(name="retrieval_step")
def retrieve_documents(query: str, top_k: int = 5):
    """Funci√≥n de recuperaci√≥n instrumentada"""
    # Tu l√≥gica de recuperaci√≥n aqu√≠
    documents = search_vector_db(query, top_k)
    
    # LangSmith autom√°ticamente captura inputs/outputs
    return documents

@traceable(name="generation_step") 
def generate_response(query: str, context: str):
    """Funci√≥n de generaci√≥n instrumentada"""
    prompt = f"Contexto: {context}\nPregunta: {query}\nRespuesta:"
    
    response = llm.invoke(prompt)
    return response

@traceable(name="rag_pipeline")
def rag_pipeline(query: str):
    """Pipeline RAG completo"""
    # Paso 1: Recuperaci√≥n
    documents = retrieve_documents(query)
    
    # Paso 2: Preparar contexto
    context = "\n".join([doc.page_content for doc in documents])
    
    # Paso 3: Generaci√≥n
    response = generate_response(query, context)
    
    return {
        "query": query,
        "retrieved_docs": documents,
        "context": context,
        "response": response
    }
```

### 2. Ejemplo con LangChain

```python
from langchain.chains import RetrievalQA
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.llms import OpenAI

# Configurar componentes
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_texts(texts, embeddings)
llm = OpenAI(temperature=0)

# Crear chain con trazabilidad autom√°tica
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(),
    return_source_documents=True
)

# Ejecutar con trazabilidad
result = qa_chain({"query": "¬øQu√© es la inteligencia artificial?"})
```

## üß™ Evaluaciones con LangSmith

### 1. Crear Dataset de Evaluaci√≥n

```python
from langsmith import Client

client = Client()

# Crear dataset
dataset_name = "rag-evaluation-dataset"
dataset = client.create_dataset(
    dataset_name=dataset_name,
    description="Dataset para evaluar sistema RAG"
)

# Agregar ejemplos al dataset
examples = [
    {
        "inputs": {"query": "¬øQu√© es la inteligencia artificial?"},
        "outputs": {"answer": "La IA es una rama de la inform√°tica..."},
        "metadata": {"category": "definiciones", "difficulty": "basic"}
    },
    {
        "inputs": {"query": "¬øC√≥mo funciona el machine learning?"},
        "outputs": {"answer": "El ML utiliza algoritmos que aprenden..."},
        "metadata": {"category": "conceptos", "difficulty": "intermediate"}
    }
]

for example in examples:
    client.create_example(
        dataset_id=dataset.id,
        inputs=example["inputs"],
        outputs=example["outputs"],
        metadata=example["metadata"]
    )
```

### 2. Ejecutar Evaluaciones

```python
from langsmith.evaluation import evaluate, LangChainStringEvaluator

# Definir evaluadores
evaluators = [
    # Evaluador de relevancia
    LangChainStringEvaluator(
        "labeled_score_string",
        config={
            "criteria": {
                "relevance": "¬øQu√© tan relevante es la respuesta para la pregunta?"
            },
            "normalize_by": 10
        }
    ),
    
    # Evaluador de fidelidad
    LangChainStringEvaluator(
        "labeled_score_string", 
        config={
            "criteria": {
                "faithfulness": "¬øLa respuesta est√° basada en el contexto proporcionado?"
            },
            "normalize_by": 10
        }
    )
]

# Ejecutar evaluaci√≥n
def rag_predict(inputs):
    """Funci√≥n que ser√° evaluada"""
    query = inputs["query"]
    result = rag_pipeline(query)
    return {"answer": result["response"]}

# Correr evaluaci√≥n
results = evaluate(
    rag_predict,
    data=dataset_name,
    evaluators=evaluators,
    experiment_prefix="rag-v1",
    metadata={"version": "1.0", "model": "gpt-4"}
)

print(f"Resultados: {results}")
```

### 3. Evaluadores Personalizados

```python
from langsmith.schemas import Run, Example

def custom_context_precision_evaluator(run: Run, example: Example) -> dict:
    """Evaluador personalizado para precisi√≥n del contexto"""
    
    # Obtener documentos recuperados de la traza
    retrieved_docs = []
    for child_run in run.child_runs:
        if child_run.name == "retrieval_step":
            retrieved_docs = child_run.outputs.get("documents", [])
    
    query = run.inputs["query"]
    
    # Evaluar relevancia de cada documento
    relevant_count = 0
    for doc in retrieved_docs:
        # L√≥gica para determinar si el documento es relevante
        relevance_score = evaluate_document_relevance(query, doc)
        if relevance_score > 0.7:
            relevant_count += 1
    
    precision = relevant_count / len(retrieved_docs) if retrieved_docs else 0
    
    return {
        "score": precision,
        "value": precision,
        "comment": f"Documentos relevantes: {relevant_count}/{len(retrieved_docs)}"
    }

# Usar evaluador personalizado
results = evaluate(
    rag_predict,
    data=dataset_name,
    evaluators=[custom_context_precision_evaluator],
    experiment_prefix="rag-context-precision"
)
```

## üìà M√©tricas Clave para RAG

### 1. M√©tricas de Recuperaci√≥n

```python
# Context Precision: ¬øLos documentos recuperados son relevantes?
def context_precision(retrieved_docs, query):
    relevant_docs = [doc for doc in retrieved_docs if is_relevant(doc, query)]
    return len(relevant_docs) / len(retrieved_docs)

# Context Recall: ¬øSe recuperaron todos los documentos relevantes?
def context_recall(retrieved_docs, all_relevant_docs):
    retrieved_relevant = set(retrieved_docs) & set(all_relevant_docs)
    return len(retrieved_relevant) / len(all_relevant_docs)
```

### 2. M√©tricas de Generaci√≥n

```python
# Faithfulness: ¬øLa respuesta es fiel al contexto?
def faithfulness_evaluator(context, response):
    prompt = f"""
    Contexto: {context}
    Respuesta: {response}
    
    ¬øLa respuesta est√° completamente basada en el contexto? (S√≠/No)
    """
    # Evaluar con LLM
    
# Relevance: ¬øLa respuesta es relevante para la pregunta?
def relevance_evaluator(query, response):
    prompt = f"""
    Pregunta: {query}
    Respuesta: {response}
    
    Califica la relevancia de 1-10:
    """
    # Evaluar con LLM
```

### 3. Configuraci√≥n de Evaluaci√≥n Completa

```python
def comprehensive_rag_evaluation():
    """Evaluaci√≥n completa del sistema RAG"""
    
    evaluators = [
        # M√©tricas autom√°ticas
        LangChainStringEvaluator("labeled_score_string", config={
            "criteria": {"relevance": "Relevancia de la respuesta"},
            "normalize_by": 10
        }),
        
        LangChainStringEvaluator("labeled_score_string", config={
            "criteria": {"faithfulness": "Fidelidad al contexto"},
            "normalize_by": 10
        }),
        
        LangChainStringEvaluator("labeled_score_string", config={
            "criteria": {"completeness": "Completitud de la respuesta"},
            "normalize_by": 10
        }),
        
        # Evaluadores personalizados
        custom_context_precision_evaluator,
        response_length_evaluator,
        semantic_similarity_evaluator
    ]
    
    results = evaluate(
        rag_predict,
        data="rag-comprehensive-dataset",
        evaluators=evaluators,
        experiment_prefix="rag-comprehensive",
        metadata={
            "model": "gpt-4",
            "retrieval_method": "semantic_search",
            "chunk_size": 500,
            "top_k": 5
        }
    )
    
    return results
```

## üîç An√°lisis de Resultados

### 1. Dashboard de LangSmith

Accede a [smith.langchain.com](https://smith.langchain.com) para ver:

- **Experiments**: Comparar diferentes configuraciones
- **Datasets**: Gestionar casos de prueba
- **Traces**: Explorar ejecuciones individuales
- **Analytics**: M√©tricas agregadas y tendencias

### 2. An√°lisis Program√°tico

```python
# Obtener resultados de experimento
experiment_results = client.list_runs(
    project_name="rag-evaluation-project",
    execution_order=1,
    is_root=True
)

# Analizar m√©tricas
scores = []
for run in experiment_results:
    if run.feedback_stats:
        scores.append(run.feedback_stats)

# Calcular estad√≠sticas
import numpy as np
avg_relevance = np.mean([s.get('relevance', 0) for s in scores])
avg_faithfulness = np.mean([s.get('faithfulness', 0) for s in scores])

print(f"Relevancia promedio: {avg_relevance:.2f}")
print(f"Fidelidad promedio: {avg_faithfulness:.2f}")
```

### 3. Exportar Datos

```python
# Exportar resultados para an√°lisis externo
import pandas as pd

def export_evaluation_results(project_name, output_file):
    """Exporta resultados de evaluaci√≥n a CSV"""
    
    runs = client.list_runs(project_name=project_name)
    
    data = []
    for run in runs:
        data.append({
            'run_id': run.id,
            'query': run.inputs.get('query', ''),
            'response': run.outputs.get('answer', ''),
            'latency': run.latency,
            'total_tokens': run.total_tokens,
            'feedback': run.feedback_stats
        })
    
    df = pd.DataFrame(data)
    df.to_csv(output_file, index=False)
    print(f"Resultados exportados a {output_file}")

export_evaluation_results("rag-evaluation-project", "rag_results.csv")
```

## üîß Mejores Pr√°cticas

### 1. Dise√±o de Datasets

- **Diversidad**: Incluir diferentes tipos de consultas
- **Ground Truth**: Respuestas de referencia bien definidas
- **Metadatos**: Categor√≠as, dificultad, dominio
- **Tama√±o**: Balance entre cobertura y tiempo de evaluaci√≥n

### 2. Configuraci√≥n de Evaluadores

- **M√∫ltiples M√©tricas**: No depender de una sola m√©trica
- **Evaluadores Humanos**: Combinar con evaluaci√≥n autom√°tica
- **Contexto Espec√≠fico**: Adaptar criterios al dominio
- **Calibraci√≥n**: Validar evaluadores con ejemplos conocidos

### 3. Monitoreo Continuo

```python
# Configurar alertas autom√°ticas
def setup_monitoring():
    """Configurar monitoreo continuo"""
    
    # Evaluaci√≥n en producci√≥n
    @traceable(name="production_rag")
    def production_rag_with_monitoring(query):
        result = rag_pipeline(query)
        
        # Evaluaci√≥n r√°pida en l√≠nea
        quick_score = quick_relevance_check(query, result["response"])
        
        # Log si la puntuaci√≥n es baja
        if quick_score < 0.5:
            print(f"‚ö†Ô∏è  Baja puntuaci√≥n detectada: {quick_score}")
        
        return result
    
    return production_rag_with_monitoring
```

### 4. Versionado y Experimentos

```python
# Configuraci√≥n de experimentos A/B
experiment_configs = {
    "baseline": {
        "chunk_size": 500,
        "top_k": 3,
        "model": "gpt-3.5-turbo"
    },
    "optimized": {
        "chunk_size": 300,
        "top_k": 5,
        "model": "gpt-4"
    }
}

for config_name, config in experiment_configs.items():
    results = evaluate(
        lambda inputs: rag_predict_with_config(inputs, config),
        data="standard-dataset",
        evaluators=standard_evaluators,
        experiment_prefix=f"rag-{config_name}",
        metadata=config
    )
```

## üéì Casos de Uso Avanzados

### 1. Evaluaci√≥n Multi-Modal

```python
# Para RAG que maneja texto e im√°genes
def multimodal_rag_evaluator(run, example):
    """Evaluador para RAG multi-modal"""
    
    query = run.inputs["query"]
    response = run.outputs["answer"]
    images = run.inputs.get("images", [])
    
    # Evaluar coherencia entre texto e im√°genes
    coherence_score = evaluate_text_image_coherence(response, images)
    
    return {"score": coherence_score}
```

### 2. Evaluaci√≥n de Dominio Espec√≠fico

```python
# Para RAG m√©dico, legal, t√©cnico, etc.
def domain_specific_evaluator(domain):
    """Crear evaluador espec√≠fico del dominio"""
    
    def evaluator(run, example):
        response = run.outputs["answer"]
        
        # Verificar terminolog√≠a espec√≠fica del dominio
        terminology_score = check_domain_terminology(response, domain)
        
        # Verificar precisi√≥n factual
        factual_score = verify_domain_facts(response, domain)
        
        return {
            "terminology_score": terminology_score,
            "factual_accuracy": factual_score
        }
    
    return evaluator
```

## üìö Recursos Adicionales

### Documentaci√≥n Oficial
- [LangSmith Documentation](https://docs.smith.langchain.com/)
- [LangSmith Python SDK](https://python.langchain.com/docs/langsmith/)
- [Evaluation Guide](https://docs.smith.langchain.com/evaluation)

### Ejemplos de C√≥digo
- [LangSmith Cookbook](https://github.com/langchain-ai/langsmith-cookbook)
- [RAG Evaluation Examples](https://github.com/langchain-ai/rag-evaluation-examples)

### Comunidad
- [LangChain Discord](https://discord.gg/langchain)
- [GitHub Discussions](https://github.com/langchain-ai/langchain/discussions)

## üéØ Conclusi√≥n

LangSmith proporciona una plataforma robusta para evaluar sistemas RAG de manera sistem√°tica y escalable. La combinaci√≥n de trazabilidad autom√°tica, evaluadores flexibles y an√°lisis visual permite optimizar continuamente el rendimiento de tus aplicaciones de IA.

**Pr√≥ximos pasos recomendados:**
1. Configurar LangSmith en tu proyecto RAG actual
2. Crear un dataset de evaluaci√≥n representativo
3. Implementar evaluadores b√°sicos (relevancia, fidelidad)
4. Establecer un pipeline de evaluaci√≥n continua
5. Iterar y optimizar bas√°ndose en los resultados