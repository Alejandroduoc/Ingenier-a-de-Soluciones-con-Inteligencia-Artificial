import os
import sys
import time
import pickle
from typing import List, Dict, Any
import streamlit as st
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.schema import HumanMessage, SystemMessage, AIMessage
from langchain.memory import ConversationBufferMemory
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document

class ChatbotEvaluacion:
    def __init__(self, documentos: List[str] = None):
        # ConfiguraciÃ³n del modelo con streaming
        self.llm = ChatOpenAI(
            base_url="https://models.github.ai/inference",  #descubri que aqui  cambia de url base  
            api_key=os.getenv("GITHUB_TOKEN"),     
            model="openai/gpt-4o-mini",
            temperature=0.7,
            streaming=True
        )
        
        # Modelo de embeddings
        self.embeddings = OpenAIEmbeddings(
            base_url="https://models.github.ai/inference",
            api_key=os.getenv("GITHUB_TOKEN")
        )
        
        # Sistema de memoria
        self.memory = ConversationBufferMemory(
            return_messages=True,
            memory_key="chat_history"
        )
        
        # Base de datos vectorial
        self.vectorstore = None
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        
        # Inicializar con documentos si se proporcionan
        if documentos:
            self.inicializar_rag(documentos)
    
    def inicializar_rag(self, documentos: List[str]):
        """Inicializa el sistema RAG con los documentos proporcionados"""
        st.info("ğŸ“š Inicializando sistema RAG...")
        
        # Crear documentos
        docs = []
        for i, texto in enumerate(documentos):
            docs.append(Document(
                page_content=texto,
                metadata={"source": f"doc_{i}", "page": 1}
            ))
        
        # Dividir en chunks
        chunks = self.text_splitter.split_documents(docs)
        st.success(f"âœ“ Divididos {len(chunks)} chunks")
        
        # Generar embeddings y crear base vectorial
        self.vectorstore = FAISS.from_documents(chunks, self.embeddings)
        st.success("âœ“ Base de datos vectorial FAISS creada")
        
        # Guardar la base de datos
        self.vectorstore.save_local("faiss_index")
        st.success("âœ“ Base de datos guardada en 'faiss_index'")
    
    def cargar_rag(self, ruta: str = "faiss_index"):
        """Carga una base de datos vectorial existente"""
        if os.path.exists(ruta):
            self.vectorstore = FAISS.load_local(ruta, self.embeddings)
            st.success("âœ“ Base de datos vectorial cargada")
            return True
        return False
    
    def buscar_contexto(self, pregunta: str, k: int = 3) -> str:
        """Busca contexto relevante en la base de datos vectorial"""
        if not self.vectorstore:
            return "Sistema RAG no inicializado. Por favor, carga documentos primero."
        
        docs = self.vectorstore.similarity_search(pregunta, k=k)
        contexto = "\n\n".join([doc.page_content for doc in docs])
        return f"Contexto relevante:\n{contexto}"
    
    def generar_respuesta_con_streaming(self, pregunta: str, mostrar_contexto: bool = False):
        """Genera respuesta con streaming integrando RAG y memoria"""
        
        # Buscar contexto relevante
        contexto = self.buscar_contexto(pregunta) if self.vectorstore else ""
        
        # Obtener historial de conversaciÃ³n
        historial = self.memory.load_memory_variables({})["chat_history"]
        
        # Construir mensajes del sistema
        system_prompt = """Eres un asistente educativo especializado en evaluaciones. 
        Responde basÃ¡ndote en el contexto proporcionado y mantÃ©n un tono profesional.
        Si la informaciÃ³n no estÃ¡ en el contexto, indica claramente que no tienes esa informaciÃ³n."""
        
        messages = [SystemMessage(content=system_prompt)]
        
        # Agregar historial de conversaciÃ³n
        for msg in historial:
            messages.append(msg)
        
        # Agregar contexto y pregunta actual
        if contexto and mostrar_contexto:
            messages.append(SystemMessage(content=contexto))
        
        messages.append(HumanMessage(content=pregunta))
        
        # Generar respuesta con streaming
        respuesta_completa = ""
        try:
            # Crear un placeholder para la respuesta en streaming
            respuesta_placeholder = st.empty()
            respuesta_texto = "ğŸ¤– **Asistente:** "
            
            for chunk in self.llm.stream(messages):
                contenido = chunk.content
                respuesta_texto += contenido
                respuesta_placeholder.markdown(respuesta_texto + "â–Œ")
                respuesta_completa += contenido
                time.sleep(0.02)
            
            # Mostrar respuesta final sin el cursor
            respuesta_placeholder.markdown(respuesta_texto)
            
            # Guardar en memoria
            self.memory.save_context(
                {"input": pregunta},
                {"output": respuesta_completa}
            )
            
            return respuesta_completa
            
        except Exception as e:
            error_msg = f"âŒ Error en la generaciÃ³n: {e}"
            st.error(error_msg)
            return error_msg
    
    def evaluar_respuesta(self, pregunta: str, respuesta_usuario: str) -> Dict[str, Any]:
        """EvalÃºa una respuesta del usuario comparÃ¡ndola con el contexto"""
        if not self.vectorstore:
            return {"error": "Sistema RAG no inicializado"}
        
        # Buscar contexto relevante
        contexto = self.buscar_contexto(pregunta)
        
        prompt_evaluacion = f'''
        EvalÃºa la siguiente respuesta del estudiante:
        
        Pregunta: {pregunta}
        Respuesta del estudiante: {respuesta_usuario}
        
        Contexto de referencia: {contexto}
        
        Proporciona una evaluaciÃ³n con:
        1. PuntuaciÃ³n (0-10)
        2. Puntos fuertes
        3. Ãreas de mejora
        4. ExplicaciÃ³n breve
        '''
        
        try:
            respuesta = self.llm.invoke([HumanMessage(content=prompt_evaluacion)])
            return {
                "puntuacion": 0,  # Se extraerÃ­a del anÃ¡lisis
                "feedback": respuesta.content,
                "contexto_utilizado": contexto[:500] + "..." if len(contexto) > 500 else contexto
            }
        except Exception as e:
            return {"error": f"Error en evaluaciÃ³n: {e}"}
    
    def mostrar_estadisticas(self):
        """Muestra estadÃ­sticas del chatbot"""
        historial = self.memory.load_memory_variables({})["chat_history"]
        st.subheader("ğŸ“Š EstadÃ­sticas del Chatbot")
        st.write(f"ğŸ’¬ **Mensajes en memoria:** {len(historial)}")
        st.write(f"ğŸ“š **Base vectorial:** {'âœ“ Cargada' if self.vectorstore else 'âœ— No disponible'}")
        if self.vectorstore:
            st.write(f"ğŸ“– **Documentos indexados:** {self.vectorstore.index.ntotal}")

# FUNCIONES DE UTILIDAD PARA LA EVALUACIÃ“N

def inicializar_documentos_ejemplo() -> List[str]:
    """Proporciona documentos de ejemplo para la evaluaciÃ³n"""
    documentos = [
        """
        La inteligencia artificial (IA) es la simulaciÃ³n de procesos de inteligencia humana por mÃ¡quinas, 
        especialmente sistemas informÃ¡ticos. Estos procesos incluyen el aprendizaje, el razonamiento 
        y la autocorrecciÃ³n.
        """,
        """
        El machine learning es un subconjunto de la IA que se centra en el desarrollo de algoritmos 
        que permiten a las computadoras aprender y hacer predicciones basadas en datos.
        """,
        """
        LangChain es un framework para desarrollar aplicaciones con modelos de lenguaje. 
        Proporciona herramientas para la gestiÃ³n de memoria, cadenas de procesamiento 
        y integraciÃ³n con bases de datos vectoriales.
        """,
        """
        Los embeddings son representaciones vectoriales de texto que capturan el significado semÃ¡ntico. 
        Se utilizan en bÃºsqueda semÃ¡ntica y sistemas de recomendaciÃ³n.
        """,
        """
        FAISS (Facebook AI Similarity Search) es una biblioteca para bÃºsqueda eficiente de similitudes 
        en vectores de alta dimensiÃ³n. Es ideal para sistemas de recuperaciÃ³n de informaciÃ³n.
        """
    ]
    return documentos

def main():
    st.set_page_config(
        page_title="Chatbot de EvaluaciÃ³n Educativa",
        page_icon="ğŸ¤–",
        layout="wide"
    )
    
    st.title("ğŸ¤– Chatbot de EvaluaciÃ³n Educativa")
    st.markdown("Sistema RAG con memoria y evaluaciÃ³n de respuestas")
    
    # Inicializar el chatbot en session state
    if 'chatbot' not in st.session_state:
        st.session_state.chatbot = ChatbotEvaluacion()
    
    # Sidebar para configuraciÃ³n
    with st.sidebar:
        st.header("ConfiguraciÃ³n")
        
        # OpciÃ³n para cargar documentos
        if st.button("ğŸ“š Cargar documentos de ejemplo"):
            documentos = inicializar_documentos_ejemplo()
            st.session_state.chatbot.inicializar_rag(documentos)
        
        # OpciÃ³n para cargar base existente
        if st.button("ğŸ” Cargar base de datos existente"):
            if st.session_state.chatbot.cargar_rag():
                st.success("Base de datos cargada exitosamente")
            else:
                st.error("No se encontrÃ³ base de datos existente")
        
        # Mostrar estadÃ­sticas
        if st.button("ğŸ“Š Mostrar estadÃ­sticas"):
            st.session_state.chatbot.mostrar_estadisticas()
        
        # Limpiar memoria
        if st.button("ğŸ—‘ï¸ Limpiar memoria"):
            st.session_state.chatbot.memory.clear()
            st.success("Memoria limpiada")
        
        st.markdown("---")
        st.markdown("### Modos de uso")
        modo = st.radio("Selecciona el modo:", 
                       ["ğŸ’¬ Chat", "ğŸ“ EvaluaciÃ³n"])
    
    # Contenido principal segÃºn el modo seleccionado
    if modo == "ğŸ’¬ Chat":
        st.header("ğŸ’¬ Modo Chat")
        
        # Mostrar historial de chat
        if hasattr(st.session_state.chatbot, 'memory'):
            historial = st.session_state.chatbot.memory.load_memory_variables({})["chat_history"]
            if historial:
                with st.expander("ğŸ“œ Historial de conversaciÃ³n"):
                    for i, msg in enumerate(historial):
                        if isinstance(msg, HumanMessage):
                            st.write(f"**ğŸ§‘ Usuario:** {msg.content}")
                        elif isinstance(msg, AIMessage):
                            st.write(f"**ğŸ¤– Asistente:** {msg.content}")
        
        # Input del usuario
        pregunta = st.text_area("Escribe tu pregunta:", placeholder="Â¿En quÃ© puedo ayudarte?")
        
        col1, col2 = st.columns([1, 5])
        with col1:
            mostrar_contexto = st.checkbox("Mostrar contexto", value=False)
        with col2:
            if st.button("ğŸš€ Enviar pregunta", type="primary"):
                if pregunta.strip():
                    with st.spinner("Generando respuesta..."):
                        st.session_state.chatbot.generar_respuesta_con_streaming(
                            pregunta, 
                            mostrar_contexto=mostrar_contexto
                        )
                else:
                    st.warning("Por favor, escribe una pregunta")
    
    elif modo == "ğŸ“ EvaluaciÃ³n":
        st.header("ğŸ“ Modo EvaluaciÃ³n")
        
        st.markdown("EvalÃºa una respuesta del estudiante comparÃ¡ndola con el contexto disponible.")
        
        col1, col2 = st.columns(2)
        
        with col1:
            pregunta_eval = st.text_area(
                "Pregunta de evaluaciÃ³n:",
                placeholder="Ej: Â¿QuÃ© es la inteligencia artificial?",
                height=100
            )
        
        with col2:
            respuesta_estudiante = st.text_area(
                "Respuesta del estudiante:",
                placeholder="Ej: La IA es cuando las mÃ¡quinas piensan como humanos",
                height=100
            )
        
        if st.button("ğŸ“‹ Evaluar respuesta", type="primary"):
            if pregunta_eval.strip() and respuesta_estudiante.strip():
                with st.spinner("Evaluando respuesta..."):
                    evaluacion = st.session_state.chatbot.evaluar_respuesta(
                        pregunta_eval, 
                        respuesta_estudiante
                    )
                    
                    if "error" not in evaluacion:
                        st.subheader("Resultado de la evaluaciÃ³n")
                        st.markdown(evaluacion["feedback"])
                        
                        with st.expander("ğŸ“š Contexto utilizado"):
                            st.text(evaluacion["contexto_utilizado"])
                    else:
                        st.error(evaluacion["error"])
            else:
                st.warning("Por favor, completa ambos campos")

    # Footer
    st.markdown("---")
    st.markdown("*Sistema de evaluaciÃ³n educativa con RAG y memoria de conversaciÃ³n*")

if __name__ == "__main__":
    # Verificar que la API key estÃ© configurada
    if not os.getenv("GITHUB_TOKEN"):
        st.error("âš ï¸ Por favor, configura la variable de entorno GITHUB_TOKEN")
        st.info("""
        Para usar esta aplicaciÃ³n, necesitas:
        1. Obtener un token de GitHub con permisos adecuados
        2. Configurarlo como variable de entorno:
           ```bash
           export GITHUB_TOKEN="tu_token_aqui"
           ```
        """)
    else:
        main()