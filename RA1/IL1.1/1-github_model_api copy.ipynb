{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9019b973",
   "metadata": {},
   "source": [
    "## Primera Llamada al Modelo\n",
    "\n",
    "En este ejercicio, aprenderemos a realizar nuestra primera llamada a un modelo de lenguaje usando GitHub Models API.\n",
    "\n",
    "# 1. GitHub Models API - Conexión Directa con OpenAI Client\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Configurar una conexión directa con GitHub Models usando el cliente OpenAI\n",
    "- Comprender los parámetros básicos de configuración de API\n",
    "- Implementar llamadas básicas a modelos de lenguaje\n",
    "- Aplicar mejores prácticas de seguridad con API keys\n",
    "\n",
    "## Introducción\n",
    "GitHub Models proporciona acceso a varios modelos de lenguaje mediante una API compatible con OpenAI. En este notebook aprenderemos a:\n",
    "1. Configurar el entorno y las credenciales\n",
    "2. Establecer una conexión con la API\n",
    "3. Realizar llamadas básicas al modelo\n",
    "4. Explorar diferentes parámetros de configuración\n",
    "\n",
    "## Configuración de Variables de Entorno\n",
    "\n",
    "Antes de ejecutar el código, asegúrate de tener configuradas las siguientes variables de entorno:\n",
    "\n",
    "```bash\n",
    "export GITHUB_BASE_URL=\"https://models.inference.ai.azure.com\"\n",
    "export GITHUB_TOKEN=\"tu_token_de_github_aqui\"\n",
    "```\n",
    "\n",
    "**Mejores Prácticas de Seguridad:**\n",
    "- Nunca hardcodees API keys en el código\n",
    "- Usa variables de entorno o archivos .env\n",
    "- No compartas credenciales en repositorios públicos\n",
    "- Rota las API keys regularmente\n",
    "\n",
    "## Instalación de Dependencias\n",
    "```bash\n",
    "pip install openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab37f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI library version: 1.100.0\n",
      "Python version: 3.13.3 (tags/v3.13.3:6280bb5, Apr  8 2025, 14:47:33) [MSC v.1943 64 bit (AMD64)]\n",
      "Base URL configurada: https://models.inference.ai.azure.com\n",
      "API Key configurada: ✓\n",
      "API Key preview: ghp_CjH5Uv...51og\n"
     ]
    }
   ],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Verificar que tenemos las bibliotecas correctas\n",
    "print(\"OpenAI library version:\", __import__('openai').__version__)\n",
    "print(\"Python version:\", __import__('sys').version)\n",
    "\n",
    "# Configuración del cliente OpenAI para GitHub Models\n",
    "try:\n",
    "    # Configurar el cliente con variables de entorno\n",
    "    client = OpenAI(\n",
    "        base_url=os.environ.get(\"GITHUB_BASE_URL\"),\n",
    "        api_key=os.environ.get(\"GITHUB_TOKEN\")\n",
    "    )\n",
    "    \n",
    "    # Verificar configuración (sin mostrar la API key completa por seguridad)\n",
    "    print(\"Base URL configurada:\", client.base_url)\n",
    "    print(\"API Key configurada:\", \"✓\" if client.api_key else \"✗\")\n",
    "    \n",
    "    if client.api_key:\n",
    "        print(\"API Key preview:\", client.api_key[:10] + \"...\" + client.api_key[-4:])\n",
    "    else:\n",
    "        print(\"⚠️  API Key no encontrada. Asegúrate de configurar GITHUB_TOKEN\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error en configuración: {e}\")\n",
    "    print(\"Verifica que las variables de entorno estén configuradas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a49b487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Respuesta del Modelo ===\n",
      "¡Hola! Estoy aquí y listo para ayudarte.\n",
      "\n",
      "=== Información Técnica ===\n",
      "Modelo usado: gpt-4o-2024-11-20\n",
      "Tokens usados: 30\n",
      "Tokens de entrada: 19\n",
      "Tokens de salida: 11\n",
      "que tenemos en response: ChatCompletion(id='chatcmpl-C60wl4v8uINHB1ujVfQoCXXMUqXge', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='¡Hola! Estoy aquí y listo para ayudarte.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1755549643, model='gpt-4o-2024-11-20', object='chat.completion', service_tier=None, system_fingerprint='fp_ee1d74bde0', usage=CompletionUsage(completion_tokens=11, prompt_tokens=19, total_tokens=30, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n"
     ]
    }
   ],
   "source": [
    "# Primera llamada básica al modelo\n",
    "def llamada_basica():\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": \"Hola, ¿cómo estás? Responde en una oración.\"}\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_tokens=150\n",
    "        )\n",
    "        \n",
    "        print(\"=== Respuesta del Modelo ===\")\n",
    "        print(response.choices[0].message.content)\n",
    "        print(\"\\n=== Información Técnica ===\")\n",
    "        print(f\"Modelo usado: {response.model}\")\n",
    "        print(f\"Tokens usados: {response.usage.total_tokens}\")\n",
    "        print(f\"Tokens de entrada: {response.usage.prompt_tokens}\")\n",
    "        print(f\"Tokens de salida: {response.usage.completion_tokens}\")\n",
    "        print (f\"que tenemos en response: {response}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error en la llamada: {e}\")\n",
    "        print(\"Verifica tu configuración y conexión a internet\")\n",
    "\n",
    "# Ejecutar la función\n",
    "llamada_basica()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cccf4a",
   "metadata": {},
   "source": [
    "## Usando Roles del Sistema\n",
    "\n",
    "El rol \"system\" permite establecer el comportamiento y contexto del asistente antes de la conversación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j1b57r1oeo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Respuesta con Mensaje de Sistema ===\n",
      "¡Claro! Vamos a desglosarlo de forma simple y con un ejemplo práctico.\n",
      "\n",
      "### ¿Qué significa API?\n",
      "API son las siglas de **Application Programming Interface** (Interfaz de Programación de Aplicaciones). En palabras sencillas, una API es como un **traductor o puente** que permite que dos aplicaciones, programas o sistemas se comuniquen entre sí y compartan información o funciones.\n",
      "\n",
      "### ¿Cómo funciona una API?\n",
      "Imagina que estás en un restaurante. Tú eres el cliente (la aplicación 1) y quieres pedir comida de la cocina (la aplicación 2). En este caso, el **mesero** actúa como la API:\n",
      "1. Tú le dices al mesero lo que quieres (haces una solicitud).\n",
      "2. El mesero lleva tu pedido a la cocina y traduce tu solicitud para que los chefs la entiendan.\n",
      "3. La cocina prepara tu comida y se la da al mesero.\n",
      "4. Finalmente, el mesero te trae\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo con mensaje de sistema\n",
    "def usar_mensaje_sistema():\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"Eres un experto en tecnología que explica conceptos complejos de manera simple y amigable. Siempre incluyes ejemplos prácticos.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": \"¿Qué es una IA?\"\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        \n",
    "        print(\"=== Respuesta con Mensaje de Sistema ===\")\n",
    "        print(response.choices[0].message.content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Ejecutar función\n",
    "usar_mensaje_sistema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3a290e",
   "metadata": {},
   "source": [
    "## Explorando Parámetros de Configuración\n",
    "\n",
    "Los parámetros más importantes al hacer llamadas a LLMs son:\n",
    "\n",
    "- **temperature**: Controla la creatividad (0.0 = determinístico, 1.0 = muy creativo)\n",
    "- **max_tokens**: Límite de tokens en la respuesta\n",
    "- **model**: El modelo específico a usar (gpt-4o, gpt-3.5-turbo, etc.)\n",
    "- **messages**: Array de mensajes con roles (system, user, assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "j48bg48xqs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TEMPERATURE: 0.1\n",
      "==================================================\n",
      "En un pequeño taller lleno de herramientas y cables, un robot llamado K1-T0 despertó a la vida. Su creador, un inventor apasionado, le había dado una misión: aprender a cocinar para ayudar en la cocina del vecindario.\n",
      "\n",
      "Al principio, K1-T0 era torpe. Confundía la sal con el azúcar y cortaba las zanahorias en formas extrañas. Pero cada error era una lección. Observaba tutoriales, leía libros de recetas\n",
      "\n",
      "Tokens usados: 121\n",
      "\n",
      "==================================================\n",
      "TEMPERATURE: 0.5\n",
      "==================================================\n",
      "En un pequeño taller de una ciudad futurista, un robot llamado C1-B0, diseñado para tareas de limpieza, encontró un viejo libro de recetas olvidado en una esquina polvorienta. Intrigado por las coloridas imágenes de platos, decidió intentarlo.\n",
      "\n",
      "Al principio, C1-B0 confundió la sal con el azúcar y quemó más de un sartén, pero su curiosidad era inagotable. Día tras día, ajustaba sus algoritmos, perfeccionando sus\n",
      "\n",
      "Tokens usados: 121\n",
      "\n",
      "==================================================\n",
      "TEMPERATURE: 0.9\n",
      "==================================================\n",
      "En un pequeño taller lleno de herramientas y cables sueltos, un robot llamado K-3PO fue activado por primera vez. Su creador, el inventor Arturo, le había dado una tarea sencilla: aprender a cocinar. K-3PO, equipado con sensores de última generación pero sin experiencia culinaria, comenzó su travesía.\n",
      "\n",
      "El robot pasó sus primeras horas estudiando recetas digitales, midiendo ingredientes con precisión milimétrica y observando tutoriales de chefs famosos. Sin embargo,\n",
      "\n",
      "Tokens usados: 121\n"
     ]
    }
   ],
   "source": [
    "# Comparando diferentes valores de temperature\n",
    "def comparar_temperature():\n",
    "    prompt = \"Escribe una historia muy corta sobre un robot que aprende a cocinar.\"\n",
    "    \n",
    "    temperatures = [0.1, 0.5, 0.9]\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"TEMPERATURE: {temp}\")\n",
    "        print('='*50)\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temp,\n",
    "                max_tokens=100\n",
    "            )\n",
    "            \n",
    "            print(response.choices[0].message.content)\n",
    "            print(f\"\\nTokens usados: {response.usage.total_tokens}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "# Ejecutar comparación\n",
    "comparar_temperature()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6fc272",
   "metadata": {},
   "source": [
    "## Ejercicios Prácticos\n",
    "\n",
    "### Ejercicio 1: Experimentar con Diferentes Modelos\n",
    "Modifica el código para probar diferentes modelos disponibles (si tienes acceso):\n",
    "- gpt-4o\n",
    "- gpt-4o-mini\n",
    "- DeepSeek-R1-0528\n",
    "\n",
    "Revisa todos los modelos diponibles en la [documentación de Github Marketplace](https://github.com/marketplace?type=models)\n",
    "\n",
    "### Ejercicio 2: Crear un Asistente Especializado\n",
    "Diseña un mensaje de sistema para crear un asistente especializado en un tema específico (ejemplo: finanzas, salud, educación).\n",
    "\n",
    "### Ejercicio 3: Optimización de Tokens\n",
    "Experimenta con diferentes valores de max_tokens para encontrar el equilibrio entre respuesta completa y eficiencia de costos.\n",
    "\n",
    "## Conceptos Clave\n",
    "\n",
    "1. **Configuración segura** de APIs usando variables de entorno\n",
    "2. **Parámetros básicos** para controlar el comportamiento del modelo\n",
    "3. **Manejo de errores** en llamadas a APIs\n",
    "4. **Roles de mensajes** (system, user, assistant)\n",
    "5. **Monitoreo de uso** de tokens y costos\n",
    "\n",
    "## Próximos Pasos\n",
    "\n",
    "En el siguiente notebook exploraremos cómo LangChain simplifica y abstrae estas operaciones, proporcionando herramientas más poderosas para el desarrollo de aplicaciones con LLMs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
