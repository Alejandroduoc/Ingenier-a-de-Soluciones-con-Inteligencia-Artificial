{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hfn0f7nyumn",
   "metadata": {},
   "source": [
    "# 3. LangChain Streaming - Respuestas en Tiempo Real\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Comprender qu√© es el streaming y cu√°ndo usarlo\n",
    "- Implementar streaming con LangChain\n",
    "- Manejar chunks de datos en tiempo real\n",
    "- Construir interfaces de usuario reactivas\n",
    "\n",
    "## ¬øQu√© es el Streaming?\n",
    "\n",
    "El streaming permite recibir la respuesta del modelo **token por token** conforme se genera, en lugar de esperar a que termine completamente. Esto mejora significativamente la experiencia de usuario en aplicaciones interactivas.\n",
    "\n",
    "### Ventajas del Streaming:\n",
    "- **Percepci√≥n de velocidad**: El usuario ve progreso inmediato\n",
    "- **Mejor UX**: Interfaces m√°s reactivas e interactivas  \n",
    "- **Engagement**: Mantiene la atenci√≥n del usuario\n",
    "- **Debugging**: Permite ver el proceso de generaci√≥n\n",
    "\n",
    "### Casos de Uso Ideales:\n",
    "- Chatbots y asistentes conversacionales\n",
    "- Generaci√≥n de contenido largo\n",
    "- Aplicaciones web interactivas\n",
    "- Demostraciones en vivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa694f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas importadas correctamente para streaming\n"
     ]
    }
   ],
   "source": [
    "# Importar bibliotecas necesarias\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"Bibliotecas importadas correctamente para streaming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f037cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Modelo configurado con streaming habilitado\n",
      "Modelo: gpt-4o\n",
      "Streaming: True\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n del modelo con streaming habilitado\n",
    "try:\n",
    "    llm = ChatOpenAI(\n",
    "        base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "        api_key=os.getenv(\"GITHUB_TOKEN\"),\n",
    "        model=\"gpt-4o\",\n",
    "        streaming=True,  # ¬°Importante: habilitar streaming!\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Modelo configurado con streaming habilitado\")\n",
    "    print(f\"Modelo: {llm.model_name}\")\n",
    "    print(f\"Streaming: {llm.streaming}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error en configuraci√≥n: {e}\")\n",
    "    print(\"Verifica las variables de entorno\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da0d27",
   "metadata": {},
   "source": [
    "## Streaming B√°sico\n",
    "\n",
    "El m√©todo `.stream()` devuelve un generador que produce chunks de texto conforme se generan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f31f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STREAMING EN TIEMPO REAL ===\n",
      "Generando respuesta...\n",
      "--------------------------------------------------\n",
      "Hab√≠a una vez un programador llamado Mart√≠n, un verdadero apasionado de las l√≠neas de c√≥digo y las noches interminables frente a la pantalla. Trabajaba como desarrollador en una peque√±a empresa de tecnolog√≠a, pero en secreto so√±aba con crear algo √∫nico, algo que cambiara el mundo. Su vida transcurr√≠a entre teclados, tazas de caf√© y un sinf√≠n de errores de compilaci√≥n.\n",
      "\n",
      "Una noche particularmente sombr√≠a, mientras trabajaba solo en su apartamento, Mart√≠n tropez√≥ con un archivo antiguo en su computadora. El archivo ten√≠a un nombre cr√≠ptico: **\"arcano.py\"**. No recordaba haberlo visto antes, pero, intrigado, decidi√≥ abrirlo. Lo que encontr√≥ dentro era un c√≥digo extra√±o, con s√≠mbolos y palabras que no correspond√≠an a ning√∫n lenguaje de programaci√≥n que conociera. A pesar de que las funciones y m√©todos parec√≠an familiares, hab√≠a algo‚Ä¶ distinto. Algo que parec√≠a vivo.\n",
      "\n",
      "Movido por la curiosidad, Mart√≠n ejecut√≥ el archivo. La pantalla parpade√≥, y de repente, las luces del apartamento titilaron. Una suave brisa, imposible en un espacio cerrado, recorri√≥ la habitaci√≥n. En su monitor apareci√≥ un mensaje en letras doradas:\n",
      "\n",
      "**\"Bienvenido, te has convertido en el tejedor del c√≥digo arcano. Todo lo que escribas aqu√≠ cobrar√° vida. √ösalo con sabidur√≠a.\"**\n",
      "\n",
      "Mart√≠n pens√≥ que era alguna clase de broma o un virus sofisticado, pero su coraz√≥n lat√≠a con fuerza. Decidi√≥ probarlo. Escribi√≥ una l√≠nea simple:\n",
      "\n",
      "```python\n",
      "crear_objeto(\"taza_flotante\")\n",
      "```\n",
      "\n",
      "Para su asombro, una peque√±a taza de caf√© apareci√≥ flotando frente a √©l, girando lentamente en el aire. La dej√≥ caer en su mano, tibia y real. No pod√≠a creerlo. ¬øEra esto magia? ¬øTecnolog√≠a avanzada? ¬øUn sue√±o?\n",
      "\n",
      "Durante las semanas siguientes, Mart√≠n experiment√≥ con el c√≥digo. Descubri√≥ que pod√≠a crear objetos, alterar el clima en su apartamento e incluso sanar una cortada en su dedo con unas pocas l√≠neas cuidadosamente redactadas. Pero lo m√°s asombroso fue cuando intent√≥ escribir una funci√≥n que le permitiera entender a su gato, Copo. De repente, Copo comenz√≥ a hablarle, con una voz grave y sabia que desentonaba completamente con su apariencia esponjosa.\n",
      "\n",
      "El poder era indescriptible, pero tambi√©n aterrador. Mart√≠n pronto se dio cuenta de que cada l√≠nea de c√≥digo que escrib√≠a ten√≠a consecuencias impredecibles. Un d√≠a, cuando intent√≥ generar dinero para pagar sus deudas, el banco local sufri√≥ un inexplicable error en sus sistemas. Otro d√≠a, cuando escribi√≥ un programa para detener la lluvia, desencaden√≥ una sequ√≠a en toda la ciudad.\n",
      "\n",
      "Mart√≠n entendi√≥ que el c√≥digo arcano no era un juguete; era una herramienta que requer√≠a responsabilidad. As√≠ que decidi√≥ usarlo para ayudar al mundo, pero de manera cuidadosa. Cre√≥ peque√±os programas que reparaban carreteras, limpiaban los r√≠os y curaban enfermedades a nivel local. Todo lo hac√≠a en secreto, dejando que otros se llevaran el cr√©dito, porque sab√≠a que el poder absoluto nunca deb√≠a estar en manos de una sola persona.\n",
      "\n",
      "Con el tiempo, Mart√≠n se convirti√≥ en una figura misteriosa en la comunidad tecnol√≥gica. Nadie sab√≠a de d√≥nde ven√≠an sus soluciones milagrosas, pero todos lo admiraban. √âl, por su parte, guardaba el archivo \"arcano.py\" en un rinc√≥n oculto de su computadora, sabiendo que, aunque hab√≠a descubierto la magia en el c√≥digo, su mayor logro ser√≠a usarla con bondad y sabidur√≠a.\n",
      "\n",
      "Y as√≠, Mart√≠n se convirti√≥ no solo en un programador, sino en un mago del mundo moderno, tejedor de l√≠neas que cambiaban la realidad.\n",
      "--------------------------------------------------\n",
      "‚úì Streaming completado\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo b√°sico de streaming\n",
    "def streaming_basico():\n",
    "    prompt = \"Cu√©ntame una historia corta sobre un programador que descubre la magia en el c√≥digo\"\n",
    "    \n",
    "    print(\"=== STREAMING EN TIEMPO REAL ===\")\n",
    "    print(\"Generando respuesta...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # stream() devuelve un generador de chunks\n",
    "        for chunk in llm.stream([HumanMessage(content=prompt)]):\n",
    "            # Imprimir cada chunk sin nueva l√≠nea\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "            time.sleep(0.01)  # Peque√±a pausa para simular streaming visual\n",
    "            \n",
    "        print(\"\\n\" + \"-\" * 50)\n",
    "        print(\"‚úì Streaming completado\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error en streaming: {e}\")\n",
    "\n",
    "# Ejecutar streaming b√°sico\n",
    "streaming_basico()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6214408",
   "metadata": {},
   "source": [
    "## Comparaci√≥n: Streaming vs No-Streaming\n",
    "\n",
    "Veamos la diferencia en experiencia de usuario entre ambos enfoques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8e5874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPARACI√ìN: STREAMING vs NO-STREAMING ===\\n\n",
      "1. SIN STREAMING:\n",
      "--------------------\n",
      "Esperando respuesta completa...\n",
      "\\n[Respuesta recibida despu√©s de 3.72 segundos]\n",
      "Python es un lenguaje de programaci√≥n ampliamente reconocido por su simplicidad y versatilidad, lo que lo convierte en una opci√≥n ideal tanto para principiantes como para desarrolladores experimentados. Una de sus principales ventajas es su sintaxis clara y legible, que permite escribir y entender c√≥digo de manera m√°s eficiente, reduciendo la curva de aprendizaje y facilitando la colaboraci√≥n en proyectos. Adem√°s, cuenta con una extensa biblioteca est√°ndar y una enorme cantidad de paquetes externos que abarcan √°reas como an√°lisis de datos, inteligencia artificial, desarrollo web, automatizaci√≥n y m√°s, lo que ahorra tiempo y esfuerzo en el desarrollo de soluciones. Python es tambi√©n un lenguaje multiplataforma, lo que significa que los programas escritos en √©l pueden ejecutarse en diversos sistemas operativos sin modificaciones significativas. Su comunidad activa y en constante crecimiento proporciona un valioso soporte, actualizaciones frecuentes y recursos educativos que enriquecen a√∫n m√°s la experiencia de aprendizaje y desarrollo. Estas caracter√≠sticas hacen de Python una herramienta poderosa y flexible para abordar una amplia variedad de proyectos y desaf√≠os tecnol√≥gicos.\n",
      "\\n============================================================\\n\n",
      "2. CON STREAMING:\n",
      "------------------\n",
      "Respuesta en tiempo real:\n",
      "Python es un lenguaje de programaci√≥n altamente valorado por su versatilidad, simplicidad y comunidad activa, lo que lo convierte en una herramienta poderosa tanto para principiantes como para expertos. Su sintaxis clara y legible facilita el aprendizaje y la escritura de c√≥digo, permitiendo a los desarrolladores centrarse en resolver problemas en lugar de luchar con la complejidad del lenguaje. Adem√°s, Python cuenta con una vasta biblioteca est√°ndar y una amplia gama de paquetes y frameworks de terceros que abarcan √°reas como ciencia de datos, inteligencia artificial, desarrollo web, automatizaci√≥n y m√°s, lo que ahorra tiempo y esfuerzo en la implementaci√≥n de soluciones. Su naturaleza multiplataforma permite que el c√≥digo escrito en Python se ejecute en diversos sistemas operativos sin modificaciones significativas. Por √∫ltimo, la comunidad global de Python proporciona soporte constante, recursos educativos y actualizaciones regulares, asegurando que el lenguaje siga evolucionando y adapt√°ndose a las necesidades del mundo tecnol√≥gico.\\n\\n[Streaming completado en 6.78 segundos]\n",
      "\\n============================================================\n",
      "OBSERVACIONES:\n",
      "- Sin streaming: El usuario espera sin feedback\n",
      "- Con streaming: El usuario ve progreso inmediato\n",
      "- Mejor percepci√≥n de velocidad con streaming\n",
      "- Streaming es especial para respuestas largas\n"
     ]
    }
   ],
   "source": [
    "# Comparaci√≥n entre streaming y no-streaming\n",
    "def comparar_streaming():\n",
    "    # Modelo sin streaming\n",
    "    llm_no_stream = ChatOpenAI(\n",
    "        base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "        api_key=os.getenv(\"GITHUB_TOKEN\"),\n",
    "        model=\"gpt-4o\",\n",
    "        streaming=False,  # Sin streaming\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    prompt = \"Escribe un p√°rrafo sobre las ventajas de la programaci√≥n en Python\"\n",
    "    \n",
    "    print(\"=== COMPARACI√ìN: STREAMING vs NO-STREAMING ===\\\\n\")\n",
    "    \n",
    "    # 1. Sin streaming\n",
    "    print(\"1. SIN STREAMING:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(\"Esperando respuesta completa...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = llm_no_stream.invoke([HumanMessage(content=prompt)])\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"\\\\n[Respuesta recibida despu√©s de {end_time - start_time:.2f} segundos]\")\n",
    "        print(response.content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60 + \"\\\\n\")\n",
    "    \n",
    "    # 2. Con streaming\n",
    "    print(\"2. CON STREAMING:\")\n",
    "    print(\"-\" * 18)\n",
    "    print(\"Respuesta en tiempo real:\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        for chunk in llm.stream([HumanMessage(content=prompt)]):\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "            time.sleep(0.03)  # Simular pausa para efecto visual\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"\\\\n\\\\n[Streaming completado en {end_time - start_time:.2f} segundos]\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"OBSERVACIONES:\")\n",
    "    print(\"- Sin streaming: El usuario espera sin feedback\")\n",
    "    print(\"- Con streaming: El usuario ve progreso inmediato\")\n",
    "    print(\"- Mejor percepci√≥n de velocidad con streaming\")\n",
    "    print(\"- Streaming es especial para respuestas largas\")\n",
    "\n",
    "# Ejecutar comparaci√≥n\n",
    "comparar_streaming()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725ad79d",
   "metadata": {},
   "source": [
    "## Implementaci√≥n de un Chatbot Simple con Streaming\n",
    "\n",
    "Creemos un chatbot b√°sico que demuestre el streaming en un contexto pr√°ctico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231dac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a05a03cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHATBOT CON STREAMING ===\n",
      "Escribe 'salir' para terminar la conversaci√≥n\\n\n",
      "\\nüëã ¬°Hasta luego!\n",
      "\\n¬°Gracias por usar el chatbot!\n",
      "üí° Descomenta la l√≠nea anterior para probar el chatbot interactivo\n"
     ]
    }
   ],
   "source": [
    "# Chatbot simple con streaming\n",
    "def chatbot_streaming():\n",
    "    print(\"=== CHATBOT CON STREAMING ===\")\n",
    "    print(\"Escribe 'salir' para terminar la conversaci√≥n\\\\n\")\n",
    "    \n",
    "    # Configurar asistente con personalidad\n",
    "    system_message = \"\"\"Eres un asistente √∫til y amigable especializado en tecnolog√≠a. \n",
    "    Respondes de manera clara y concisa, y siempre intentas ser educativo.\"\"\"\n",
    "    \n",
    "    while True:\n",
    "        # Obtener input del usuario\n",
    "        user_input = input(\"\\\\nüßë T√∫: \")\n",
    "        \n",
    "        if user_input.lower() in ['salir', 'exit', 'quit']:\n",
    "            print(\"\\\\nüëã ¬°Hasta luego!\")\n",
    "            break\n",
    "            \n",
    "        if not user_input.strip():\n",
    "            continue\n",
    "            \n",
    "        print(\"\\\\nü§ñ Asistente: \", end=\"\", flush=True)\n",
    "        \n",
    "        try:\n",
    "            # Streaming de la respuesta\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_input}\n",
    "            ]\n",
    "            \n",
    "            # Convertir a formato LangChain\n",
    "            from langchain.schema import SystemMessage\n",
    "            lc_messages = [\n",
    "                SystemMessage(content=system_message),\n",
    "                HumanMessage(content=user_input)\n",
    "            ]\n",
    "            \n",
    "            full_response = \"\"\n",
    "            for chunk in llm.stream(lc_messages):\n",
    "                content = chunk.content\n",
    "                print(content, end=\"\", flush=True)\n",
    "                full_response += content\n",
    "                time.sleep(0.02)\n",
    "                \n",
    "            print()  # Nueva l√≠nea al final\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\\\n\\\\n‚è∏Ô∏è Interrumpido por el usuario\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\\\n‚ùå Error: {e}\")\n",
    "            \n",
    "    print(\"\\\\n¬°Gracias por usar el chatbot!\")\n",
    "\n",
    "# Ejecutar chatbot (¬°Pru√©balo!)\n",
    "chatbot_streaming() \n",
    " # Descomenta esta l√≠nea para ejecutar\n",
    "\n",
    "print(\"üí° Descomenta la l√≠nea anterior para probar el chatbot interactivo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6060e43c",
   "metadata": {},
   "source": [
    "## Streaming Avanzado con Manejo de Chunks\n",
    "\n",
    "Podemos procesar cada chunk individualmente para crear experiencias m√°s sofisticadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f86fa1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STREAMING AVANZADO CON AN√ÅLISIS ===\n",
      "Analizando chunks conforme llegan...\n",
      "\n",
      "¬°Claro! Vamos a explicar estos\\n[Progreso: 10 chunks, ~8 palabras]\\n\n",
      " conceptos de manera clara y sencilla.\n",
      "\n",
      "---\n",
      "\n",
      "### **\\n[Progreso: 20 chunks, ~18 palabras]\\n\n",
      "¬øQu√© es la Inteligencia Artificial (IA)?\\n[Progreso: 30 chunks, ~28 palabras]\\n\n",
      "**\n",
      "\n",
      "La **Inteligencia Artificial (IA)**\\n[Progreso: 40 chunks, ~38 palabras]\\n\n",
      " es una rama de la inform√°tica que busca crear sistemas\\n[Progreso: 50 chunks, ~48 palabras]\\n\n",
      " o m√°quinas capaces de realizar tareas que normalmente requieren inteligencia\\n[Progreso: 60 chunks, ~58 palabras]\\n\n",
      " humana. Estas tareas incluyen cosas como:\n",
      "\n",
      "- Recon\\n[Progreso: 70 chunks, ~68 palabras]\\n\n",
      "ocer patrones.\n",
      "- Resolver problemas.\n",
      "- Tomar\\n[Progreso: 80 chunks, ~78 palabras]\\n\n",
      " decisiones.\n",
      "- Aprender de la experiencia.\n",
      "-\\n[Progreso: 90 chunks, ~88 palabras]\\n\n",
      " Entender y procesar lenguajes humanos.\n",
      "\n",
      "\\n[Progreso: 100 chunks, ~98 palabras]\\n\n",
      "En resumen, la IA intenta imitar la forma\\n[Progreso: 110 chunks, ~108 palabras]\\n\n",
      " en que los humanos pensamos y actuamos, permit\\n[Progreso: 120 chunks, ~118 palabras]\\n\n",
      "iendo que las m√°quinas realicen tareas de manera m√°s\\n[Progreso: 130 chunks, ~128 palabras]\\n\n",
      " \"inteligente\".\n",
      "\n",
      "Hay diferentes tipos de IA\\n[Progreso: 140 chunks, ~138 palabras]\\n\n",
      ":\n",
      "1. **IA d√©bil o espec√≠fica**\\n[Progreso: 150 chunks, ~148 palabras]\\n\n",
      ": Dise√±ada para realizar tareas concretas,\\n[Progreso: 160 chunks, ~158 palabras]\\n\n",
      " como asistentes virtuales (por ejemplo, Siri o\\n[Progreso: 170 chunks, ~168 palabras]\\n\n",
      " Alexa) o sistemas de recomendaci√≥n como los que\\n[Progreso: 180 chunks, ~178 palabras]\\n\n",
      " usa Netflix.\n",
      "2. **IA fuerte o general\\n[Progreso: 190 chunks, ~188 palabras]\\n\n",
      "**: Ser√≠a una inteligencia similar a la humana\\n[Progreso: 200 chunks, ~198 palabras]\\n\n",
      ", capaz de realizar cualquier tarea intelectual. Este tipo\\n[Progreso: 210 chunks, ~208 palabras]\\n\n",
      " de IA a√∫n no se ha desarrollado por completo.\n",
      "\n",
      "\\n[Progreso: 220 chunks, ~218 palabras]\\n\n",
      "---\n",
      "\n",
      "### **¬øQu√© es el Machine Learning (\\n[Progreso: 230 chunks, ~228 palabras]\\n\n",
      "ML)?**\n",
      "\n",
      "El **Machine Learning (ML)**\\n[Progreso: 240 chunks, ~238 palabras]\\n\n",
      ", o aprendizaje autom√°tico, es una subrama de\\n[Progreso: 250 chunks, ~248 palabras]\\n\n",
      " la inteligencia artificial. Se centra en desarrollar algoritmos\\n[Progreso: 260 chunks, ~258 palabras]\\n\n",
      " o modelos que permiten a los sistemas aprender autom√°ticamente a\\n[Progreso: 270 chunks, ~268 palabras]\\n\n",
      " partir de datos **sin ser programados expl√≠citamente\\n[Progreso: 280 chunks, ~278 palabras]\\n\n",
      "** para realizar cada tarea.\n",
      "\n",
      "En lugar de escribir\\n[Progreso: 290 chunks, ~288 palabras]\\n\n",
      " reglas espec√≠ficas para una tarea, en el ML se\\n[Progreso: 300 chunks, ~298 palabras]\\n\n",
      " entrena a un modelo con datos para que apr\\n[Progreso: 310 chunks, ~308 palabras]\\n\n",
      "enda patrones y realice predicciones o clasificaciones\\n[Progreso: 320 chunks, ~318 palabras]\\n\n",
      ". \n",
      "\n",
      "Por ejemplo:\n",
      "- Una app de fotos\\n[Progreso: 330 chunks, ~327 palabras]\\n\n",
      " puede aprender a identificar gatos al entrenarse con miles\\n[Progreso: 340 chunks, ~337 palabras]\\n\n",
      " de im√°genes etiquetadas como \"gato\"\\n[Progreso: 350 chunks, ~347 palabras]\\n\n",
      " y \"no gato\".\n",
      "- Un sistema de dete\\n[Progreso: 360 chunks, ~357 palabras]\\n\n",
      "cci√≥n de fraudes bancarios puede aprender a identificar\\n[Progreso: 370 chunks, ~367 palabras]\\n\n",
      " transacciones sospechosas analizando datos hist√≥ricos de\\n[Progreso: 380 chunks, ~377 palabras]\\n\n",
      " actividades fraudulentas.\n",
      "\n",
      "---\n",
      "\n",
      "### **¬øC√≥mo funciona\\n[Progreso: 390 chunks, ~387 palabras]\\n\n",
      " el Machine Learning?**\n",
      "\n",
      "El ML se basa en\\n[Progreso: 400 chunks, ~397 palabras]\\n\n",
      " **datos** y en **algoritmos matem\\n[Progreso: 410 chunks, ~407 palabras]\\n\n",
      "√°ticos**. El proceso general funciona as√≠:\n",
      "\n",
      "1\\n[Progreso: 420 chunks, ~417 palabras]\\n\n",
      ". **Recolecci√≥n de datos**:\n",
      "  \\n[Progreso: 430 chunks, ~426 palabras]\\n\n",
      " - El sistema necesita datos para aprender. Estos datos\\n[Progreso: 440 chunks, ~436 palabras]\\n\n",
      " pueden ser n√∫meros, im√°genes, texto, sonidos,\\n[Progreso: 450 chunks, ~446 palabras]\\n\n",
      " etc.\n",
      "\n",
      "2. **Entrenamiento del modelo\\n[Progreso: 460 chunks, ~456 palabras]\\n\n",
      "**:\n",
      "   - Se alimenta al modelo con\\n[Progreso: 470 chunks, ~465 palabras]\\n\n",
      " datos etiquetados (en el caso del aprendizaje\\n[Progreso: 480 chunks, ~475 palabras]\\n\n",
      " supervisado) o sin etiquetas (en el caso\\n[Progreso: 490 chunks, ~485 palabras]\\n\n",
      " del aprendizaje no supervisado). Aqu√≠, el modelo\\n[Progreso: 500 chunks, ~495 palabras]\\n\n",
      " busca patrones o asociaciones en los datos.\n",
      "   -\\n[Progreso: 510 chunks, ~504 palabras]\\n\n",
      " Por ejemplo, en un sistema de reconocimiento facial,\\n[Progreso: 520 chunks, ~514 palabras]\\n\n",
      " se entrenar√° el modelo con muchas fotos que indiqu\\n[Progreso: 530 chunks, ~524 palabras]\\n\n",
      "en qu√© rostro pertenece a qui√©n.\n",
      "\n",
      "3. **\\n[Progreso: 540 chunks, ~534 palabras]\\n\n",
      "Ajuste y optimizaci√≥n**:\n",
      "   -\\n[Progreso: 550 chunks, ~543 palabras]\\n\n",
      " El modelo ajusta sus par√°metros internos (como pesos\\n[Progreso: 560 chunks, ~553 palabras]\\n\n",
      " en las redes neuronales) para minimizar los errores\\n[Progreso: 570 chunks, ~563 palabras]\\n\n",
      " en sus predicciones. Esto se hace mediante m√©todos\\n[Progreso: 580 chunks, ~573 palabras]\\n\n",
      " como el **gradiente descendente**.\n",
      "\n",
      "4\\n[Progreso: 590 chunks, ~583 palabras]\\n\n",
      ". **Evaluaci√≥n**:\n",
      "   - Despu√©s de\\n[Progreso: 600 chunks, ~592 palabras]\\n\n",
      " entrenar, el modelo se eval√∫a con datos\\n[Progreso: 610 chunks, ~602 palabras]\\n\n",
      " nuevos (no vistos durante el entrenamiento) para verificar\\n[Progreso: 620 chunks, ~612 palabras]\\n\n",
      " qu√© tan bien funciona.\n",
      "\n",
      "5. **Predicci√≥n\\n[Progreso: 630 chunks, ~622 palabras]\\n\n",
      " o inferencia**:\n",
      "   - Una vez entren\\n[Progreso: 640 chunks, ~631 palabras]\\n\n",
      "ado y afinado, el modelo est√° listo para\\n[Progreso: 650 chunks, ~641 palabras]\\n\n",
      " hacer predicciones o tomar decisiones basadas en nuevos\\n[Progreso: 660 chunks, ~651 palabras]\\n\n",
      " datos.\n",
      "\n",
      "---\n",
      "\n",
      "### **Tipos de Machine Learning**\n",
      "\n",
      "\\n[Progreso: 670 chunks, ~661 palabras]\\n\n",
      "Existen tres principales enfoques de ML:\n",
      "\n",
      "1\\n[Progreso: 680 chunks, ~671 palabras]\\n\n",
      ". **Aprendizaje supervisado**:\n",
      "  \\n[Progreso: 690 chunks, ~680 palabras]\\n\n",
      " - El modelo se entrena con datos etiquet\\n[Progreso: 700 chunks, ~690 palabras]\\n\n",
      "ados (cada dato de entrada tiene un resultado esperado\\n[Progreso: 710 chunks, ~700 palabras]\\n\n",
      ").\n",
      "   - Ejemplo: Entrenar un modelo\\n[Progreso: 720 chunks, ~709 palabras]\\n\n",
      " con im√°genes de perros y gatos etiquetadas para\\n[Progreso: 730 chunks, ~719 palabras]\\n\n",
      " que pueda identificar si una nueva imagen es de un\\n[Progreso: 740 chunks, ~729 palabras]\\n\n",
      " perro o un gato.\n",
      "\n",
      "2. **Aprendiz\\n[Progreso: 750 chunks, ~739 palabras]\\n\n",
      "aje no supervisado**:\n",
      "   - No hay\\n[Progreso: 760 chunks, ~748 palabras]\\n\n",
      " etiquetas en los datos, y el modelo trata de\\n[Progreso: 770 chunks, ~758 palabras]\\n\n",
      " encontrar patrones o estructuras ocultas.\n",
      "   - Ej\\n[Progreso: 780 chunks, ~767 palabras]\\n\n",
      "emplo: Agrupar clientes con caracter√≠sticas similares en un\\n[Progreso: 790 chunks, ~777 palabras]\\n\n",
      " an√°lisis de mercado.\n",
      "\n",
      "3. **Aprendizaje\\n[Progreso: 800 chunks, ~787 palabras]\\n\n",
      " por refuerzo**:\n",
      "   - El modelo aprende\\n[Progreso: 810 chunks, ~796 palabras]\\n\n",
      " a trav√©s de prueba y error, recibiendo\\n[Progreso: 820 chunks, ~806 palabras]\\n\n",
      " recompensas o castigos seg√∫n sus acciones.\n",
      "  \\n[Progreso: 830 chunks, ~815 palabras]\\n\n",
      " - Ejemplo: Entrenar a un robot para\\n[Progreso: 840 chunks, ~825 palabras]\\n\n",
      " jugar un videojuego aprendiendo qu√© acciones lo acerc\\n[Progreso: 850 chunks, ~835 palabras]\\n\n",
      "an a ganar.\n",
      "\n",
      "---\n",
      "\n",
      "En resumen, la **\\n[Progreso: 860 chunks, ~845 palabras]\\n\n",
      "IA** es el concepto general de m√°quinas que sim\\n[Progreso: 870 chunks, ~855 palabras]\\n\n",
      "ulan inteligencia, mientras que el **Machine Learning**\\n[Progreso: 880 chunks, ~865 palabras]\\n\n",
      " es una t√©cnica espec√≠fica dentro de la IA que permite\\n[Progreso: 890 chunks, ~875 palabras]\\n\n",
      " a las m√°quinas"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mn‚úó Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Ejecutar streaming avanzado\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[43mstreaming_avanzado\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mstreaming_avanzado\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# Imprimir el contenido\u001b[39;00m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mprint\u001b[39m(content, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.02\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pausa ligeramente m√°s larga para ver el an√°lisis\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Estad√≠sticas finales\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mn\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mn=== ESTAD√çSTICAS FINALES ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Streaming con an√°lisis de chunks\n",
    "def streaming_avanzado():\n",
    "    prompt = \"Explica qu√© es la inteligencia artificial y c√≥mo funciona el machine learning\"\n",
    "    \n",
    "    print(\"=== STREAMING AVANZADO CON AN√ÅLISIS ===\")\n",
    "    print(\"Analizando chunks conforme llegan...\\n\")\n",
    "    \n",
    "    # Variables para estad√≠sticas\n",
    "    chunk_count = 0\n",
    "    total_content = \"\"\n",
    "    words_processed = 0\n",
    "    \n",
    "    try:\n",
    "        for chunk in llm.stream([HumanMessage(content=prompt)]):\n",
    "            chunk_count += 1\n",
    "            content = chunk.content\n",
    "            total_content += content\n",
    "            \n",
    "            # Contar palabras aproximadas\n",
    "            if content.strip():\n",
    "                words_in_chunk = len(content.split())\n",
    "                words_processed += words_in_chunk\n",
    "            \n",
    "            # Mostrar progreso cada 10 chunks\n",
    "            if chunk_count % 10 == 0:\n",
    "                print(f\"\\\\n[Progreso: {chunk_count} chunks, ~{words_processed} palabras]\\\\n\")\n",
    "            \n",
    "            # Imprimir el contenido\n",
    "            print(content, end=\"\", flush=True)\n",
    "            time.sleep(0.02)  # Pausa ligeramente m√°s larga para ver el an√°lisis\n",
    "        \n",
    "        # Estad√≠sticas finales\n",
    "        print(f\"\\\\n\\\\n=== ESTAD√çSTICAS FINALES ===\")\n",
    "        print(f\"Total de chunks: {chunk_count}\")\n",
    "        print(f\"Palabras aproximadas: {words_processed}\")\n",
    "        print(f\"Caracteres totales: {len(total_content)}\")\n",
    "        print(f\"Promedio chars/chunk: {len(total_content)/chunk_count if chunk_count > 0 else 0:.1f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\\\n‚úó Error: {e}\")\n",
    "\n",
    "# Ejecutar streaming avanzado\n",
    "streaming_avanzado()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac251a9d",
   "metadata": {},
   "source": [
    "## Consideraciones T√©cnicas del Streaming\n",
    "\n",
    "### Cu√°ndo Usar Streaming:\n",
    "‚úÖ **S√ç usar streaming:**\n",
    "- Respuestas largas (>100 tokens)\n",
    "- Aplicaciones interactivas\n",
    "- Chatbots y asistentes\n",
    "- Demostraciones en vivo\n",
    "- Cuando la UX es prioritaria\n",
    "\n",
    "‚ùå **NO usar streaming:**\n",
    "- Respuestas muy cortas\n",
    "- Procesamiento batch\n",
    "- APIs de backend sin interfaz\n",
    "- Cuando necesitas la respuesta completa antes de procesar\n",
    "\n",
    "### Mejores Pr√°cticas:\n",
    "1. **Manejo de errores**: Siempre incluye try/catch\n",
    "2. **Indicadores visuales**: Muestra progreso al usuario\n",
    "3. **Cancelaci√≥n**: Permite al usuario interrumpir\n",
    "4. **Buffer management**: Para interfaces web, considera buffering\n",
    "5. **Performance**: Monitorea el uso de recursos\n",
    "\n",
    "## Ejercicios Pr√°cticos\n",
    "\n",
    "### Ejercicio 1: Indicador de Progreso\n",
    "Modifica el c√≥digo para mostrar un indicador de progreso (spinner, barra, porcentaje).\n",
    "\n",
    "### Ejercicio 2: Streaming con Filtros\n",
    "Implementa streaming que filtre o procese chunks espec√≠ficos (ej: resaltar palabras clave).\n",
    "\n",
    "### Ejercicio 3: Chatbot Mejorado\n",
    "Extiende el chatbot con:\n",
    "- Historial de conversaci√≥n\n",
    "- Comandos especiales (/help, /clear)\n",
    "- Diferentes personalidades\n",
    "\n",
    "## Conceptos Clave Aprendidos\n",
    "\n",
    "1. **Streaming** mejora la percepci√≥n de velocidad\n",
    "2. **Chunks** se procesan individualmente en tiempo real\n",
    "3. **UX** es significativamente mejor con streaming\n",
    "4. **Implementaci√≥n** requiere manejo cuidadoso de generadores\n",
    "5. **Casos de uso** espec√≠ficos donde streaming aporta valor\n",
    "\n",
    "## Pr√≥ximos Pasos\n",
    "\n",
    "En el siguiente notebook exploraremos la **memoria en LangChain**, que nos permite mantener contexto entre m√∫ltiples interacciones del usuario."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
