{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hfn0f7nyumn",
   "metadata": {},
   "source": [
    "# 3. LangChain Streaming - Respuestas en Tiempo Real\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Comprender qué es el streaming y cuándo usarlo\n",
    "- Implementar streaming con LangChain\n",
    "- Manejar chunks de datos en tiempo real\n",
    "- Construir interfaces de usuario reactivas\n",
    "\n",
    "## ¿Qué es el Streaming?\n",
    "\n",
    "El streaming permite recibir la respuesta del modelo **token por token** conforme se genera, en lugar de esperar a que termine completamente. Esto mejora significativamente la experiencia de usuario en aplicaciones interactivas.\n",
    "\n",
    "### Ventajas del Streaming:\n",
    "- **Percepción de velocidad**: El usuario ve progreso inmediato\n",
    "- **Mejor UX**: Interfaces más reactivas e interactivas  \n",
    "- **Engagement**: Mantiene la atención del usuario\n",
    "- **Debugging**: Permite ver el proceso de generación\n",
    "\n",
    "### Casos de Uso Ideales:\n",
    "- Chatbots y asistentes conversacionales\n",
    "- Generación de contenido largo\n",
    "- Aplicaciones web interactivas\n",
    "- Demostraciones en vivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa694f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas importadas correctamente para streaming\n"
     ]
    }
   ],
   "source": [
    "# Importar bibliotecas necesarias\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"Bibliotecas importadas correctamente para streaming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f037cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Modelo configurado con streaming habilitado\n",
      "Modelo: gpt-4o\n",
      "Streaming: True\n"
     ]
    }
   ],
   "source": [
    "# Configuración del modelo con streaming habilitado\n",
    "try:\n",
    "    llm = ChatOpenAI(\n",
    "        base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "        api_key=os.getenv(\"GITHUB_TOKEN\"),\n",
    "        model=\"gpt-4o\",\n",
    "        streaming=True,  # ¡Importante: habilitar streaming!\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Modelo configurado con streaming habilitado\")\n",
    "    print(f\"Modelo: {llm.model_name}\")\n",
    "    print(f\"Streaming: {llm.streaming}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error en configuración: {e}\")\n",
    "    print(\"Verifica las variables de entorno\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da0d27",
   "metadata": {},
   "source": [
    "## Streaming Básico\n",
    "\n",
    "El método `.stream()` devuelve un generador que produce chunks de texto conforme se generan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f31f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STREAMING EN TIEMPO REAL ===\n",
      "Generando respuesta...\n",
      "--------------------------------------------------\n",
      "Había una vez un programador llamado Martín, un verdadero apasionado de las líneas de código y las noches interminables frente a la pantalla. Trabajaba como desarrollador en una pequeña empresa de tecnología, pero en secreto soñaba con crear algo único, algo que cambiara el mundo. Su vida transcurría entre teclados, tazas de café y un sinfín de errores de compilación.\n",
      "\n",
      "Una noche particularmente sombría, mientras trabajaba solo en su apartamento, Martín tropezó con un archivo antiguo en su computadora. El archivo tenía un nombre críptico: **\"arcano.py\"**. No recordaba haberlo visto antes, pero, intrigado, decidió abrirlo. Lo que encontró dentro era un código extraño, con símbolos y palabras que no correspondían a ningún lenguaje de programación que conociera. A pesar de que las funciones y métodos parecían familiares, había algo… distinto. Algo que parecía vivo.\n",
      "\n",
      "Movido por la curiosidad, Martín ejecutó el archivo. La pantalla parpadeó, y de repente, las luces del apartamento titilaron. Una suave brisa, imposible en un espacio cerrado, recorrió la habitación. En su monitor apareció un mensaje en letras doradas:\n",
      "\n",
      "**\"Bienvenido, te has convertido en el tejedor del código arcano. Todo lo que escribas aquí cobrará vida. Úsalo con sabiduría.\"**\n",
      "\n",
      "Martín pensó que era alguna clase de broma o un virus sofisticado, pero su corazón latía con fuerza. Decidió probarlo. Escribió una línea simple:\n",
      "\n",
      "```python\n",
      "crear_objeto(\"taza_flotante\")\n",
      "```\n",
      "\n",
      "Para su asombro, una pequeña taza de café apareció flotando frente a él, girando lentamente en el aire. La dejó caer en su mano, tibia y real. No podía creerlo. ¿Era esto magia? ¿Tecnología avanzada? ¿Un sueño?\n",
      "\n",
      "Durante las semanas siguientes, Martín experimentó con el código. Descubrió que podía crear objetos, alterar el clima en su apartamento e incluso sanar una cortada en su dedo con unas pocas líneas cuidadosamente redactadas. Pero lo más asombroso fue cuando intentó escribir una función que le permitiera entender a su gato, Copo. De repente, Copo comenzó a hablarle, con una voz grave y sabia que desentonaba completamente con su apariencia esponjosa.\n",
      "\n",
      "El poder era indescriptible, pero también aterrador. Martín pronto se dio cuenta de que cada línea de código que escribía tenía consecuencias impredecibles. Un día, cuando intentó generar dinero para pagar sus deudas, el banco local sufrió un inexplicable error en sus sistemas. Otro día, cuando escribió un programa para detener la lluvia, desencadenó una sequía en toda la ciudad.\n",
      "\n",
      "Martín entendió que el código arcano no era un juguete; era una herramienta que requería responsabilidad. Así que decidió usarlo para ayudar al mundo, pero de manera cuidadosa. Creó pequeños programas que reparaban carreteras, limpiaban los ríos y curaban enfermedades a nivel local. Todo lo hacía en secreto, dejando que otros se llevaran el crédito, porque sabía que el poder absoluto nunca debía estar en manos de una sola persona.\n",
      "\n",
      "Con el tiempo, Martín se convirtió en una figura misteriosa en la comunidad tecnológica. Nadie sabía de dónde venían sus soluciones milagrosas, pero todos lo admiraban. Él, por su parte, guardaba el archivo \"arcano.py\" en un rincón oculto de su computadora, sabiendo que, aunque había descubierto la magia en el código, su mayor logro sería usarla con bondad y sabiduría.\n",
      "\n",
      "Y así, Martín se convirtió no solo en un programador, sino en un mago del mundo moderno, tejedor de líneas que cambiaban la realidad.\n",
      "--------------------------------------------------\n",
      "✓ Streaming completado\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo básico de streaming\n",
    "def streaming_basico():\n",
    "    prompt = \"Cuéntame una historia corta sobre un programador que descubre la magia en el código\"\n",
    "    \n",
    "    print(\"=== STREAMING EN TIEMPO REAL ===\")\n",
    "    print(\"Generando respuesta...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # stream() devuelve un generador de chunks\n",
    "        for chunk in llm.stream([HumanMessage(content=prompt)]):\n",
    "            # Imprimir cada chunk sin nueva línea\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "            time.sleep(0.01)  # Pequeña pausa para simular streaming visual\n",
    "            \n",
    "        print(\"\\n\" + \"-\" * 50)\n",
    "        print(\"✓ Streaming completado\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error en streaming: {e}\")\n",
    "\n",
    "# Ejecutar streaming básico\n",
    "streaming_basico()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6214408",
   "metadata": {},
   "source": [
    "## Comparación: Streaming vs No-Streaming\n",
    "\n",
    "Veamos la diferencia en experiencia de usuario entre ambos enfoques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8e5874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPARACIÓN: STREAMING vs NO-STREAMING ===\\n\n",
      "1. SIN STREAMING:\n",
      "--------------------\n",
      "Esperando respuesta completa...\n",
      "\\n[Respuesta recibida después de 3.72 segundos]\n",
      "Python es un lenguaje de programación ampliamente reconocido por su simplicidad y versatilidad, lo que lo convierte en una opción ideal tanto para principiantes como para desarrolladores experimentados. Una de sus principales ventajas es su sintaxis clara y legible, que permite escribir y entender código de manera más eficiente, reduciendo la curva de aprendizaje y facilitando la colaboración en proyectos. Además, cuenta con una extensa biblioteca estándar y una enorme cantidad de paquetes externos que abarcan áreas como análisis de datos, inteligencia artificial, desarrollo web, automatización y más, lo que ahorra tiempo y esfuerzo en el desarrollo de soluciones. Python es también un lenguaje multiplataforma, lo que significa que los programas escritos en él pueden ejecutarse en diversos sistemas operativos sin modificaciones significativas. Su comunidad activa y en constante crecimiento proporciona un valioso soporte, actualizaciones frecuentes y recursos educativos que enriquecen aún más la experiencia de aprendizaje y desarrollo. Estas características hacen de Python una herramienta poderosa y flexible para abordar una amplia variedad de proyectos y desafíos tecnológicos.\n",
      "\\n============================================================\\n\n",
      "2. CON STREAMING:\n",
      "------------------\n",
      "Respuesta en tiempo real:\n",
      "Python es un lenguaje de programación altamente valorado por su versatilidad, simplicidad y comunidad activa, lo que lo convierte en una herramienta poderosa tanto para principiantes como para expertos. Su sintaxis clara y legible facilita el aprendizaje y la escritura de código, permitiendo a los desarrolladores centrarse en resolver problemas en lugar de luchar con la complejidad del lenguaje. Además, Python cuenta con una vasta biblioteca estándar y una amplia gama de paquetes y frameworks de terceros que abarcan áreas como ciencia de datos, inteligencia artificial, desarrollo web, automatización y más, lo que ahorra tiempo y esfuerzo en la implementación de soluciones. Su naturaleza multiplataforma permite que el código escrito en Python se ejecute en diversos sistemas operativos sin modificaciones significativas. Por último, la comunidad global de Python proporciona soporte constante, recursos educativos y actualizaciones regulares, asegurando que el lenguaje siga evolucionando y adaptándose a las necesidades del mundo tecnológico.\\n\\n[Streaming completado en 6.78 segundos]\n",
      "\\n============================================================\n",
      "OBSERVACIONES:\n",
      "- Sin streaming: El usuario espera sin feedback\n",
      "- Con streaming: El usuario ve progreso inmediato\n",
      "- Mejor percepción de velocidad con streaming\n",
      "- Streaming es especial para respuestas largas\n"
     ]
    }
   ],
   "source": [
    "# Comparación entre streaming y no-streaming\n",
    "def comparar_streaming():\n",
    "    # Modelo sin streaming\n",
    "    llm_no_stream = ChatOpenAI(\n",
    "        base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "        api_key=os.getenv(\"GITHUB_TOKEN\"),\n",
    "        model=\"gpt-4o\",\n",
    "        streaming=False,  # Sin streaming\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    prompt = \"Escribe un párrafo sobre las ventajas de la programación en Python\"\n",
    "    \n",
    "    print(\"=== COMPARACIÓN: STREAMING vs NO-STREAMING ===\\\\n\")\n",
    "    \n",
    "    # 1. Sin streaming\n",
    "    print(\"1. SIN STREAMING:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(\"Esperando respuesta completa...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = llm_no_stream.invoke([HumanMessage(content=prompt)])\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"\\\\n[Respuesta recibida después de {end_time - start_time:.2f} segundos]\")\n",
    "        print(response.content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60 + \"\\\\n\")\n",
    "    \n",
    "    # 2. Con streaming\n",
    "    print(\"2. CON STREAMING:\")\n",
    "    print(\"-\" * 18)\n",
    "    print(\"Respuesta en tiempo real:\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        for chunk in llm.stream([HumanMessage(content=prompt)]):\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "            time.sleep(0.03)  # Simular pausa para efecto visual\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"\\\\n\\\\n[Streaming completado en {end_time - start_time:.2f} segundos]\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"OBSERVACIONES:\")\n",
    "    print(\"- Sin streaming: El usuario espera sin feedback\")\n",
    "    print(\"- Con streaming: El usuario ve progreso inmediato\")\n",
    "    print(\"- Mejor percepción de velocidad con streaming\")\n",
    "    print(\"- Streaming es especial para respuestas largas\")\n",
    "\n",
    "# Ejecutar comparación\n",
    "comparar_streaming()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725ad79d",
   "metadata": {},
   "source": [
    "## Implementación de un Chatbot Simple con Streaming\n",
    "\n",
    "Creemos un chatbot básico que demuestre el streaming en un contexto práctico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231dac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a05a03cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHATBOT CON STREAMING ===\n",
      "Escribe 'salir' para terminar la conversación\\n\n",
      "\\n👋 ¡Hasta luego!\n",
      "\\n¡Gracias por usar el chatbot!\n",
      "💡 Descomenta la línea anterior para probar el chatbot interactivo\n"
     ]
    }
   ],
   "source": [
    "# Chatbot simple con streaming\n",
    "def chatbot_streaming():\n",
    "    print(\"=== CHATBOT CON STREAMING ===\")\n",
    "    print(\"Escribe 'salir' para terminar la conversación\\\\n\")\n",
    "    \n",
    "    # Configurar asistente con personalidad\n",
    "    system_message = \"\"\"Eres un asistente útil y amigable especializado en tecnología. \n",
    "    Respondes de manera clara y concisa, y siempre intentas ser educativo.\"\"\"\n",
    "    \n",
    "    while True:\n",
    "        # Obtener input del usuario\n",
    "        user_input = input(\"\\\\n🧑 Tú: \")\n",
    "        \n",
    "        if user_input.lower() in ['salir', 'exit', 'quit']:\n",
    "            print(\"\\\\n👋 ¡Hasta luego!\")\n",
    "            break\n",
    "            \n",
    "        if not user_input.strip():\n",
    "            continue\n",
    "            \n",
    "        print(\"\\\\n🤖 Asistente: \", end=\"\", flush=True)\n",
    "        \n",
    "        try:\n",
    "            # Streaming de la respuesta\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_input}\n",
    "            ]\n",
    "            \n",
    "            # Convertir a formato LangChain\n",
    "            from langchain.schema import SystemMessage\n",
    "            lc_messages = [\n",
    "                SystemMessage(content=system_message),\n",
    "                HumanMessage(content=user_input)\n",
    "            ]\n",
    "            \n",
    "            full_response = \"\"\n",
    "            for chunk in llm.stream(lc_messages):\n",
    "                content = chunk.content\n",
    "                print(content, end=\"\", flush=True)\n",
    "                full_response += content\n",
    "                time.sleep(0.02)\n",
    "                \n",
    "            print()  # Nueva línea al final\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\\\n\\\\n⏸️ Interrumpido por el usuario\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\\\n❌ Error: {e}\")\n",
    "            \n",
    "    print(\"\\\\n¡Gracias por usar el chatbot!\")\n",
    "\n",
    "# Ejecutar chatbot (¡Pruébalo!)\n",
    "chatbot_streaming() \n",
    " # Descomenta esta línea para ejecutar\n",
    "\n",
    "print(\"💡 Descomenta la línea anterior para probar el chatbot interactivo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6060e43c",
   "metadata": {},
   "source": [
    "## Streaming Avanzado con Manejo de Chunks\n",
    "\n",
    "Podemos procesar cada chunk individualmente para crear experiencias más sofisticadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f86fa1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STREAMING AVANZADO CON ANÁLISIS ===\n",
      "Analizando chunks conforme llegan...\n",
      "\n",
      "¡Claro! Vamos a explicar estos\\n[Progreso: 10 chunks, ~8 palabras]\\n\n",
      " conceptos de manera clara y sencilla.\n",
      "\n",
      "---\n",
      "\n",
      "### **\\n[Progreso: 20 chunks, ~18 palabras]\\n\n",
      "¿Qué es la Inteligencia Artificial (IA)?\\n[Progreso: 30 chunks, ~28 palabras]\\n\n",
      "**\n",
      "\n",
      "La **Inteligencia Artificial (IA)**\\n[Progreso: 40 chunks, ~38 palabras]\\n\n",
      " es una rama de la informática que busca crear sistemas\\n[Progreso: 50 chunks, ~48 palabras]\\n\n",
      " o máquinas capaces de realizar tareas que normalmente requieren inteligencia\\n[Progreso: 60 chunks, ~58 palabras]\\n\n",
      " humana. Estas tareas incluyen cosas como:\n",
      "\n",
      "- Recon\\n[Progreso: 70 chunks, ~68 palabras]\\n\n",
      "ocer patrones.\n",
      "- Resolver problemas.\n",
      "- Tomar\\n[Progreso: 80 chunks, ~78 palabras]\\n\n",
      " decisiones.\n",
      "- Aprender de la experiencia.\n",
      "-\\n[Progreso: 90 chunks, ~88 palabras]\\n\n",
      " Entender y procesar lenguajes humanos.\n",
      "\n",
      "\\n[Progreso: 100 chunks, ~98 palabras]\\n\n",
      "En resumen, la IA intenta imitar la forma\\n[Progreso: 110 chunks, ~108 palabras]\\n\n",
      " en que los humanos pensamos y actuamos, permit\\n[Progreso: 120 chunks, ~118 palabras]\\n\n",
      "iendo que las máquinas realicen tareas de manera más\\n[Progreso: 130 chunks, ~128 palabras]\\n\n",
      " \"inteligente\".\n",
      "\n",
      "Hay diferentes tipos de IA\\n[Progreso: 140 chunks, ~138 palabras]\\n\n",
      ":\n",
      "1. **IA débil o específica**\\n[Progreso: 150 chunks, ~148 palabras]\\n\n",
      ": Diseñada para realizar tareas concretas,\\n[Progreso: 160 chunks, ~158 palabras]\\n\n",
      " como asistentes virtuales (por ejemplo, Siri o\\n[Progreso: 170 chunks, ~168 palabras]\\n\n",
      " Alexa) o sistemas de recomendación como los que\\n[Progreso: 180 chunks, ~178 palabras]\\n\n",
      " usa Netflix.\n",
      "2. **IA fuerte o general\\n[Progreso: 190 chunks, ~188 palabras]\\n\n",
      "**: Sería una inteligencia similar a la humana\\n[Progreso: 200 chunks, ~198 palabras]\\n\n",
      ", capaz de realizar cualquier tarea intelectual. Este tipo\\n[Progreso: 210 chunks, ~208 palabras]\\n\n",
      " de IA aún no se ha desarrollado por completo.\n",
      "\n",
      "\\n[Progreso: 220 chunks, ~218 palabras]\\n\n",
      "---\n",
      "\n",
      "### **¿Qué es el Machine Learning (\\n[Progreso: 230 chunks, ~228 palabras]\\n\n",
      "ML)?**\n",
      "\n",
      "El **Machine Learning (ML)**\\n[Progreso: 240 chunks, ~238 palabras]\\n\n",
      ", o aprendizaje automático, es una subrama de\\n[Progreso: 250 chunks, ~248 palabras]\\n\n",
      " la inteligencia artificial. Se centra en desarrollar algoritmos\\n[Progreso: 260 chunks, ~258 palabras]\\n\n",
      " o modelos que permiten a los sistemas aprender automáticamente a\\n[Progreso: 270 chunks, ~268 palabras]\\n\n",
      " partir de datos **sin ser programados explícitamente\\n[Progreso: 280 chunks, ~278 palabras]\\n\n",
      "** para realizar cada tarea.\n",
      "\n",
      "En lugar de escribir\\n[Progreso: 290 chunks, ~288 palabras]\\n\n",
      " reglas específicas para una tarea, en el ML se\\n[Progreso: 300 chunks, ~298 palabras]\\n\n",
      " entrena a un modelo con datos para que apr\\n[Progreso: 310 chunks, ~308 palabras]\\n\n",
      "enda patrones y realice predicciones o clasificaciones\\n[Progreso: 320 chunks, ~318 palabras]\\n\n",
      ". \n",
      "\n",
      "Por ejemplo:\n",
      "- Una app de fotos\\n[Progreso: 330 chunks, ~327 palabras]\\n\n",
      " puede aprender a identificar gatos al entrenarse con miles\\n[Progreso: 340 chunks, ~337 palabras]\\n\n",
      " de imágenes etiquetadas como \"gato\"\\n[Progreso: 350 chunks, ~347 palabras]\\n\n",
      " y \"no gato\".\n",
      "- Un sistema de dete\\n[Progreso: 360 chunks, ~357 palabras]\\n\n",
      "cción de fraudes bancarios puede aprender a identificar\\n[Progreso: 370 chunks, ~367 palabras]\\n\n",
      " transacciones sospechosas analizando datos históricos de\\n[Progreso: 380 chunks, ~377 palabras]\\n\n",
      " actividades fraudulentas.\n",
      "\n",
      "---\n",
      "\n",
      "### **¿Cómo funciona\\n[Progreso: 390 chunks, ~387 palabras]\\n\n",
      " el Machine Learning?**\n",
      "\n",
      "El ML se basa en\\n[Progreso: 400 chunks, ~397 palabras]\\n\n",
      " **datos** y en **algoritmos matem\\n[Progreso: 410 chunks, ~407 palabras]\\n\n",
      "áticos**. El proceso general funciona así:\n",
      "\n",
      "1\\n[Progreso: 420 chunks, ~417 palabras]\\n\n",
      ". **Recolección de datos**:\n",
      "  \\n[Progreso: 430 chunks, ~426 palabras]\\n\n",
      " - El sistema necesita datos para aprender. Estos datos\\n[Progreso: 440 chunks, ~436 palabras]\\n\n",
      " pueden ser números, imágenes, texto, sonidos,\\n[Progreso: 450 chunks, ~446 palabras]\\n\n",
      " etc.\n",
      "\n",
      "2. **Entrenamiento del modelo\\n[Progreso: 460 chunks, ~456 palabras]\\n\n",
      "**:\n",
      "   - Se alimenta al modelo con\\n[Progreso: 470 chunks, ~465 palabras]\\n\n",
      " datos etiquetados (en el caso del aprendizaje\\n[Progreso: 480 chunks, ~475 palabras]\\n\n",
      " supervisado) o sin etiquetas (en el caso\\n[Progreso: 490 chunks, ~485 palabras]\\n\n",
      " del aprendizaje no supervisado). Aquí, el modelo\\n[Progreso: 500 chunks, ~495 palabras]\\n\n",
      " busca patrones o asociaciones en los datos.\n",
      "   -\\n[Progreso: 510 chunks, ~504 palabras]\\n\n",
      " Por ejemplo, en un sistema de reconocimiento facial,\\n[Progreso: 520 chunks, ~514 palabras]\\n\n",
      " se entrenará el modelo con muchas fotos que indiqu\\n[Progreso: 530 chunks, ~524 palabras]\\n\n",
      "en qué rostro pertenece a quién.\n",
      "\n",
      "3. **\\n[Progreso: 540 chunks, ~534 palabras]\\n\n",
      "Ajuste y optimización**:\n",
      "   -\\n[Progreso: 550 chunks, ~543 palabras]\\n\n",
      " El modelo ajusta sus parámetros internos (como pesos\\n[Progreso: 560 chunks, ~553 palabras]\\n\n",
      " en las redes neuronales) para minimizar los errores\\n[Progreso: 570 chunks, ~563 palabras]\\n\n",
      " en sus predicciones. Esto se hace mediante métodos\\n[Progreso: 580 chunks, ~573 palabras]\\n\n",
      " como el **gradiente descendente**.\n",
      "\n",
      "4\\n[Progreso: 590 chunks, ~583 palabras]\\n\n",
      ". **Evaluación**:\n",
      "   - Después de\\n[Progreso: 600 chunks, ~592 palabras]\\n\n",
      " entrenar, el modelo se evalúa con datos\\n[Progreso: 610 chunks, ~602 palabras]\\n\n",
      " nuevos (no vistos durante el entrenamiento) para verificar\\n[Progreso: 620 chunks, ~612 palabras]\\n\n",
      " qué tan bien funciona.\n",
      "\n",
      "5. **Predicción\\n[Progreso: 630 chunks, ~622 palabras]\\n\n",
      " o inferencia**:\n",
      "   - Una vez entren\\n[Progreso: 640 chunks, ~631 palabras]\\n\n",
      "ado y afinado, el modelo está listo para\\n[Progreso: 650 chunks, ~641 palabras]\\n\n",
      " hacer predicciones o tomar decisiones basadas en nuevos\\n[Progreso: 660 chunks, ~651 palabras]\\n\n",
      " datos.\n",
      "\n",
      "---\n",
      "\n",
      "### **Tipos de Machine Learning**\n",
      "\n",
      "\\n[Progreso: 670 chunks, ~661 palabras]\\n\n",
      "Existen tres principales enfoques de ML:\n",
      "\n",
      "1\\n[Progreso: 680 chunks, ~671 palabras]\\n\n",
      ". **Aprendizaje supervisado**:\n",
      "  \\n[Progreso: 690 chunks, ~680 palabras]\\n\n",
      " - El modelo se entrena con datos etiquet\\n[Progreso: 700 chunks, ~690 palabras]\\n\n",
      "ados (cada dato de entrada tiene un resultado esperado\\n[Progreso: 710 chunks, ~700 palabras]\\n\n",
      ").\n",
      "   - Ejemplo: Entrenar un modelo\\n[Progreso: 720 chunks, ~709 palabras]\\n\n",
      " con imágenes de perros y gatos etiquetadas para\\n[Progreso: 730 chunks, ~719 palabras]\\n\n",
      " que pueda identificar si una nueva imagen es de un\\n[Progreso: 740 chunks, ~729 palabras]\\n\n",
      " perro o un gato.\n",
      "\n",
      "2. **Aprendiz\\n[Progreso: 750 chunks, ~739 palabras]\\n\n",
      "aje no supervisado**:\n",
      "   - No hay\\n[Progreso: 760 chunks, ~748 palabras]\\n\n",
      " etiquetas en los datos, y el modelo trata de\\n[Progreso: 770 chunks, ~758 palabras]\\n\n",
      " encontrar patrones o estructuras ocultas.\n",
      "   - Ej\\n[Progreso: 780 chunks, ~767 palabras]\\n\n",
      "emplo: Agrupar clientes con características similares en un\\n[Progreso: 790 chunks, ~777 palabras]\\n\n",
      " análisis de mercado.\n",
      "\n",
      "3. **Aprendizaje\\n[Progreso: 800 chunks, ~787 palabras]\\n\n",
      " por refuerzo**:\n",
      "   - El modelo aprende\\n[Progreso: 810 chunks, ~796 palabras]\\n\n",
      " a través de prueba y error, recibiendo\\n[Progreso: 820 chunks, ~806 palabras]\\n\n",
      " recompensas o castigos según sus acciones.\n",
      "  \\n[Progreso: 830 chunks, ~815 palabras]\\n\n",
      " - Ejemplo: Entrenar a un robot para\\n[Progreso: 840 chunks, ~825 palabras]\\n\n",
      " jugar un videojuego aprendiendo qué acciones lo acerc\\n[Progreso: 850 chunks, ~835 palabras]\\n\n",
      "an a ganar.\n",
      "\n",
      "---\n",
      "\n",
      "En resumen, la **\\n[Progreso: 860 chunks, ~845 palabras]\\n\n",
      "IA** es el concepto general de máquinas que sim\\n[Progreso: 870 chunks, ~855 palabras]\\n\n",
      "ulan inteligencia, mientras que el **Machine Learning**\\n[Progreso: 880 chunks, ~865 palabras]\\n\n",
      " es una técnica específica dentro de la IA que permite\\n[Progreso: 890 chunks, ~875 palabras]\\n\n",
      " a las máquinas"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mn✗ Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Ejecutar streaming avanzado\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[43mstreaming_avanzado\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mstreaming_avanzado\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# Imprimir el contenido\u001b[39;00m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mprint\u001b[39m(content, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.02\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pausa ligeramente más larga para ver el análisis\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Estadísticas finales\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mn\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mn=== ESTADÍSTICAS FINALES ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Streaming con análisis de chunks\n",
    "def streaming_avanzado():\n",
    "    prompt = \"Explica qué es la inteligencia artificial y cómo funciona el machine learning\"\n",
    "    \n",
    "    print(\"=== STREAMING AVANZADO CON ANÁLISIS ===\")\n",
    "    print(\"Analizando chunks conforme llegan...\\n\")\n",
    "    \n",
    "    # Variables para estadísticas\n",
    "    chunk_count = 0\n",
    "    total_content = \"\"\n",
    "    words_processed = 0\n",
    "    \n",
    "    try:\n",
    "        for chunk in llm.stream([HumanMessage(content=prompt)]):\n",
    "            chunk_count += 1\n",
    "            content = chunk.content\n",
    "            total_content += content\n",
    "            \n",
    "            # Contar palabras aproximadas\n",
    "            if content.strip():\n",
    "                words_in_chunk = len(content.split())\n",
    "                words_processed += words_in_chunk\n",
    "            \n",
    "            # Mostrar progreso cada 10 chunks\n",
    "            if chunk_count % 10 == 0:\n",
    "                print(f\"\\\\n[Progreso: {chunk_count} chunks, ~{words_processed} palabras]\\\\n\")\n",
    "            \n",
    "            # Imprimir el contenido\n",
    "            print(content, end=\"\", flush=True)\n",
    "            time.sleep(0.02)  # Pausa ligeramente más larga para ver el análisis\n",
    "        \n",
    "        # Estadísticas finales\n",
    "        print(f\"\\\\n\\\\n=== ESTADÍSTICAS FINALES ===\")\n",
    "        print(f\"Total de chunks: {chunk_count}\")\n",
    "        print(f\"Palabras aproximadas: {words_processed}\")\n",
    "        print(f\"Caracteres totales: {len(total_content)}\")\n",
    "        print(f\"Promedio chars/chunk: {len(total_content)/chunk_count if chunk_count > 0 else 0:.1f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\\\n✗ Error: {e}\")\n",
    "\n",
    "# Ejecutar streaming avanzado\n",
    "streaming_avanzado()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac251a9d",
   "metadata": {},
   "source": [
    "## Consideraciones Técnicas del Streaming\n",
    "\n",
    "### Cuándo Usar Streaming:\n",
    "✅ **SÍ usar streaming:**\n",
    "- Respuestas largas (>100 tokens)\n",
    "- Aplicaciones interactivas\n",
    "- Chatbots y asistentes\n",
    "- Demostraciones en vivo\n",
    "- Cuando la UX es prioritaria\n",
    "\n",
    "❌ **NO usar streaming:**\n",
    "- Respuestas muy cortas\n",
    "- Procesamiento batch\n",
    "- APIs de backend sin interfaz\n",
    "- Cuando necesitas la respuesta completa antes de procesar\n",
    "\n",
    "### Mejores Prácticas:\n",
    "1. **Manejo de errores**: Siempre incluye try/catch\n",
    "2. **Indicadores visuales**: Muestra progreso al usuario\n",
    "3. **Cancelación**: Permite al usuario interrumpir\n",
    "4. **Buffer management**: Para interfaces web, considera buffering\n",
    "5. **Performance**: Monitorea el uso de recursos\n",
    "\n",
    "## Ejercicios Prácticos\n",
    "\n",
    "### Ejercicio 1: Indicador de Progreso\n",
    "Modifica el código para mostrar un indicador de progreso (spinner, barra, porcentaje).\n",
    "\n",
    "### Ejercicio 2: Streaming con Filtros\n",
    "Implementa streaming que filtre o procese chunks específicos (ej: resaltar palabras clave).\n",
    "\n",
    "### Ejercicio 3: Chatbot Mejorado\n",
    "Extiende el chatbot con:\n",
    "- Historial de conversación\n",
    "- Comandos especiales (/help, /clear)\n",
    "- Diferentes personalidades\n",
    "\n",
    "## Conceptos Clave Aprendidos\n",
    "\n",
    "1. **Streaming** mejora la percepción de velocidad\n",
    "2. **Chunks** se procesan individualmente en tiempo real\n",
    "3. **UX** es significativamente mejor con streaming\n",
    "4. **Implementación** requiere manejo cuidadoso de generadores\n",
    "5. **Casos de uso** específicos donde streaming aporta valor\n",
    "\n",
    "## Próximos Pasos\n",
    "\n",
    "En el siguiente notebook exploraremos la **memoria en LangChain**, que nos permite mantener contexto entre múltiples interacciones del usuario."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
