{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hfn0f7nyumn",
   "metadata": {},
   "source": [
    "# 3. LangChain Streaming - Respuestas en Tiempo Real\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Comprender qué es el streaming y cuándo usarlo\n",
    "- Implementar streaming con LangChain\n",
    "- Manejar chunks de datos en tiempo real\n",
    "- Construir interfaces de usuario reactivas\n",
    "\n",
    "## ¿Qué es el Streaming?\n",
    "\n",
    "El streaming permite recibir la respuesta del modelo **token por token** conforme se genera, en lugar de esperar a que termine completamente. Esto mejora significativamente la experiencia de usuario en aplicaciones interactivas.\n",
    "\n",
    "### Ventajas del Streaming:\n",
    "- **Percepción de velocidad**: El usuario ve progreso inmediato\n",
    "- **Mejor UX**: Interfaces más reactivas e interactivas  \n",
    "- **Engagement**: Mantiene la atención del usuario\n",
    "- **Debugging**: Permite ver el proceso de generación\n",
    "\n",
    "### Casos de Uso Ideales:\n",
    "- Chatbots y asistentes conversacionales\n",
    "- Generación de contenido largo\n",
    "- Aplicaciones web interactivas\n",
    "- Demostraciones en vivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa694f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas necesarias\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"Bibliotecas importadas correctamente para streaming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f037cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del modelo con streaming habilitado\n",
    "try:\n",
    "    \n",
    "    # llm = ChatOpenAI(\n",
    "    #     base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "    #     api_key=os.getenv(\"GITHUB_TOKEN\"),\n",
    "    #     model=\"gpt-4o\",\n",
    "    #     streaming=True,  # ¡Importante: habilitar streaming!\n",
    "    #     temperature=0.7\n",
    "    # )\n",
    "    llm = ChatOpenAI(\n",
    "        base_url=\"https://models.github.ai/inference\",  #descubri que aqui  cambia de url base  \n",
    "        api_key=os.getenv(\"GITHUB_TOKEN\"),               \n",
    "        model = \"openai/gpt-4.1\",               # modelo DeepSeek\n",
    "        temperature=0.7,\n",
    "        streaming=True, \n",
    "        max_tokens=150\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Modelo configurado con streaming habilitado\")\n",
    "    print(f\"Modelo: {llm.model_name}\")\n",
    "    print(f\"Streaming: {llm.streaming}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error en configuración: {e}\")\n",
    "    print(\"Verifica las variables de entorno\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da0d27",
   "metadata": {},
   "source": [
    "## Streaming Básico\n",
    "\n",
    "El método `.stream()` devuelve un generador que produce chunks de texto conforme se generan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f31f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo básico de streaming\n",
    "def streaming_basico():\n",
    "    prompt = \"Cuéntame una historia breve sobre los beneficios de el oregano \"\n",
    "    \n",
    "    print(\"=== STREAMING EN TIEMPO REAL ===\")\n",
    "    print(\"Generando respuesta...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # stream() devuelve un generador de chunks\n",
    "        for chunk in llm.stream([HumanMessage(content=prompt)]):\n",
    "            # Imprimir cada chunk sin nueva línea\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "            time.sleep(0.01)  # Pequeña pausa para simular streaming visual\n",
    "            \n",
    "        print(\"\\n\" + \"-\" * 50)\n",
    "        print(\"✓ Streaming completado\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error en streaming: {e}\")\n",
    "\n",
    "# Ejecutar streaming básico\n",
    "streaming_basico()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e10285d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EJERCICIO 1A: SPINNER SIMPLE ===\n",
      "Generando respuesta...\n",
      "\n",
      "✓ Completado!   \n",
      "\n",
      "Respuesta completa:\n",
      "El **orégano** (Origanum vulgare) es una hierba aromática comúnmente utilizada en la cocina mediterránea, que además posee varios beneficios para la salud gracias a sus compuestos bioactivos, como los aceites esenciales, antioxidantes y vitaminas. Entre sus principales beneficios destacan:\n",
      "\n",
      "### 1. **Propiedades antioxidantes**\n",
      "El orégano contiene compuestos como el **carvacrol**, el **timol** y los **flavonoides**, que ayudan a combatir el daño causado por los radicales libres en las células, lo que puede proteger contra el envejecimiento prematuro y ciertas enfermedades crónicas.\n",
      "\n",
      "### 2. **Efecto antimicrobiano**\n",
      "El orégano\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "from itertools import cycle\n",
    "from datetime import datetime\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EJERCICIO 1: INDICADORES DE PROGRESO\n",
    "# =============================================================================\n",
    "\n",
    "def ejercicio1_spinner_simple():\n",
    "    \"\"\"Spinner básico que rota mientras llegan chunks\"\"\"\n",
    "  \n",
    "    spinner = cycle(['|', '/', '-', '\\\\'])\n",
    "    prompt = \"Explica los beneficios del oregano para la salud\"\n",
    "    \n",
    "    print(\"=== EJERCICIO 1A: SPINNER SIMPLE ===\")\n",
    "    print(\"Generando respuesta...\\n\")\n",
    "    \n",
    "    response_text = \"\"\n",
    "    try:\n",
    "        \n",
    "        for chunk in llm.stream([HumanMessage(content=prompt)]):\n",
    "            # Mostrar spinner\n",
    "            sys.stdout.write(f'\\r{next(spinner)} Procesando...')\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            response_text += chunk.content\n",
    "            time.sleep(0.05)\n",
    "            #print(chunk.content, end=\"\", flush=True)\n",
    "  \n",
    "        # Limpiar spinner y mostrar resultado\n",
    "        sys.stdout.write('\\r✓ Completado!   \\n')\n",
    "        print(f\"\\nRespuesta completa:\\n{response_text}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "ejercicio1_spinner_simple()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2190b9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EJERCICIO 1B: BARRA DE PROGRESO ===\n",
      "[██████████████████████████████] 100% (608 chars)\n",
      "\n",
      "\n",
      "Respuesta final:\n",
      "La **fotosíntesis** es el proceso mediante el cual las plantas fabrican su propio alimento utilizando la energía de la luz solar. Ocurre principalmente en las hojas y, específicamente, en unos orgánulos llamados **cloroplastos**.\n",
      "\n",
      "El proceso se puede resumir en los siguientes pasos:\n",
      "\n",
      "1. **Captura de luz solar:** Las plantas absorben la luz solar gracias a la clorofila, un pigmento verde presente en los cloroplastos.\n",
      "\n",
      "2. **Absorción de dióxido de carbono y agua:** Las plantas toman dióxido de carbono (CO₂) del aire a través de pequeños poros en las hojas llamados estomas y absorben agua (H₂O) del suelo\n",
      "\n",
      "✅ Barra de progreso completada!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "def ejercicio1_barra_progreso():\n",
    "    \"\"\"Barra de progreso visual basada en longitud de texto generado\"\"\"\n",
    "    \n",
    "    prompt = \"Describe el proceso de fotosíntesis en las plantas\"\n",
    "    print(\"=== EJERCICIO 1B: BARRA DE PROGRESO ===\")\n",
    "    \n",
    "    response_text = \"\"\n",
    "    last_percent = 0\n",
    "    bar_length = 30\n",
    "    \n",
    "    try:\n",
    "        for chunk in llm.stream([HumanMessage(content=prompt)]):\n",
    "            response_text += chunk.content\n",
    "            \n",
    "            # Progreso estimado basado en cantidad de caracteres\n",
    "            total_estimate = 500  # puedes ajustar según lo largo que esperas que sea la respuesta\n",
    "            progress = min(len(response_text) / total_estimate, 1.0)\n",
    "            \n",
    "            filled_length = int(bar_length * progress)\n",
    "            bar = '█' * filled_length + '░' * (bar_length - filled_length)\n",
    "            percent = int(progress * 100)\n",
    "            \n",
    "            # Solo actualizar si hay un cambio real en porcentaje\n",
    "            if percent != last_percent:\n",
    "                sys.stdout.write(f'\\r[{bar}] {percent}% ({len(response_text)} chars)')\n",
    "                sys.stdout.flush()\n",
    "                last_percent = percent\n",
    "            \n",
    "            time.sleep(0.03)\n",
    "        \n",
    "        # Completa la barra al final\n",
    "        sys.stdout.write('\\r[' + '█'*bar_length + '] 100% (' + str(len(response_text)) + ' chars)\\n')\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        print(f\"\\n\\nRespuesta final:\\n{response_text}\\n\")\n",
    "        print(\"✅ Barra de progreso completada!\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "\n",
    "ejercicio1_barra_progreso()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6214408",
   "metadata": {},
   "source": [
    "## Comparación: Streaming vs No-Streaming\n",
    "\n",
    "Veamos la diferencia en experiencia de usuario entre ambos enfoques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8e5874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparación entre streaming y no-streaming\n",
    "def comparar_streaming():\n",
    "\n",
    "    # Modelo sin streaming\n",
    "    llm_no_stream = ChatOpenAI(\n",
    "        base_url=\"https://models.github.ai/inference\",  #descubri que aqui  cambia de url base  \n",
    "        api_key=os.getenv(\"GITHUB_TOKEN\"),               \n",
    "        model = \"openai/gpt-4.1\",               # modelo DeepSeek\n",
    "        temperature=0.7,\n",
    "        streaming=False  # Sin streaming\n",
    "       \n",
    "    )\n",
    "    \n",
    "    prompt = \"Escribe un párrafo sobre beneficions del oregano\"\n",
    "    \n",
    "    print(\"=== COMPARACIÓN: STREAMING vs NO-STREAMING ===\\\\n\")\n",
    "    \n",
    "    # 1. Sin streaming\n",
    "    print(\"1. SIN STREAMING:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(\"Esperando respuesta completa...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = llm_no_stream.invoke([HumanMessage(content=prompt)])\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"\\\\n[Respuesta recibida después de {end_time - start_time:.2f} segundos]\")\n",
    "        print(response.content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60 + \"\\\\n\")\n",
    "    \n",
    "    # 2. Con streaming\n",
    "    print(\"2. CON STREAMING:\")\n",
    "    print(\"-\" * 18)\n",
    "    print(\"Respuesta en tiempo real:\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        for chunk in llm.stream([HumanMessage(content=prompt)]):\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "            time.sleep(0.03)  # Simular pausa para efecto visual\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"\\\\n\\\\n[Streaming completado en {end_time - start_time:.2f} segundos]\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"OBSERVACIONES:\")\n",
    "    print(\"- Sin streaming: El usuario espera sin feedback\")\n",
    "    print(\"- Con streaming: El usuario ve progreso inmediato\")\n",
    "    print(\"- Mejor percepción de velocidad con streaming\")\n",
    "    print(\"- Streaming es especial para respuestas largas\")\n",
    "\n",
    "# Ejecutar comparación\n",
    "comparar_streaming()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725ad79d",
   "metadata": {},
   "source": [
    "## Implementación de un Chatbot Simple con Streaming\n",
    "\n",
    "Creemos un chatbot básico que demuestre el streaming en un contexto práctico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231dac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05a03cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatbot simple con streaming\n",
    "def chatbot_streaming():\n",
    "    print(\"=== CHATBOT CON STREAMING ===\")\n",
    "    print(\"Escribe 'salir' para terminar la conversación\\\\n\")\n",
    "    \n",
    "    # Configurar asistente con personalidad\n",
    "    system_message = \"\"\"Eres un asistente útil y amigable especializado en tecnología. \n",
    "    Respondes de manera clara y concisa, y siempre intentas ser educativo.\"\"\"\n",
    "    \n",
    "    while True:\n",
    "        # Obtener input del usuario\n",
    "        user_input = input(\"\\\\n🧑 Tú: \")\n",
    "        \n",
    "        if user_input.lower() in ['salir', 'exit', 'quit']:\n",
    "            print(\"\\\\n👋 ¡Hasta luego!\")\n",
    "            break\n",
    "            \n",
    "        if not user_input.strip():\n",
    "            continue\n",
    "            \n",
    "        print(\"\\\\n🤖 Asistente: \", end=\"\", flush=True)\n",
    "        \n",
    "        try:\n",
    "            # Streaming de la respuesta\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_input}\n",
    "            ]\n",
    "            \n",
    "            # Convertir a formato LangChain\n",
    "            from langchain.schema import SystemMessage\n",
    "            lc_messages = [\n",
    "                SystemMessage(content=system_message),\n",
    "                HumanMessage(content=user_input)\n",
    "            ]\n",
    "            \n",
    "            full_response = \"\"\n",
    "            for chunk in llm.stream(lc_messages):\n",
    "                content = chunk.content\n",
    "                print(content, end=\"\", flush=True)\n",
    "                full_response += content\n",
    "                time.sleep(0.02)\n",
    "                \n",
    "            print()  # Nueva línea al final\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\\\n\\\\n⏸️ Interrumpido por el usuario\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\\\n❌ Error: {e}\")\n",
    "            \n",
    "    print(\"\\\\n¡Gracias por usar el chatbot!\")\n",
    "\n",
    "# Ejecutar chatbot (¡Pruébalo!)\n",
    "chatbot_streaming() \n",
    " # Descomenta esta línea para ejecutar\n",
    "\n",
    "print(\"💡 Descomenta la línea anterior para probar el chatbot interactivo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6060e43c",
   "metadata": {},
   "source": [
    "## Streaming Avanzado con Manejo de Chunks\n",
    "\n",
    "Podemos procesar cada chunk individualmente para crear experiencias más sofisticadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86fa1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming con análisis de chunks\n",
    "def streaming_avanzado():\n",
    "    prompt = \"Explica qué es la inteligencia artificial y cómo funciona el machine learning\"\n",
    "    \n",
    "    print(\"=== STREAMING AVANZADO CON ANÁLISIS ===\")\n",
    "    print(\"Analizando chunks conforme llegan...\\n\")\n",
    "    \n",
    "    # Variables para estadísticas\n",
    "    chunk_count = 0\n",
    "    total_content = \"\"\n",
    "    words_processed = 0\n",
    "    \n",
    "    try:\n",
    "        for chunk in llm.stream([HumanMessage(content=prompt)]):\n",
    "            chunk_count += 1\n",
    "            content = chunk.content\n",
    "            total_content += content\n",
    "            \n",
    "            # Contar palabras aproximadas\n",
    "            if content.strip():\n",
    "                words_in_chunk = len(content.split())\n",
    "                words_processed += words_in_chunk\n",
    "            \n",
    "            # Mostrar progreso cada 10 chunks\n",
    "            if chunk_count % 10 == 0:\n",
    "                print(f\"\\\\n[Progreso: {chunk_count} chunks, ~{words_processed} palabras]\\\\n\")\n",
    "            \n",
    "            # Imprimir el contenido\n",
    "            print(content, end=\"\", flush=True)\n",
    "            time.sleep(0.02)  # Pausa ligeramente más larga para ver el análisis\n",
    "        \n",
    "        # Estadísticas finales\n",
    "        print(f\"\\\\n\\\\n=== ESTADÍSTICAS FINALES ===\")\n",
    "        print(f\"Total de chunks: {chunk_count}\")\n",
    "        print(f\"Palabras aproximadas: {words_processed}\")\n",
    "        print(f\"Caracteres totales: {len(total_content)}\")\n",
    "        print(f\"Promedio chars/chunk: {len(total_content)/chunk_count if chunk_count > 0 else 0:.1f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\\\n✗ Error: {e}\")\n",
    "\n",
    "# Ejecutar streaming avanzado\n",
    "streaming_avanzado()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac251a9d",
   "metadata": {},
   "source": [
    "## Consideraciones Técnicas del Streaming\n",
    "\n",
    "### Cuándo Usar Streaming:\n",
    "✅ **SÍ usar streaming:**\n",
    "- Respuestas largas (>100 tokens)\n",
    "- Aplicaciones interactivas\n",
    "- Chatbots y asistentes\n",
    "- Demostraciones en vivo\n",
    "- Cuando la UX es prioritaria\n",
    "\n",
    "❌ **NO usar streaming:**\n",
    "- Respuestas muy cortas\n",
    "- Procesamiento batch\n",
    "- APIs de backend sin interfaz\n",
    "- Cuando necesitas la respuesta completa antes de procesar\n",
    "\n",
    "### Mejores Prácticas:\n",
    "1. **Manejo de errores**: Siempre incluye try/catch\n",
    "2. **Indicadores visuales**: Muestra progreso al usuario\n",
    "3. **Cancelación**: Permite al usuario interrumpir\n",
    "4. **Buffer management**: Para interfaces web, considera buffering\n",
    "5. **Performance**: Monitorea el uso de recursos\n",
    "\n",
    "## Ejercicios Prácticos\n",
    "\n",
    "### Ejercicio 1: Indicador de Progreso\n",
    "Modifica el código para mostrar un indicador de progreso (spinner, barra, porcentaje).\n",
    "\n",
    "### Ejercicio 2: Streaming con Filtros\n",
    "Implementa streaming que filtre o procese chunks específicos (ej: resaltar palabras clave).\n",
    "\n",
    "### Ejercicio 3: Chatbot Mejorado\n",
    "Extiende el chatbot con:\n",
    "- Historial de conversación\n",
    "- Comandos especiales (/help, /clear)\n",
    "- Diferentes personalidades\n",
    "\n",
    "## Conceptos Clave Aprendidos\n",
    "\n",
    "1. **Streaming** mejora la percepción de velocidad\n",
    "2. **Chunks** se procesan individualmente en tiempo real\n",
    "3. **UX** es significativamente mejor con streaming\n",
    "4. **Implementación** requiere manejo cuidadoso de generadores\n",
    "5. **Casos de uso** específicos donde streaming aporta valor\n",
    "\n",
    "## Próximos Pasos\n",
    "\n",
    "En el siguiente notebook exploraremos la **memoria en LangChain**, que nos permite mantener contexto entre múltiples interacciones del usuario."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
