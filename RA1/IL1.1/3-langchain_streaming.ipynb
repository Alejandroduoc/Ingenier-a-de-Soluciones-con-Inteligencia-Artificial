{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hfn0f7nyumn",
   "metadata": {},
   "source": [
    "# 3. LangChain Streaming - Respuestas en Tiempo Real\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Comprender qu√© es el streaming y cu√°ndo usarlo\n",
    "- Implementar streaming con LangChain\n",
    "- Manejar chunks de datos en tiempo real\n",
    "- Construir interfaces de usuario reactivas\n",
    "\n",
    "## ¬øQu√© es el Streaming?\n",
    "\n",
    "El streaming permite recibir la respuesta del modelo **token por token** conforme se genera, en lugar de esperar a que termine completamente. Esto mejora significativamente la experiencia de usuario en aplicaciones interactivas.\n",
    "\n",
    "### Ventajas del Streaming:\n",
    "- **Percepci√≥n de velocidad**: El usuario ve progreso inmediato\n",
    "- **Mejor UX**: Interfaces m√°s reactivas e interactivas  \n",
    "- **Engagement**: Mantiene la atenci√≥n del usuario\n",
    "- **Debugging**: Permite ver el proceso de generaci√≥n\n",
    "\n",
    "### Casos de Uso Ideales:\n",
    "- Chatbots y asistentes conversacionales\n",
    "- Generaci√≥n de contenido largo\n",
    "- Aplicaciones web interactivas\n",
    "- Demostraciones en vivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aa694f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas importadas correctamente para streaming\n"
     ]
    }
   ],
   "source": [
    "# Importar bibliotecas necesarias\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"Bibliotecas importadas correctamente para streaming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13f037cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Modelo configurado con streaming habilitado\n",
      "Modelo: gpt-4o\n",
      "Streaming: True\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n del modelo con streaming habilitado\n",
    "try:\n",
    "    llm = ChatOpenAI(\n",
    "        base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "        api_key=os.getenv(\"GITHUB_TOKEN\"),\n",
    "        model=\"gpt-4o\",\n",
    "        streaming=True,  # ¬°Importante: habilitar streaming!\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Modelo configurado con streaming habilitado\")\n",
    "    print(f\"Modelo: {llm.model_name}\")\n",
    "    print(f\"Streaming: {llm.streaming}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error en configuraci√≥n: {e}\")\n",
    "    print(\"Verifica las variables de entorno\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da0d27",
   "metadata": {},
   "source": [
    "## Streaming B√°sico\n",
    "\n",
    "El m√©todo `.stream()` devuelve un generador que produce chunks de texto conforme se generan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1f31f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STREAMING EN TIEMPO REAL ===\n",
      "Generando respuesta...\n",
      "--------------------------------------------------\n",
      "Hab√≠a una vez un programador llamado Mart√≠n, un verdadero apasionado de las l√≠neas de c√≥digo y las noches interminables frente a la pantalla. Trabajaba como desarrollador en una peque√±a empresa de tecnolog√≠a, pero en secreto so√±aba con crear algo √∫nico, algo que cambiara el mundo. Su vida transcurr√≠a entre teclados, tazas de caf√© y un sinf√≠n de errores de compilaci√≥n.\n",
      "\n",
      "Una noche particularmente sombr√≠a, mientras trabajaba solo en su apartamento, Mart√≠n tropez√≥ con un archivo antiguo en su computadora. El archivo ten√≠a un nombre cr√≠ptico: **\"arcano.py\"**. No recordaba haberlo visto antes, pero, intrigado, decidi√≥ abrirlo. Lo que encontr√≥ dentro era un c√≥digo extra√±o, con s√≠mbolos y palabras que no correspond√≠an a ning√∫n lenguaje de programaci√≥n que conociera. A pesar de que las funciones y m√©todos parec√≠an familiares, hab√≠a algo‚Ä¶ distinto. Algo que parec√≠a vivo.\n",
      "\n",
      "Movido por la curiosidad, Mart√≠n ejecut√≥ el archivo. La pantalla parpade√≥, y de repente, las luces del apartamento titilaron. Una suave brisa, imposible en un espacio cerrado, recorri√≥ la habitaci√≥n. En su monitor apareci√≥ un mensaje en letras doradas:\n",
      "\n",
      "**\"Bienvenido, te has convertido en el tejedor del c√≥digo arcano. Todo lo que escribas aqu√≠ cobrar√° vida. √ösalo con sabidur√≠a.\"**\n",
      "\n",
      "Mart√≠n pens√≥ que era alguna clase de broma o un virus sofisticado, pero su coraz√≥n lat√≠a con fuerza. Decidi√≥ probarlo. Escribi√≥ una l√≠nea simple:\n",
      "\n",
      "```python\n",
      "crear_objeto(\"taza_flotante\")\n",
      "```\n",
      "\n",
      "Para su asombro, una peque√±a taza de caf√© apareci√≥ flotando frente a √©l, girando lentamente en el aire. La dej√≥ caer en su mano, tibia y real. No pod√≠a creerlo. ¬øEra esto magia? ¬øTecnolog√≠a avanzada? ¬øUn sue√±o?\n",
      "\n",
      "Durante las semanas siguientes, Mart√≠n experiment√≥ con el c√≥digo. Descubri√≥ que pod√≠a crear objetos, alterar el clima en su apartamento e incluso sanar una cortada en su dedo con unas pocas l√≠neas cuidadosamente redactadas. Pero lo m√°s asombroso fue cuando intent√≥ escribir una funci√≥n que le permitiera entender a su gato, Copo. De repente, Copo comenz√≥ a hablarle, con una voz grave y sabia que desentonaba completamente con su apariencia esponjosa.\n",
      "\n",
      "El poder era indescriptible, pero tambi√©n aterrador. Mart√≠n pronto se dio cuenta de que cada l√≠nea de c√≥digo que escrib√≠a ten√≠a consecuencias impredecibles. Un d√≠a, cuando intent√≥ generar dinero para pagar sus deudas, el banco local sufri√≥ un inexplicable error en sus sistemas. Otro d√≠a, cuando escribi√≥ un programa para detener la lluvia, desencaden√≥ una sequ√≠a en toda la ciudad.\n",
      "\n",
      "Mart√≠n entendi√≥ que el c√≥digo arcano no era un juguete; era una herramienta que requer√≠a responsabilidad. As√≠ que decidi√≥ usarlo para ayudar al mundo, pero de manera cuidadosa. Cre√≥ peque√±os programas que reparaban carreteras, limpiaban los r√≠os y curaban enfermedades a nivel local. Todo lo hac√≠a en secreto, dejando que otros se llevaran el cr√©dito, porque sab√≠a que el poder absoluto nunca deb√≠a estar en manos de una sola persona.\n",
      "\n",
      "Con el tiempo, Mart√≠n se convirti√≥ en una figura misteriosa en la comunidad tecnol√≥gica. Nadie sab√≠a de d√≥nde ven√≠an sus soluciones milagrosas, pero todos lo admiraban. √âl, por su parte, guardaba el archivo \"arcano.py\" en un rinc√≥n oculto de su computadora, sabiendo que, aunque hab√≠a descubierto la magia en el c√≥digo, su mayor logro ser√≠a usarla con bondad y sabidur√≠a.\n",
      "\n",
      "Y as√≠, Mart√≠n se convirti√≥ no solo en un programador, sino en un mago del mundo moderno, tejedor de l√≠neas que cambiaban la realidad.\n",
      "--------------------------------------------------\n",
      "‚úì Streaming completado\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo b√°sico de streaming\n",
    "def streaming_basico():\n",
    "    prompt = \"Cu√©ntame una historia corta sobre un programador que descubre la magia en el c√≥digo\"\n",
    "    \n",
    "    print(\"=== STREAMING EN TIEMPO REAL ===\")\n",
    "    print(\"Generando respuesta...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # stream() devuelve un generador de chunks\n",
    "        for chunk in llm.stream([HumanMessage(content=prompt)]):\n",
    "            # Imprimir cada chunk sin nueva l√≠nea\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "            time.sleep(0.01)  # Peque√±a pausa para simular streaming visual\n",
    "            \n",
    "        print(\"\\n\" + \"-\" * 50)\n",
    "        print(\"‚úì Streaming completado\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error en streaming: {e}\")\n",
    "\n",
    "# Ejecutar streaming b√°sico\n",
    "streaming_basico()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6214408",
   "metadata": {},
   "source": [
    "## Comparaci√≥n: Streaming vs No-Streaming\n",
    "\n",
    "Veamos la diferencia en experiencia de usuario entre ambos enfoques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db8e5874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPARACI√ìN: STREAMING vs NO-STREAMING ===\\n\n",
      "1. SIN STREAMING:\n",
      "--------------------\n",
      "Esperando respuesta completa...\n",
      "\\n[Respuesta recibida despu√©s de 3.72 segundos]\n",
      "Python es un lenguaje de programaci√≥n ampliamente reconocido por su simplicidad y versatilidad, lo que lo convierte en una opci√≥n ideal tanto para principiantes como para desarrolladores experimentados. Una de sus principales ventajas es su sintaxis clara y legible, que permite escribir y entender c√≥digo de manera m√°s eficiente, reduciendo la curva de aprendizaje y facilitando la colaboraci√≥n en proyectos. Adem√°s, cuenta con una extensa biblioteca est√°ndar y una enorme cantidad de paquetes externos que abarcan √°reas como an√°lisis de datos, inteligencia artificial, desarrollo web, automatizaci√≥n y m√°s, lo que ahorra tiempo y esfuerzo en el desarrollo de soluciones. Python es tambi√©n un lenguaje multiplataforma, lo que significa que los programas escritos en √©l pueden ejecutarse en diversos sistemas operativos sin modificaciones significativas. Su comunidad activa y en constante crecimiento proporciona un valioso soporte, actualizaciones frecuentes y recursos educativos que enriquecen a√∫n m√°s la experiencia de aprendizaje y desarrollo. Estas caracter√≠sticas hacen de Python una herramienta poderosa y flexible para abordar una amplia variedad de proyectos y desaf√≠os tecnol√≥gicos.\n",
      "\\n============================================================\\n\n",
      "2. CON STREAMING:\n",
      "------------------\n",
      "Respuesta en tiempo real:\n",
      "Python es un lenguaje de programaci√≥n altamente valorado por su versatilidad, simplicidad y comunidad activa, lo que lo convierte en una herramienta poderosa tanto para principiantes como para expertos. Su sintaxis clara y legible facilita el aprendizaje y la escritura de c√≥digo, permitiendo a los desarrolladores centrarse en resolver problemas en lugar de luchar con la complejidad del lenguaje. Adem√°s, Python cuenta con una vasta biblioteca est√°ndar y una amplia gama de paquetes y frameworks de terceros que abarcan √°reas como ciencia de datos, inteligencia artificial, desarrollo web, automatizaci√≥n y m√°s, lo que ahorra tiempo y esfuerzo en la implementaci√≥n de soluciones. Su naturaleza multiplataforma permite que el c√≥digo escrito en Python se ejecute en diversos sistemas operativos sin modificaciones significativas. Por √∫ltimo, la comunidad global de Python proporciona soporte constante, recursos educativos y actualizaciones regulares, asegurando que el lenguaje siga evolucionando y adapt√°ndose a las necesidades del mundo tecnol√≥gico.\\n\\n[Streaming completado en 6.78 segundos]\n",
      "\\n============================================================\n",
      "OBSERVACIONES:\n",
      "- Sin streaming: El usuario espera sin feedback\n",
      "- Con streaming: El usuario ve progreso inmediato\n",
      "- Mejor percepci√≥n de velocidad con streaming\n",
      "- Streaming es especial para respuestas largas\n"
     ]
    }
   ],
   "source": [
    "# Comparaci√≥n entre streaming y no-streaming\n",
    "def comparar_streaming():\n",
    "    # Modelo sin streaming\n",
    "    llm_no_stream = ChatOpenAI(\n",
    "        base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "        api_key=os.getenv(\"GITHUB_TOKEN\"),\n",
    "        model=\"gpt-4o\",\n",
    "        streaming=False,  # Sin streaming\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    prompt = \"Escribe un p√°rrafo sobre las ventajas de la programaci√≥n en Python\"\n",
    "    \n",
    "    print(\"=== COMPARACI√ìN: STREAMING vs NO-STREAMING ===\\\\n\")\n",
    "    \n",
    "    # 1. Sin streaming\n",
    "    print(\"1. SIN STREAMING:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(\"Esperando respuesta completa...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = llm_no_stream.invoke([HumanMessage(content=prompt)])\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"\\\\n[Respuesta recibida despu√©s de {end_time - start_time:.2f} segundos]\")\n",
    "        print(response.content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60 + \"\\\\n\")\n",
    "    \n",
    "    # 2. Con streaming\n",
    "    print(\"2. CON STREAMING:\")\n",
    "    print(\"-\" * 18)\n",
    "    print(\"Respuesta en tiempo real:\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        for chunk in llm.stream([HumanMessage(content=prompt)]):\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "            time.sleep(0.03)  # Simular pausa para efecto visual\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"\\\\n\\\\n[Streaming completado en {end_time - start_time:.2f} segundos]\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"OBSERVACIONES:\")\n",
    "    print(\"- Sin streaming: El usuario espera sin feedback\")\n",
    "    print(\"- Con streaming: El usuario ve progreso inmediato\")\n",
    "    print(\"- Mejor percepci√≥n de velocidad con streaming\")\n",
    "    print(\"- Streaming es especial para respuestas largas\")\n",
    "\n",
    "# Ejecutar comparaci√≥n\n",
    "comparar_streaming()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725ad79d",
   "metadata": {},
   "source": [
    "## Implementaci√≥n de un Chatbot Simple con Streaming\n",
    "\n",
    "Creemos un chatbot b√°sico que demuestre el streaming en un contexto pr√°ctico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05a03cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHATBOT CON STREAMING ===\n",
      "Escribe 'salir' para terminar la conversaci√≥n\\n\n"
     ]
    }
   ],
   "source": [
    "# Chatbot simple con streaming\n",
    "def chatbot_streaming():\n",
    "    print(\"=== CHATBOT CON STREAMING ===\")\n",
    "    print(\"Escribe 'salir' para terminar la conversaci√≥n\\\\n\")\n",
    "    \n",
    "    # Configurar asistente con personalidad\n",
    "    system_message = \"\"\"Eres un asistente √∫til y amigable especializado en tecnolog√≠a. \n",
    "    Respondes de manera clara y concisa, y siempre intentas ser educativo.\"\"\"\n",
    "    \n",
    "    while True:\n",
    "        # Obtener input del usuario\n",
    "        user_input = input(\"\\\\nüßë T√∫: \")\n",
    "        \n",
    "        if user_input.lower() in ['salir', 'exit', 'quit']:\n",
    "            print(\"\\\\nüëã ¬°Hasta luego!\")\n",
    "            break\n",
    "            \n",
    "        if not user_input.strip():\n",
    "            continue\n",
    "            \n",
    "        print(\"\\\\nü§ñ Asistente: \", end=\"\", flush=True)\n",
    "        \n",
    "        try:\n",
    "            # Streaming de la respuesta\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_input}\n",
    "            ]\n",
    "            \n",
    "            # Convertir a formato LangChain\n",
    "            from langchain.schema import SystemMessage\n",
    "            lc_messages = [\n",
    "                SystemMessage(content=system_message),\n",
    "                HumanMessage(content=user_input)\n",
    "            ]\n",
    "            \n",
    "            full_response = \"\"\n",
    "            for chunk in llm.stream(lc_messages):\n",
    "                content = chunk.content\n",
    "                print(content, end=\"\", flush=True)\n",
    "                full_response += content\n",
    "                time.sleep(0.02)\n",
    "                \n",
    "            print()  # Nueva l√≠nea al final\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\\\n\\\\n‚è∏Ô∏è Interrumpido por el usuario\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\\\n‚ùå Error: {e}\")\n",
    "            \n",
    "    print(\"\\\\n¬°Gracias por usar el chatbot!\")\n",
    "\n",
    "# Ejecutar chatbot (¬°Pru√©balo!)\n",
    "chatbot_streaming() \n",
    " # Descomenta esta l√≠nea para ejecutar\n",
    "\n",
    "print(\"üí° Descomenta la l√≠nea anterior para probar el chatbot interactivo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6060e43c",
   "metadata": {},
   "source": [
    "## Streaming Avanzado con Manejo de Chunks\n",
    "\n",
    "Podemos procesar cada chunk individualmente para crear experiencias m√°s sofisticadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86fa1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming con an√°lisis de chunks\n",
    "def streaming_avanzado():\n",
    "    prompt = \"Explica qu√© es la inteligencia artificial y c√≥mo funciona el machine learning\"\n",
    "    \n",
    "    print(\"=== STREAMING AVANZADO CON AN√ÅLISIS ===\")\n",
    "    print(\"Analizando chunks conforme llegan...\\n\")\n",
    "    \n",
    "    # Variables para estad√≠sticas\n",
    "    chunk_count = 0\n",
    "    total_content = \"\"\n",
    "    words_processed = 0\n",
    "    \n",
    "    try:\n",
    "        for chunk in llm.stream([HumanMessage(content=prompt)]):\n",
    "            chunk_count += 1\n",
    "            content = chunk.content\n",
    "            total_content += content\n",
    "            \n",
    "            # Contar palabras aproximadas\n",
    "            if content.strip():\n",
    "                words_in_chunk = len(content.split())\n",
    "                words_processed += words_in_chunk\n",
    "            \n",
    "            # Mostrar progreso cada 10 chunks\n",
    "            if chunk_count % 10 == 0:\n",
    "                print(f\"\\\\n[Progreso: {chunk_count} chunks, ~{words_processed} palabras]\\\\n\")\n",
    "            \n",
    "            # Imprimir el contenido\n",
    "            print(content, end=\"\", flush=True)\n",
    "            time.sleep(0.02)  # Pausa ligeramente m√°s larga para ver el an√°lisis\n",
    "        \n",
    "        # Estad√≠sticas finales\n",
    "        print(f\"\\\\n\\\\n=== ESTAD√çSTICAS FINALES ===\")\n",
    "        print(f\"Total de chunks: {chunk_count}\")\n",
    "        print(f\"Palabras aproximadas: {words_processed}\")\n",
    "        print(f\"Caracteres totales: {len(total_content)}\")\n",
    "        print(f\"Promedio chars/chunk: {len(total_content)/chunk_count if chunk_count > 0 else 0:.1f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\\\n‚úó Error: {e}\")\n",
    "\n",
    "# Ejecutar streaming avanzado\n",
    "streaming_avanzado()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac251a9d",
   "metadata": {},
   "source": [
    "## Consideraciones T√©cnicas del Streaming\n",
    "\n",
    "### Cu√°ndo Usar Streaming:\n",
    "‚úÖ **S√ç usar streaming:**\n",
    "- Respuestas largas (>100 tokens)\n",
    "- Aplicaciones interactivas\n",
    "- Chatbots y asistentes\n",
    "- Demostraciones en vivo\n",
    "- Cuando la UX es prioritaria\n",
    "\n",
    "‚ùå **NO usar streaming:**\n",
    "- Respuestas muy cortas\n",
    "- Procesamiento batch\n",
    "- APIs de backend sin interfaz\n",
    "- Cuando necesitas la respuesta completa antes de procesar\n",
    "\n",
    "### Mejores Pr√°cticas:\n",
    "1. **Manejo de errores**: Siempre incluye try/catch\n",
    "2. **Indicadores visuales**: Muestra progreso al usuario\n",
    "3. **Cancelaci√≥n**: Permite al usuario interrumpir\n",
    "4. **Buffer management**: Para interfaces web, considera buffering\n",
    "5. **Performance**: Monitorea el uso de recursos\n",
    "\n",
    "## Ejercicios Pr√°cticos\n",
    "\n",
    "### Ejercicio 1: Indicador de Progreso\n",
    "Modifica el c√≥digo para mostrar un indicador de progreso (spinner, barra, porcentaje).\n",
    "\n",
    "### Ejercicio 2: Streaming con Filtros\n",
    "Implementa streaming que filtre o procese chunks espec√≠ficos (ej: resaltar palabras clave).\n",
    "\n",
    "### Ejercicio 3: Chatbot Mejorado\n",
    "Extiende el chatbot con:\n",
    "- Historial de conversaci√≥n\n",
    "- Comandos especiales (/help, /clear)\n",
    "- Diferentes personalidades\n",
    "\n",
    "## Conceptos Clave Aprendidos\n",
    "\n",
    "1. **Streaming** mejora la percepci√≥n de velocidad\n",
    "2. **Chunks** se procesan individualmente en tiempo real\n",
    "3. **UX** es significativamente mejor con streaming\n",
    "4. **Implementaci√≥n** requiere manejo cuidadoso de generadores\n",
    "5. **Casos de uso** espec√≠ficos donde streaming aporta valor\n",
    "\n",
    "## Pr√≥ximos Pasos\n",
    "\n",
    "En el siguiente notebook exploraremos la **memoria en LangChain**, que nos permite mantener contexto entre m√∫ltiples interacciones del usuario."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
