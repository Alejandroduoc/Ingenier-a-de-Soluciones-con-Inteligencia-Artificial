{
 "cells": [
  {
   "cell_type": "code",
   "id": "c8864c5e",
   "metadata": {},
   "outputs": [],
   "source": "# Configuraci√≥n del modelo para memoria\ntry:\n    llm = ChatOpenAI(\n        base_url=os.getenv(\"OPENAI_BASE_URL\"),\n        api_key=os.getenv(\"GITHUB_TOKEN\"),\n        model=\"gpt-4o\",\n        temperature=0.7\n    )\n    \n    print(\"‚úì Modelo configurado para experimentos de memoria\")\n    print(f\"Modelo: {llm.model_name}\")\n    \nexcept Exception as e:\n    print(f\"‚úó Error en configuraci√≥n: {e}\")\n    print(\"Verifica las variables de entorno\")"
  },
  {
   "cell_type": "markdown",
   "id": "7ik68dw996b",
   "source": "## Consideraciones T√©cnicas y Mejores Pr√°cticas\n\n### Selecci√≥n del Tipo de Memoria\n\n| Tipo | Cu√°ndo Usarlo | Ventajas | Desventajas |\n|------|---------------|----------|-------------|\n| **Buffer** | Conversaciones cortas | Contexto completo | Alto consumo de tokens |\n| **Window** | Contexto reciente importante | Eficiente en tokens | Puede perder informaci√≥n clave |\n| **Summary** | Conversaciones muy largas | Balance eficiencia/contexto | P√©rdida de detalles espec√≠ficos |\n\n### Mejores Pr√°cticas:\n\n1. **Gesti√≥n de Tokens**:\n   - Monitorea el uso de tokens regularmente\n   - Establece l√≠mites m√°ximos para evitar costos excesivos\n   - Considera el costo vs. calidad del contexto\n\n2. **Selecci√≥n Estrat√©gica**:\n   - Usa Buffer para sesiones cortas e importantes\n   - Usa Window para conversaciones con contexto limitado\n   - Usa Summary para sesiones largas de asistencia\n\n3. **Optimizaci√≥n**:\n   - Limpia memoria peri√≥dicamente si es necesario\n   - Implementa estrategias h√≠bridas seg√∫n el caso de uso\n   - Considera almacenamiento persistente para memoria a largo plazo\n\n## Ejercicios Pr√°cticos\n\n### Ejercicio 1: An√°lisis de Consumo\nImplementa un sistema que monitoree y reporte el uso de tokens con diferentes tipos de memoria.\n\n### Ejercicio 2: Memoria H√≠brida\nDise√±a una estrategia que combine multiple tipos de memoria seg√∫n el contexto.\n\n### Ejercicio 3: Persistencia\nExtiende el chatbot para guardar y cargar memoria entre sesiones.\n\n## Conceptos Clave Aprendidos\n\n1. **Importancia de la memoria** en conversaciones naturales\n2. **Tipos de memoria** y sus casos de uso espec√≠ficos\n3. **Balance** entre contexto y eficiencia de tokens\n4. **Implementaci√≥n pr√°ctica** con LangChain\n5. **Estrategias de optimizaci√≥n** para diferentes escenarios\n\n## Conclusi√≥n del M√≥dulo IL1.1\n\nHas completado la introducci√≥n a LLMs y conexiones API. Los conceptos aprendidos:\n\n1. **APIs directas** vs **frameworks** como LangChain\n2. **Streaming** para mejor experiencia de usuario\n3. **Memoria** para conversaciones contextuales\n4. **Mejores pr√°cticas** de seguridad y optimizaci√≥n\n\n### Pr√≥ximos Pasos\nEn **IL1.2** exploraremos t√©cnicas avanzadas de **prompt engineering** incluyendo zero-shot, few-shot, y chain-of-thought prompting.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "m9p1wxqs70j",
   "source": "# Chatbot avanzado con memoria configurable\ndef chatbot_con_memoria():\n    print(\"=== CHATBOT CON MEMORIA CONFIGURABLE ===\")\n    print(\"Tipos disponibles:\")\n    print(\"1. buffer - Mantiene todo el historial\")\n    print(\"2. window - Mantiene solo N mensajes recientes\") \n    print(\"3. summary - Resume conversaciones largas\")\n    print(\"\\\\nEscribe 'cambiar' para cambiar tipo de memoria\")\n    print(\"Escribe 'memoria' para ver el contenido actual\")\n    print(\"Escribe 'salir' para terminar\\\\n\")\n    \n    # Configuraci√≥n inicial\n    memory_type = \"buffer\"\n    memory = ConversationBufferMemory()\n    conversation = ConversationChain(llm=llm, memory=memory, verbose=False)\n    \n    def crear_memoria(tipo):\n        if tipo == \"buffer\":\n            return ConversationBufferMemory()\n        elif tipo == \"window\":\n            return ConversationBufferWindowMemory(k=3)\n        elif tipo == \"summary\":\n            return ConversationSummaryMemory(llm=llm)\n        else:\n            return ConversationBufferMemory()\n    \n    print(f\"Memoria actual: {memory_type}\")\n    \n    while True:\n        user_input = input(\"\\\\nüßë T√∫: \")\n        \n        if user_input.lower() == 'salir':\n            print(\"\\\\nüëã ¬°Hasta luego!\")\n            break\n            \n        elif user_input.lower() == 'cambiar':\n            print(\"\\\\nTipos disponibles: buffer, window, summary\")\n            nuevo_tipo = input(\"Nuevo tipo de memoria: \").lower()\n            \n            if nuevo_tipo in ['buffer', 'window', 'summary']:\n                memory_type = nuevo_tipo\n                memory = crear_memoria(memory_type)\n                conversation = ConversationChain(llm=llm, memory=memory, verbose=False)\n                print(f\"‚úì Memoria cambiada a: {memory_type}\")\n                print(\"‚ö†Ô∏è Historial de conversaci√≥n reiniciado\")\n            else:\n                print(\"‚ùå Tipo no v√°lido\")\n            continue\n            \n        elif user_input.lower() == 'memoria':\n            print(f\"\\\\n=== MEMORIA ACTUAL ({memory_type}) ===\")\n            print(f\"Contenido: {memory.buffer}\")\n            print(f\"Longitud: {len(memory.buffer)} caracteres\")\n            continue\n            \n        elif not user_input.strip():\n            continue\n        \n        try:\n            print(f\"\\\\nü§ñ Asistente ({memory_type}): \", end=\"\", flush=True)\n            response = conversation.predict(input=user_input)\n            print(response)\n            \n        except Exception as e:\n            print(f\"‚ùå Error: {e}\")\n\n# Funci√≥n para ejecutar el chatbot\n# chatbot_con_memoria()  # Descomenta para ejecutar\n\nprint(\"üí° Descomenta la l√≠nea anterior para probar el chatbot con memoria configurable\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7bhpf37fqml",
   "source": "## Chatbot Avanzado con Memoria Personalizable\n\nImplementemos un chatbot que permite al usuario elegir el tipo de memoria.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "sxpuqwyqc3d",
   "source": "# Comparaci√≥n entre tipos de memoria\ndef comparar_memorias():\n    print(\"=== COMPARACI√ìN DE TIPOS DE MEMORIA ===\\\\n\")\n    \n    # Configurar diferentes tipos de memoria\n    buffer_memory = ConversationBufferMemory()\n    window_memory = ConversationBufferWindowMemory(k=2)\n    summary_memory = ConversationSummaryMemory(llm=llm)\n    \n    # Crear conversaciones con cada tipo\n    conversations = {\n        \"Buffer (Todo)\": ConversationChain(llm=llm, memory=buffer_memory, verbose=False),\n        \"Window (k=2)\": ConversationChain(llm=llm, memory=window_memory, verbose=False),\n        \"Summary\": ConversationChain(llm=llm, memory=summary_memory, verbose=False)\n    }\n    \n    # Secuencia de inputs para probar\n    test_inputs = [\n        \"Mi nombre es Alex y estudio ingenier√≠a inform√°tica\",\n        \"Tengo 22 a√±os y me especializo en IA\",\n        \"Mi lenguaje favorito es Python\",\n        \"Tambi√©n me gusta JavaScript para desarrollo web\",\n        \"¬øCu√°l es mi edad y carrera?\"  # Pregunta que requiere memoria\n    ]\n    \n    # Ejecutar la misma conversaci√≥n con cada tipo de memoria\n    for memory_type, conversation in conversations.items():\n        print(f\"\\\\n{'='*20} {memory_type.upper()} {'='*20}\")\n        \n        for i, user_input in enumerate(test_inputs, 1):\n            try:\n                if i < len(test_inputs):  # No mostrar la pregunta final a√∫n\n                    response = conversation.predict(input=user_input)\n                    print(f\"{i}. Input: {user_input}\")\n                    print(f\"   Respuesta: {response[:100]}...\")\n                else:  # Pregunta final para probar memoria\n                    response = conversation.predict(input=user_input)\n                    print(f\"\\\\n{i}. PREGUNTA DE MEMORIA: {user_input}\")\n                    print(f\"   RESPUESTA: {response}\")\n                    \n                    # Mostrar estado de memoria\n                    if memory_type == \"Buffer (Todo)\":\n                        print(f\"   Memoria: {len(buffer_memory.buffer)} caracteres\")\n                    elif memory_type == \"Window (k=2)\":\n                        print(f\"   Memoria: {len(window_memory.buffer)} caracteres\")\n                    else:\n                        print(f\"   Resumen: {len(summary_memory.buffer)} caracteres\")\n                        \n            except Exception as e:\n                print(f\"   Error: {e}\")\n        \n        print(\"\\\\n\" + \"-\"*60)\n    \n    print(\"\\\\n=== AN√ÅLISIS ===\")\n    print(\"‚Ä¢ Buffer Memory: Recuerda todo pero consume m√°s tokens\")\n    print(\"‚Ä¢ Window Memory: Eficiente pero puede olvidar informaci√≥n importante\")  \n    print(\"‚Ä¢ Summary Memory: Balance entre memoria y eficiencia\")\n\n# Ejecutar comparaci√≥n\ncomparar_memorias()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "14h7w98kyppc",
   "source": "## Comparaci√≥n de Tipos de Memoria\n\nVeamos las diferencias entre los tipos de memoria en una misma conversaci√≥n.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cx47oerjh3g",
   "source": "# Ejemplo con ConversationSummaryMemory\ndef ejemplo_summary_memory():\n    print(\"=== CONVERSATIONSUMMARYMEMORY ===\")\n    print(\"Resume conversaciones largas para ahorrar tokens\\\\n\")\n    \n    try:\n        # Crear memoria con resumen\n        memory = ConversationSummaryMemory(llm=llm)\n        \n        conversation = ConversationChain(\n            llm=llm,\n            memory=memory,\n            verbose=True\n        )\n        \n        # Simular una conversaci√≥n larga con muchos detalles\n        long_inputs = [\n            \"Hola, me llamo Mar√≠a Gonz√°lez, tengo 35 a√±os y soy ingeniera de software especializada en desarrollo web con React y Node.js. Trabajo en una startup de fintech en Madrid desde hace 3 a√±os.\",\n            \"Mi proyecto actual involucra crear una plataforma de pagos digitales que debe manejar transacciones en tiempo real con alta seguridad. Usamos microservicios con Docker y Kubernetes.\",\n            \"El mayor desaf√≠o t√©cnico que enfrentamos es la latencia en las transacciones internacionales. Estamos considerando implementar edge computing y optimizar nuestras APIs.\",\n            \"Tambi√©n estoy trabajando en mejorar la experiencia de usuario de nuestra aplicaci√≥n m√≥vil. Los usuarios se quejan de que el proceso de verificaci√≥n de identidad es muy lento.\",\n            \"¬øPuedes resumir qui√©n soy y cu√°les son mis principales desaf√≠os profesionales?\"\n        ]\n        \n        for i, user_input in enumerate(long_inputs, 1):\n            print(f\"{i}. Input: {user_input[:100]}...\")\n            response = conversation.predict(input=user_input)\n            print(f\"   Respuesta: {response}\\\\n\")\n            \n            # Mostrar el resumen actual\n            print(\"   Resumen actual en memoria:\")\n            print(f\"   {memory.buffer}\\\\n\")\n            print(\"-\" * 80)\n            \n    except Exception as e:\n        print(f\"Error: {e}\")\n\n# Ejecutar ejemplo\nejemplo_summary_memory()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9uv0f1q0yvd",
   "source": "## 3. ConversationSummaryMemory - Resumen Inteligente\n\nEsta memoria **resume** conversaciones largas en lugar de mantener todo el texto completo, ahorrando tokens significativamente.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "gl63nm8s8al",
   "source": "# Ejemplo con ConversationBufferWindowMemory\ndef ejemplo_window_memory():\n    print(\"=== CONVERSATIONBUFFERWINDOWMEMORY ===\")\n    print(\"Mantiene solo los 2 intercambios m√°s recientes\\\\n\")\n    \n    try:\n        # Crear memoria con ventana de 2 intercambios\n        memory = ConversationBufferWindowMemory(k=2)\n        \n        conversation = ConversationChain(\n            llm=llm,\n            memory=memory,\n            verbose=True\n        )\n        \n        # M√∫ltiples interacciones para ver el efecto de la ventana\n        inputs = [\n            \"Mi nombre es Carlos y tengo 30 a√±os\",\n            \"Trabajo como dise√±ador gr√°fico\",\n            \"Me gusta el caf√© y la m√∫sica jazz\",\n            \"¬øPuedes recordar mi edad?\",  # Deber√≠a olvidar esto\n            \"¬øCu√°l es mi profesi√≥n?\"  # Deber√≠a recordar esto\n        ]\n        \n        for i, user_input in enumerate(inputs, 1):\n            print(f\"{i}. Pregunta: {user_input}\")\n            response = conversation.predict(input=user_input)\n            print(f\"   Respuesta: {response}\\\\n\")\n            \n            # Mostrar contenido actual de la memoria\n            print(f\"   Memoria actual (k=2):\")\n            print(f\"   {memory.buffer}\\\\n\")\n            print(\"-\" * 60)\n            \n    except Exception as e:\n        print(f\"Error: {e}\")\n\n# Ejecutar ejemplo\nejemplo_window_memory()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "nu8lm8qjsqp",
   "source": "## 2. ConversationBufferWindowMemory - Ventana Deslizante\n\nEsta memoria mantiene solo los **N mensajes m√°s recientes**, √∫til para controlar el uso de tokens.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "tspyxev5dx",
   "source": "# Ejemplo b√°sico con ConversationBufferMemory\ndef ejemplo_buffer_memory():\n    print(\"=== CONVERSATIONBUFFERMEMORY ===\")\n    print(\"Mantiene todo el historial de conversaci√≥n\\\\n\")\n    \n    try:\n        # Crear memoria buffer\n        memory = ConversationBufferMemory()\n        \n        # Crear cadena de conversaci√≥n\n        conversation = ConversationChain(\n            llm=llm,\n            memory=memory,\n            verbose=True  # Muestra el prompt interno\n        )\n        \n        # Primera interacci√≥n\n        print(\"1. Primera pregunta:\")\n        response1 = conversation.predict(input=\"Mi nombre es Ana y soy programadora Python\")\n        print(f\"Respuesta: {response1}\\\\n\")\n        \n        # Segunda interacci√≥n (debe recordar el nombre)\n        print(\"2. Segunda pregunta:\")\n        response2 = conversation.predict(input=\"¬øCu√°l es mi nombre y profesi√≥n?\")\n        print(f\"Respuesta: {response2}\\\\n\")\n        \n        # Tercera interacci√≥n (debe recordar todo el contexto)\n        print(\"3. Tercera pregunta:\")\n        response3 = conversation.predict(input=\"¬øQu√© lenguaje de programaci√≥n mencion√©?\")\n        print(f\"Respuesta: {response3}\\\\n\")\n        \n        # Examinar el contenido de la memoria\n        print(\"=== CONTENIDO DE LA MEMORIA ===\")\n        print(memory.buffer)\n        \n    except Exception as e:\n        print(f\"Error: {e}\")\n\n# Ejecutar ejemplo\nejemplo_buffer_memory()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ylmlzyouuf",
   "source": "## 1. ConversationBufferMemory - Memoria Completa\n\nEsta memoria mantiene **todo** el historial de la conversaci√≥n. Es la m√°s simple pero puede consumir muchos tokens.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "t64g2qcwgh",
   "source": "# Importar bibliotecas necesarias para memoria\nfrom langchain_openai import ChatOpenAI\nfrom langchain.memory import (\n    ConversationBufferMemory,\n    ConversationSummaryMemory,\n    ConversationBufferWindowMemory\n)\nfrom langchain.chains import ConversationChain\nimport os\n\nprint(\"‚úì Bibliotecas de memoria importadas correctamente\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bmsvkxtyalo",
   "source": "# 4. LangChain Memory - Gesti√≥n de Contexto Conversacional\n\n## Objetivos de Aprendizaje\n- Comprender la importancia de la memoria en conversaciones con LLMs\n- Implementar diferentes tipos de memoria con LangChain\n- Gestionar el contexto de conversaciones largas\n- Optimizar el uso de tokens con estrategias de memoria\n\n## ¬øPor qu√© es Importante la Memoria?\n\nLos LLMs son **stateless** por naturaleza: no recuerdan conversaciones anteriores. La memoria permite:\n- **Contexto conversacional**: Referirse a mensajes anteriores\n- **Personalizaci√≥n**: Recordar preferencias del usuario\n- **Continuidad**: Mantener hilos de conversaci√≥n coherentes\n- **Experiencia natural**: Conversaciones que se sienten humanas\n\n## Tipos de Memoria en LangChain\n\n1. **ConversationBufferMemory**: Mantiene todo el historial\n2. **ConversationSummaryMemory**: Resume conversaciones largas\n3. **ConversationBufferWindowMemory**: Mantiene solo los N mensajes m√°s recientes\n4. **ConversationSummaryBufferMemory**: Combina resumen + buffer reciente",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}