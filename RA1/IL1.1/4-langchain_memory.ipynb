{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bmsvkxtyalo",
   "metadata": {},
   "source": [
    "# 4. LangChain Memory - Gestión de Contexto Conversacional\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Comprender la importancia de la memoria en conversaciones con LLMs\n",
    "- Implementar diferentes tipos de memoria con LangChain\n",
    "- Gestionar el contexto de conversaciones largas\n",
    "- Optimizar el uso de tokens con estrategias de memoria\n",
    "\n",
    "## ¿Por qué es Importante la Memoria?\n",
    "\n",
    "Los LLMs son **stateless** por naturaleza: no recuerdan conversaciones anteriores. La memoria permite:\n",
    "- **Contexto conversacional**: Referirse a mensajes anteriores\n",
    "- **Personalización**: Recordar preferencias del usuario\n",
    "- **Continuidad**: Mantener hilos de conversación coherentes\n",
    "- **Experiencia natural**: Conversaciones que se sienten humanas\n",
    "\n",
    "## Tipos de Memoria en LangChain\n",
    "\n",
    "1. **ConversationBufferMemory**: Mantiene todo el historial\n",
    "2. **ConversationSummaryMemory**: Resume conversaciones largas\n",
    "3. **ConversationBufferWindowMemory**: Mantiene solo los N mensajes más recientes\n",
    "4. **ConversationSummaryBufferMemory**: Combina resumen + buffer reciente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4ac0c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Bibliotecas de memoria importadas correctamente\n"
     ]
    }
   ],
   "source": [
    "# Importar bibliotecas necesarias para memoria\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationSummaryMemory,\n",
    "    ConversationBufferWindowMemory\n",
    ")\n",
    "from langchain.chains import ConversationChain\n",
    "import os\n",
    "\n",
    "print(\"✓ Bibliotecas de memoria importadas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f8f3673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Modelo configurado para experimentos de memoria\n",
      "Modelo: gpt-4o\n"
     ]
    }
   ],
   "source": [
    "# Configuración del modelo para memoria\n",
    "try:\n",
    "    llm = ChatOpenAI(\n",
    "        base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "        api_key=os.getenv(\"GITHUB_TOKEN\"),\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Modelo configurado para experimentos de memoria\")\n",
    "    print(f\"Modelo: {llm.model_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error en configuración: {e}\")\n",
    "    print(\"Verifica las variables de entorno\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc98272",
   "metadata": {},
   "source": [
    "## 1. ConversationBufferMemory - Memoria Completa\n",
    "\n",
    "Esta memoria mantiene **todo** el historial de la conversación. Es la más simple pero puede consumir muchos tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6fb2837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONVERSATIONBUFFERMEMORY ===\n",
      "Mantiene todo el historial de conversación\\n\n",
      "1. Primera pregunta:\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Mi nombre es Ana y soy programadora Python\n",
      "AI:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alejandro\\AppData\\Local\\Temp\\ipykernel_14108\\2947721021.py:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "C:\\Users\\Alejandro\\AppData\\Local\\Temp\\ipykernel_14108\\2947721021.py:11: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta: ¡Hola, Ana! Es un gusto conocerte. Python es un lenguaje fantástico, muy versátil y usado en una amplia variedad de áreas como desarrollo web, ciencia de datos, inteligencia artificial y automatización. ¿Qué tipo de proyectos o áreas te interesan más como programadora Python? ¡Me encantaría saber más sobre lo que haces!\\n\n",
      "2. Segunda pregunta:\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Mi nombre es Ana y soy programadora Python\n",
      "AI: ¡Hola, Ana! Es un gusto conocerte. Python es un lenguaje fantástico, muy versátil y usado en una amplia variedad de áreas como desarrollo web, ciencia de datos, inteligencia artificial y automatización. ¿Qué tipo de proyectos o áreas te interesan más como programadora Python? ¡Me encantaría saber más sobre lo que haces!\n",
      "Human: ¿Cuál es mi nombre y profesión?\n",
      "AI:\u001b[0m\n",
      "Error: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo básico con ConversationBufferMemory\n",
    "def ejemplo_buffer_memory():\n",
    "    print(\"=== CONVERSATIONBUFFERMEMORY ===\")\n",
    "    print(\"Mantiene todo el historial de conversación\\\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Crear memoria buffer\n",
    "        memory = ConversationBufferMemory()\n",
    "        \n",
    "        # Crear cadena de conversación\n",
    "        conversation = ConversationChain(\n",
    "            llm=llm,\n",
    "            memory=memory,\n",
    "            verbose=True  # Muestra el prompt interno\n",
    "        )\n",
    "        \n",
    "        # Primera interacción\n",
    "        print(\"1. Primera pregunta:\")\n",
    "        response1 = conversation.predict(input=\"Mi nombre es Ana y soy programadora Python\")\n",
    "        print(f\"Respuesta: {response1}\\\\n\")\n",
    "        \n",
    "        # Segunda interacción (debe recordar el nombre)\n",
    "        print(\"2. Segunda pregunta:\")\n",
    "        response2 = conversation.predict(input=\"¿Cuál es mi nombre y profesión?\")\n",
    "        print(f\"Respuesta: {response2}\\\\n\")\n",
    "        \n",
    "        # Tercera interacción (debe recordar todo el contexto)\n",
    "        print(\"3. Tercera pregunta:\")\n",
    "        response3 = conversation.predict(input=\"¿Qué lenguaje de programación mencioné?\")\n",
    "        print(f\"Respuesta: {response3}\\\\n\")\n",
    "        \n",
    "        # Examinar el contenido de la memoria\n",
    "        print(\"=== CONTENIDO DE LA MEMORIA ===\")\n",
    "        print(memory.buffer)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Ejecutar ejemplo\n",
    "ejemplo_buffer_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1acba3",
   "metadata": {},
   "source": [
    "## 2. ConversationBufferWindowMemory - Ventana Deslizante\n",
    "\n",
    "Esta memoria mantiene solo los **N mensajes más recientes**, útil para controlar el uso de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46248c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONVERSATIONSUMMARYMEMORY ===\n",
      "Resume conversaciones largas para ahorrar tokens\\n\n",
      "1. Input: Hola, me llamo María González, tengo 35 años y soy ingeniera de software especializada en desarrollo...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hola, me llamo María González, tengo 35 años y soy ingeniera de software especializada en desarrollo web con React y Node.js. Trabajo en una startup de fintech en Madrid desde hace 3 años.\n",
      "AI:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alejandro\\AppData\\Local\\Temp\\ipykernel_14108\\1771842198.py:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Rate limit of 50 per 86400s exceeded for UserByModelByDay. Please wait 79786 seconds before retrying.', 'details': 'Rate limit of 50 per 86400s exceeded for UserByModelByDay. Please wait 79786 seconds before retrying.'}}\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo con ConversationSummaryMemory\n",
    "def ejemplo_summary_memory():\n",
    "    print(\"=== CONVERSATIONSUMMARYMEMORY ===\")\n",
    "    print(\"Resume conversaciones largas para ahorrar tokens\\\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Crear memoria con resumen\n",
    "        memory = ConversationSummaryMemory(llm=llm)\n",
    "        \n",
    "        conversation = ConversationChain(\n",
    "            llm=llm,\n",
    "            memory=memory,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Simular una conversación larga con muchos detalles\n",
    "        long_inputs = [\n",
    "            \"Hola, me llamo María González, tengo 35 años y soy ingeniera de software especializada en desarrollo web con React y Node.js. Trabajo en una startup de fintech en Madrid desde hace 3 años.\",\n",
    "            \"Mi proyecto actual involucra crear una plataforma de pagos digitales que debe manejar transacciones en tiempo real con alta seguridad. Usamos microservicios con Docker y Kubernetes.\",\n",
    "            \"El mayor desafío técnico que enfrentamos es la latencia en las transacciones internacionales. Estamos considerando implementar edge computing y optimizar nuestras APIs.\",\n",
    "            \"También estoy trabajando en mejorar la experiencia de usuario de nuestra aplicación móvil. Los usuarios se quejan de que el proceso de verificación de identidad es muy lento.\",\n",
    "            \"¿Puedes resumir quién soy y cuáles son mis principales desafíos profesionales?\"\n",
    "        ]\n",
    "        \n",
    "        for i, user_input in enumerate(long_inputs, 1):\n",
    "            print(f\"{i}. Input: {user_input[:100]}...\")\n",
    "            response = conversation.predict(input=user_input)\n",
    "            print(f\"   Respuesta: {response}\\\\n\")\n",
    "            \n",
    "            # Mostrar el resumen actual\n",
    "            print(\"   Resumen actual en memoria:\")\n",
    "            print(f\"   {memory.buffer}\\\\n\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Ejecutar ejemplo\n",
    "ejemplo_summary_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2195b2",
   "metadata": {},
   "source": [
    "## 3. ConversationSummaryMemory - Resumen Inteligente\n",
    "\n",
    "Esta memoria **resume** conversaciones largas en lugar de mantener todo el texto completo, ahorrando tokens significativamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51070f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo con ConversationBufferWindowMemory\n",
    "def ejemplo_window_memory():\n",
    "    print(\"=== CONVERSATIONBUFFERWINDOWMEMORY ===\")\n",
    "    print(\"Mantiene solo los 2 intercambios más recientes\\\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Crear memoria con ventana de 2 intercambios\n",
    "        memory = ConversationBufferWindowMemory(k=2)\n",
    "        \n",
    "        conversation = ConversationChain(\n",
    "            llm=llm,\n",
    "            memory=memory,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Múltiples interacciones para ver el efecto de la ventana\n",
    "        inputs = [\n",
    "            \"Mi nombre es Carlos y tengo 30 años\",\n",
    "            \"Trabajo como diseñador gráfico\",\n",
    "            \"Me gusta el café y la música jazz\",\n",
    "            \"¿Puedes recordar mi edad?\",  # Debería olvidar esto\n",
    "            \"¿Cuál es mi profesión?\"  # Debería recordar esto\n",
    "        ]\n",
    "        \n",
    "        for i, user_input in enumerate(inputs, 1):\n",
    "            print(f\"{i}. Pregunta: {user_input}\")\n",
    "            response = conversation.predict(input=user_input)\n",
    "            print(f\"   Respuesta: {response}\\\\n\")\n",
    "            \n",
    "            # Mostrar contenido actual de la memoria\n",
    "            print(f\"   Memoria actual (k=2):\")\n",
    "            print(f\"   {memory.buffer}\\\\n\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Ejecutar ejemplo\n",
    "ejemplo_window_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697af2f4",
   "metadata": {},
   "source": [
    "## Comparación de Tipos de Memoria\n",
    "\n",
    "Veamos las diferencias entre los tipos de memoria en una misma conversación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c52e63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPARACIÓN DE TIPOS DE MEMORIA ===\\n\n",
      "\\n==================== BUFFER (TODO) ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alejandro\\AppData\\Local\\Temp\\ipykernel_11884\\4102629620.py:7: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  window_memory = ConversationBufferWindowMemory(k=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Input: Mi nombre es Alex y estudio ingeniería informática\n",
      "   Respuesta: ¡Hola, Alex! Es un gusto conocerte. La ingeniería informática es un campo fascinante y lleno de reto...\n",
      "2. Input: Tengo 22 años y me especializo en IA\n",
      "   Respuesta: ¡Eso es genial, Alex! A los 22 años ya estás especializándote en inteligencia artificial, ¡un área q...\n",
      "3. Input: Mi lenguaje favorito es Python\n",
      "   Respuesta: ¡Excelente elección, Alex! Python es un lenguaje increíblemente versátil y una de las herramientas m...\n",
      "4. Input: También me gusta JavaScript para desarrollo web\n",
      "   Respuesta: ¡Eso es fantástico, Alex! Python para inteligencia artificial y JavaScript para desarrollo web es un...\n",
      "\\n5. PREGUNTA DE MEMORIA: ¿Cuál es mi edad y carrera?\n",
      "   RESPUESTA: ¡Claro, Alex! Me dijiste que tienes **22 años** y que estás estudiando **ingeniería informática**. Además, mencionaste que te estás especializando en **inteligencia artificial**, lo cual es súper emocionante. También me contaste que tus lenguajes favoritos son **Python**, especialmente para trabajar en IA, y **JavaScript** para el desarrollo web. ¡Tienes un perfil muy interesante y versátil! 😊\n",
      "   Memoria: 5347 caracteres\n",
      "\\n------------------------------------------------------------\n",
      "\\n==================== WINDOW (K=2) ====================\n",
      "1. Input: Mi nombre es Alex y estudio ingeniería informática\n",
      "   Respuesta: ¡Hola, Alex! Encantado de conocerte. Ingeniería informática suena como una carrera fascinante y desa...\n",
      "2. Input: Tengo 22 años y me especializo en IA\n",
      "   Respuesta: ¡Qué interesante, Alex! A los 22 años y ya especializado en inteligencia artificial, ¡eso es impresi...\n",
      "3. Input: Mi lenguaje favorito es Python\n",
      "   Respuesta: ¡Excelente elección, Alex! Python es, sin duda, uno de los lenguajes más populares y versátiles, esp...\n",
      "4. Input: También me gusta JavaScript para desarrollo web\n",
      "   Respuesta: ¡Eso suena genial, Alex! JavaScript es una elección fantástica para el desarrollo web. Es uno de los...\n",
      "\\n5. PREGUNTA DE MEMORIA: ¿Cuál es mi edad y carrera?\n",
      "   RESPUESTA: No estoy completamente seguro, Alex, ya que no me has proporcionado esa información directamente en nuestra conversación. Pero si quieres compartirlo, estaré encantado de saber más sobre ti y adaptar nuestras charlas a tus intereses y conocimientos. 😊 Por ejemplo, si me cuentas tu carrera, podría sugerirte proyectos o ideas relacionadas, ya sea con Python, JavaScript o cualquier otra cosa que te interese. ¡Cuéntame más sobre ti!\n",
      "   Memoria: 2235 caracteres\n",
      "\\n------------------------------------------------------------\n",
      "\\n==================== SUMMARY ====================\n",
      "1. Input: Mi nombre es Alex y estudio ingeniería informática\n",
      "   Respuesta: ¡Hola, Alex! Encantado de conocerte. Estudiar ingeniería informática suena fascinante y lleno de des...\n"
     ]
    }
   ],
   "source": [
    "# Comparación entre tipos de memoria\n",
    "def comparar_memorias():\n",
    "    print(\"=== COMPARACIÓN DE TIPOS DE MEMORIA ===\\\\n\")\n",
    "    \n",
    "    # Configurar diferentes tipos de memoria\n",
    "    buffer_memory = ConversationBufferMemory()\n",
    "    window_memory = ConversationBufferWindowMemory(k=2)\n",
    "    summary_memory = ConversationSummaryMemory(llm=llm)\n",
    "    \n",
    "    # Crear conversaciones con cada tipo\n",
    "    conversations = {\n",
    "        \"Buffer (Todo)\": ConversationChain(llm=llm, memory=buffer_memory, verbose=False),\n",
    "        \"Window (k=2)\": ConversationChain(llm=llm, memory=window_memory, verbose=False),\n",
    "        \"Summary\": ConversationChain(llm=llm, memory=summary_memory, verbose=False)\n",
    "    }\n",
    "    \n",
    "    # Secuencia de inputs para probar\n",
    "    test_inputs = [\n",
    "        \"Mi nombre es Alex y estudio ingeniería informática\",\n",
    "        \"Tengo 22 años y me especializo en IA\",\n",
    "        \"Mi lenguaje favorito es Python\",\n",
    "        \"También me gusta JavaScript para desarrollo web\",\n",
    "        \"¿Cuál es mi edad y carrera?\"  # Pregunta que requiere memoria\n",
    "    ]\n",
    "    \n",
    "    # Ejecutar la misma conversación con cada tipo de memoria\n",
    "    for memory_type, conversation in conversations.items():\n",
    "        print(f\"\\\\n{'='*20} {memory_type.upper()} {'='*20}\")\n",
    "        \n",
    "        for i, user_input in enumerate(test_inputs, 1):\n",
    "            try:\n",
    "                if i < len(test_inputs):  # No mostrar la pregunta final aún\n",
    "                    response = conversation.predict(input=user_input)\n",
    "                    print(f\"{i}. Input: {user_input}\")\n",
    "                    print(f\"   Respuesta: {response[:100]}...\")\n",
    "                else:  # Pregunta final para probar memoria\n",
    "                    response = conversation.predict(input=user_input)\n",
    "                    print(f\"\\\\n{i}. PREGUNTA DE MEMORIA: {user_input}\")\n",
    "                    print(f\"   RESPUESTA: {response}\")\n",
    "                    \n",
    "                    # Mostrar estado de memoria\n",
    "                    if memory_type == \"Buffer (Todo)\":\n",
    "                        print(f\"   Memoria: {len(buffer_memory.buffer)} caracteres\")\n",
    "                    elif memory_type == \"Window (k=2)\":\n",
    "                        print(f\"   Memoria: {len(window_memory.buffer)} caracteres\")\n",
    "                    else:\n",
    "                        print(f\"   Resumen: {len(summary_memory.buffer)} caracteres\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"   Error: {e}\")\n",
    "        \n",
    "        print(\"\\\\n\" + \"-\"*60)\n",
    "    \n",
    "    print(\"\\\\n=== ANÁLISIS ===\")\n",
    "    print(\"• Buffer Memory: Recuerda todo pero consume más tokens\")\n",
    "    print(\"• Window Memory: Eficiente pero puede olvidar información importante\")  \n",
    "    print(\"• Summary Memory: Balance entre memoria y eficiencia\")\n",
    "\n",
    "# Ejecutar comparación\n",
    "comparar_memorias()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a818257c",
   "metadata": {},
   "source": [
    "## Chatbot Avanzado con Memoria Personalizable\n",
    "\n",
    "Implementemos un chatbot que permite al usuario elegir el tipo de memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96813f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatbot avanzado con memoria configurable\n",
    "def chatbot_con_memoria():\n",
    "    print(\"=== CHATBOT CON MEMORIA CONFIGURABLE ===\")\n",
    "    print(\"Tipos disponibles:\")\n",
    "    print(\"1. buffer - Mantiene todo el historial\")\n",
    "    print(\"2. window - Mantiene solo N mensajes recientes\") \n",
    "    print(\"3. summary - Resume conversaciones largas\")\n",
    "    print(\"\\\\nEscribe 'cambiar' para cambiar tipo de memoria\")\n",
    "    print(\"Escribe 'memoria' para ver el contenido actual\")\n",
    "    print(\"Escribe 'salir' para terminar\\\\n\")\n",
    "    \n",
    "    # Configuración inicial\n",
    "    memory_type = \"buffer\"\n",
    "    memory = ConversationBufferMemory()\n",
    "    conversation = ConversationChain(llm=llm, memory=memory, verbose=False)\n",
    "    \n",
    "    def crear_memoria(tipo):\n",
    "        if tipo == \"buffer\":\n",
    "            return ConversationBufferMemory()\n",
    "        elif tipo == \"window\":\n",
    "            return ConversationBufferWindowMemory(k=3)\n",
    "        elif tipo == \"summary\":\n",
    "            return ConversationSummaryMemory(llm=llm)\n",
    "        else:\n",
    "            return ConversationBufferMemory()\n",
    "    \n",
    "    print(f\"Memoria actual: {memory_type}\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\\\n🧑 Tú: \")\n",
    "        \n",
    "        if user_input.lower() == 'salir':\n",
    "            print(\"\\\\n👋 ¡Hasta luego!\")\n",
    "            break\n",
    "            \n",
    "        elif user_input.lower() == 'cambiar':\n",
    "            print(\"\\\\nTipos disponibles: buffer, window, summary\")\n",
    "            nuevo_tipo = input(\"Nuevo tipo de memoria: \").lower()\n",
    "            \n",
    "            if nuevo_tipo in ['buffer', 'window', 'summary']:\n",
    "                memory_type = nuevo_tipo\n",
    "                memory = crear_memoria(memory_type)\n",
    "                conversation = ConversationChain(llm=llm, memory=memory, verbose=False)\n",
    "                print(f\"✓ Memoria cambiada a: {memory_type}\")\n",
    "                print(\"⚠️ Historial de conversación reiniciado\")\n",
    "            else:\n",
    "                print(\"❌ Tipo no válido\")\n",
    "            continue\n",
    "            \n",
    "        elif user_input.lower() == 'memoria':\n",
    "            print(f\"\\\\n=== MEMORIA ACTUAL ({memory_type}) ===\")\n",
    "            print(f\"Contenido: {memory.buffer}\")\n",
    "            print(f\"Longitud: {len(memory.buffer)} caracteres\")\n",
    "            continue\n",
    "            \n",
    "        elif not user_input.strip():\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\\\n🤖 Asistente ({memory_type}): \", end=\"\", flush=True)\n",
    "            response = conversation.predict(input=user_input)\n",
    "            print(response)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "\n",
    "# Función para ejecutar el chatbot\n",
    "# chatbot_con_memoria()  # Descomenta para ejecutar\n",
    "\n",
    "print(\"💡 Descomenta la línea anterior para probar el chatbot con memoria configurable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c85cfea",
   "metadata": {},
   "source": [
    "## Consideraciones Técnicas y Mejores Prácticas\n",
    "\n",
    "### Selección del Tipo de Memoria\n",
    "\n",
    "| Tipo | Cuándo Usarlo | Ventajas | Desventajas |\n",
    "|------|---------------|----------|-------------|\n",
    "| **Buffer** | Conversaciones cortas | Contexto completo | Alto consumo de tokens |\n",
    "| **Window** | Contexto reciente importante | Eficiente en tokens | Puede perder información clave |\n",
    "| **Summary** | Conversaciones muy largas | Balance eficiencia/contexto | Pérdida de detalles específicos |\n",
    "\n",
    "### Mejores Prácticas:\n",
    "\n",
    "1. **Gestión de Tokens**:\n",
    "   - Monitorea el uso de tokens regularmente\n",
    "   - Establece límites máximos para evitar costos excesivos\n",
    "   - Considera el costo vs. calidad del contexto\n",
    "\n",
    "2. **Selección Estratégica**:\n",
    "   - Usa Buffer para sesiones cortas e importantes\n",
    "   - Usa Window para conversaciones con contexto limitado\n",
    "   - Usa Summary para sesiones largas de asistencia\n",
    "\n",
    "3. **Optimización**:\n",
    "   - Limpia memoria periódicamente si es necesario\n",
    "   - Implementa estrategias híbridas según el caso de uso\n",
    "   - Considera almacenamiento persistente para memoria a largo plazo\n",
    "\n",
    "## Ejercicios Prácticos\n",
    "\n",
    "### Ejercicio 1: Análisis de Consumo\n",
    "Implementa un sistema que monitoree y reporte el uso de tokens con diferentes tipos de memoria.\n",
    "\n",
    "### Ejercicio 2: Memoria Híbrida\n",
    "Diseña una estrategia que combine multiple tipos de memoria según el contexto.\n",
    "\n",
    "### Ejercicio 3: Persistencia\n",
    "Extiende el chatbot para guardar y cargar memoria entre sesiones.\n",
    "\n",
    "## Conceptos Clave Aprendidos\n",
    "\n",
    "1. **Importancia de la memoria** en conversaciones naturales\n",
    "2. **Tipos de memoria** y sus casos de uso específicos\n",
    "3. **Balance** entre contexto y eficiencia de tokens\n",
    "4. **Implementación práctica** con LangChain\n",
    "5. **Estrategias de optimización** para diferentes escenarios\n",
    "\n",
    "## Conclusión del Módulo IL1.1\n",
    "\n",
    "Has completado la introducción a LLMs y conexiones API. Los conceptos aprendidos:\n",
    "\n",
    "1. **APIs directas** vs **frameworks** como LangChain\n",
    "2. **Streaming** para mejor experiencia de usuario\n",
    "3. **Memoria** para conversaciones contextuales\n",
    "4. **Mejores prácticas** de seguridad y optimización\n",
    "\n",
    "### Próximos Pasos\n",
    "En **IL1.2** exploraremos técnicas avanzadas de **prompt engineering** incluyendo zero-shot, few-shot, y chain-of-thought prompting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
