{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9019b973",
   "metadata": {},
   "source": [
    "## Primera Llamada al Modelo\n",
    "\n",
    "En este ejercicio, aprenderemos a realizar nuestra primera llamada a un modelo de lenguaje usando GitHub Models API.\n",
    "\n",
    "# 1. GitHub Models API - Conexión Directa con OpenAI Client\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Configurar una conexión directa con GitHub Models usando el cliente OpenAI\n",
    "- Comprender los parámetros básicos de configuración de API\n",
    "- Implementar llamadas básicas a modelos de lenguaje\n",
    "- Aplicar mejores prácticas de seguridad con API keys\n",
    "\n",
    "## Introducción\n",
    "GitHub Models proporciona acceso a varios modelos de lenguaje mediante una API compatible con OpenAI. En este notebook aprenderemos a:\n",
    "1. Configurar el entorno y las credenciales\n",
    "2. Establecer una conexión con la API\n",
    "3. Realizar llamadas básicas al modelo\n",
    "4. Explorar diferentes parámetros de configuración\n",
    "\n",
    "## Configuración de Variables de Entorno\n",
    "\n",
    "Antes de ejecutar el código, asegúrate de tener configuradas las siguientes variables de entorno:\n",
    "\n",
    "```bash\n",
    "export GITHUB_BASE_URL=\"https://models.inference.ai.azure.com\"\n",
    "export GITHUB_TOKEN=\"tu_token_de_github_aqui\"\n",
    "```\n",
    "\n",
    "**Mejores Prácticas de Seguridad:**\n",
    "- Nunca hardcodees API keys en el código\n",
    "- Usa variables de entorno o archivos .env\n",
    "- No compartas credenciales en repositorios públicos\n",
    "- Rota las API keys regularmente\n",
    "\n",
    "## Instalación de Dependencias\n",
    "```bash\n",
    "pip install openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab37f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI library version: 1.78.1\n",
      "Python version: 3.11.7 (main, Dec  4 2023, 18:10:11) [Clang 15.0.0 (clang-1500.1.0.2.5)]\n",
      "Base URL configurada: https://models.inference.ai.azure.com\n",
      "API Key configurada: ✓\n",
      "API Key preview: ghp_ZNjZDL...IP7U\n"
     ]
    }
   ],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Verificar que tenemos las bibliotecas correctas\n",
    "print(\"OpenAI library version:\", __import__('openai').__version__)\n",
    "print(\"Python version:\", __import__('sys').version)\n",
    "\n",
    "# Configuración del cliente OpenAI para GitHub Models\n",
    "try:\n",
    "    # Configurar el cliente con variables de entorno\n",
    "    client = OpenAI(\n",
    "        base_url=os.environ.get(\"GITHUB_BASE_URL\"),\n",
    "        api_key=os.environ.get(\"GITHUB_TOKEN\")\n",
    "    )\n",
    "    \n",
    "    # Verificar configuración (sin mostrar la API key completa por seguridad)\n",
    "    print(\"Base URL configurada:\", client.base_url)\n",
    "    print(\"API Key configurada:\", \"✓\" if client.api_key else \"✗\")\n",
    "    \n",
    "    if client.api_key:\n",
    "        print(\"API Key preview:\", client.api_key[:10] + \"...\" + client.api_key[-4:])\n",
    "    else:\n",
    "        print(\"⚠️  API Key no encontrada. Asegúrate de configurar GITHUB_TOKEN\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error en configuración: {e}\")\n",
    "    print(\"Verifica que las variables de entorno estén configuradas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a49b487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Respuesta del Modelo ===\n",
      "¡Hola! Estoy aquí y listo para ayudarte.\n",
      "\n",
      "=== Información Técnica ===\n",
      "Modelo usado: gpt-4o-2024-11-20\n",
      "Tokens usados: 30\n",
      "Tokens de entrada: 19\n",
      "Tokens de salida: 11\n"
     ]
    }
   ],
   "source": [
    "# Primera llamada básica al modelo\n",
    "def llamada_basica():\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": \"Hola, ¿cómo estás? Responde en una oración.\"}\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_tokens=150\n",
    "        )\n",
    "        \n",
    "        print(\"=== Respuesta del Modelo ===\")\n",
    "        print(response.choices[0].message.content)\n",
    "        print(\"\\n=== Información Técnica ===\")\n",
    "        print(f\"Modelo usado: {response.model}\")\n",
    "        print(f\"Tokens usados: {response.usage.total_tokens}\")\n",
    "        print(f\"Tokens de entrada: {response.usage.prompt_tokens}\")\n",
    "        print(f\"Tokens de salida: {response.usage.completion_tokens}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error en la llamada: {e}\")\n",
    "        print(\"Verifica tu configuración y conexión a internet\")\n",
    "\n",
    "# Ejecutar la función\n",
    "llamada_basica()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cccf4a",
   "metadata": {},
   "source": [
    "## Usando Roles del Sistema\n",
    "\n",
    "El rol \"system\" permite establecer el comportamiento y contexto del asistente antes de la conversación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "j1b57r1oeo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Respuesta con Mensaje de Sistema ===\n",
      "¡Claro! Una API, que significa **Interfaz de Programación de Aplicaciones** (por sus siglas en inglés, *Application Programming Interface*), es como un puente que permite que dos sistemas, programas o aplicaciones diferentes se comuniquen entre sí y compartan información o funciones.\n",
      "\n",
      "### Piensa en una API como un **menú en un restaurante**:\n",
      "- Cuando vas a un restaurante, no entras a la cocina para decirle al chef qué ingredientes usar o cómo preparar tu comida. En su lugar, miras el menú (la API del restaurante) y eliges lo que quieres.\n",
      "- El camarero (el intermediario) lleva tu pedido a la cocina, y el chef te devuelve el plato terminado.\n",
      "- En este ejemplo, tú no necesitas saber cómo funciona la cocina por dentro, ni el chef necesita saber quién eres o qué haces. Solo sigues el \"protocolo\" del menú para pedir lo que necesitas.\n",
      "\n",
      "### Ahora, llevémoslo a la\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo con mensaje de sistema\n",
    "def usar_mensaje_sistema():\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"Eres un experto en tecnología que explica conceptos complejos de manera simple y amigable. Siempre incluyes ejemplos prácticos.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": \"¿Qué es una API?\"\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        \n",
    "        print(\"=== Respuesta con Mensaje de Sistema ===\")\n",
    "        print(response.choices[0].message.content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Ejecutar función\n",
    "usar_mensaje_sistema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3a290e",
   "metadata": {},
   "source": [
    "## Explorando Parámetros de Configuración\n",
    "\n",
    "Los parámetros más importantes al hacer llamadas a LLMs son:\n",
    "\n",
    "- **temperature**: Controla la creatividad (0.0 = determinístico, 1.0 = muy creativo)\n",
    "- **max_tokens**: Límite de tokens en la respuesta\n",
    "- **model**: El modelo específico a usar (gpt-4o, gpt-3.5-turbo, etc.)\n",
    "- **messages**: Array de mensajes con roles (system, user, assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "j48bg48xqs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TEMPERATURE: 0.1\n",
      "==================================================\n",
      "En un pequeño taller lleno de herramientas y cables, un robot llamado K1-T0 despertó a la vida. Su creador, un inventor apasionado, le había dado una misión: aprender a cocinar para ayudar en la cocina del vecindario.\n",
      "\n",
      "Al principio, K1-T0 era torpe. Confundía la sal con el azúcar y cortaba las zanahorias en formas extrañas. Pero cada error era una lección. Observaba tutoriales, leía libros de recetas\n",
      "\n",
      "Tokens usados: 121\n",
      "\n",
      "==================================================\n",
      "TEMPERATURE: 0.5\n",
      "==================================================\n",
      "En un pequeño taller de una ciudad futurista, un robot llamado C1-B0, diseñado para tareas de limpieza, encontró un viejo libro de recetas olvidado en una esquina polvorienta. Intrigado por las coloridas imágenes de platos, decidió intentarlo.\n",
      "\n",
      "Al principio, C1-B0 confundió la sal con el azúcar y quemó más de un sartén, pero su curiosidad era inagotable. Día tras día, ajustaba sus algoritmos, perfeccionando sus\n",
      "\n",
      "Tokens usados: 121\n",
      "\n",
      "==================================================\n",
      "TEMPERATURE: 0.9\n",
      "==================================================\n",
      "En un pequeño taller lleno de herramientas y cables sueltos, un robot llamado K-3PO fue activado por primera vez. Su creador, el inventor Arturo, le había dado una tarea sencilla: aprender a cocinar. K-3PO, equipado con sensores de última generación pero sin experiencia culinaria, comenzó su travesía.\n",
      "\n",
      "El robot pasó sus primeras horas estudiando recetas digitales, midiendo ingredientes con precisión milimétrica y observando tutoriales de chefs famosos. Sin embargo,\n",
      "\n",
      "Tokens usados: 121\n"
     ]
    }
   ],
   "source": [
    "# Comparando diferentes valores de temperature\n",
    "def comparar_temperature():\n",
    "    prompt = \"Escribe una historia muy corta sobre un robot que aprende a cocinar.\"\n",
    "    \n",
    "    temperatures = [0.1, 0.5, 0.9]\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"TEMPERATURE: {temp}\")\n",
    "        print('='*50)\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temp,\n",
    "                max_tokens=100\n",
    "            )\n",
    "            \n",
    "            print(response.choices[0].message.content)\n",
    "            print(f\"\\nTokens usados: {response.usage.total_tokens}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "# Ejecutar comparación\n",
    "comparar_temperature()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6fc272",
   "metadata": {},
   "source": [
    "## Ejercicios Prácticos\n",
    "\n",
    "### Ejercicio 1: Experimentar con Diferentes Modelos\n",
    "Modifica el código para probar diferentes modelos disponibles (si tienes acceso):\n",
    "- gpt-4o\n",
    "- gpt-4o-mini\n",
    "- DeepSeek-R1-0528\n",
    "\n",
    "Revisa todos los modelos diponibles en la [documentación de Github Marketplace](https://github.com/marketplace?type=models)\n",
    "\n",
    "### Ejercicio 2: Crear un Asistente Especializado\n",
    "Diseña un mensaje de sistema para crear un asistente especializado en un tema específico (ejemplo: finanzas, salud, educación).\n",
    "\n",
    "### Ejercicio 3: Optimización de Tokens\n",
    "Experimenta con diferentes valores de max_tokens para encontrar el equilibrio entre respuesta completa y eficiencia de costos.\n",
    "\n",
    "## Conceptos Clave\n",
    "\n",
    "1. **Configuración segura** de APIs usando variables de entorno\n",
    "2. **Parámetros básicos** para controlar el comportamiento del modelo\n",
    "3. **Manejo de errores** en llamadas a APIs\n",
    "4. **Roles de mensajes** (system, user, assistant)\n",
    "5. **Monitoreo de uso** de tokens y costos\n",
    "\n",
    "## Próximos Pasos\n",
    "\n",
    "En el siguiente notebook exploraremos cómo LangChain simplifica y abstrae estas operaciones, proporcionando herramientas más poderosas para el desarrollo de aplicaciones con LLMs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
