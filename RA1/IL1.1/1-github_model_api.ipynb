{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9019b973",
   "metadata": {},
   "source": [
    "## Primera Llamada al Modelo\n",
    "\n",
    "En este ejercicio, aprenderemos a realizar nuestra primera llamada a un modelo de lenguaje usando GitHub Models API.\n",
    "\n",
    "# 1. GitHub Models API - Conexión Directa con OpenAI Client\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Configurar una conexión directa con GitHub Models usando el cliente OpenAI\n",
    "- Comprender los parámetros básicos de configuración de API\n",
    "- Implementar llamadas básicas a modelos de lenguaje\n",
    "- Aplicar mejores prácticas de seguridad con API keys\n",
    "\n",
    "## Introducción\n",
    "GitHub Models proporciona acceso a varios modelos de lenguaje mediante una API compatible con OpenAI. En este notebook aprenderemos a:\n",
    "1. Configurar el entorno y las credenciales\n",
    "2. Establecer una conexión con la API\n",
    "3. Realizar llamadas básicas al modelo\n",
    "4. Explorar diferentes parámetros de configuración\n",
    "\n",
    "## Configuración de Variables de Entorno\n",
    "\n",
    "Antes de ejecutar el código, asegúrate de tener configuradas las siguientes variables de entorno:\n",
    "\n",
    "```bash\n",
    "export GITHUB_BASE_URL=\"https://models.inference.ai.azure.com\"\n",
    "export GITHUB_TOKEN=\"tu_token_de_github_aqui\"\n",
    "```\n",
    "\n",
    "**Mejores Prácticas de Seguridad:**\n",
    "- Nunca hardcodees API keys en el código\n",
    "- Usa variables de entorno o archivos .env\n",
    "- No compartas credenciales en repositorios públicos\n",
    "- Rota las API keys regularmente\n",
    "\n",
    "## Instalación de Dependencias\n",
    "```bash\n",
    "pip install openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab37f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI library version: 1.78.1\n",
      "Python version: 3.11.7 (main, Dec  4 2023, 18:10:11) [Clang 15.0.0 (clang-1500.1.0.2.5)]\n",
      "Base URL configurada: https://models.inference.ai.azure.com\n",
      "API Key configurada: ✓\n",
      "API Key preview: ghp_OzdBEe...B8LA\n"
     ]
    }
   ],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Verificar que tenemos las bibliotecas correctas\n",
    "print(\"OpenAI library version:\", __import__('openai').__version__)\n",
    "print(\"Python version:\", __import__('sys').version)\n",
    "\n",
    "# Configuración del cliente OpenAI para GitHub Models\n",
    "try:\n",
    "    # Configurar el cliente con variables de entorno\n",
    "    client = OpenAI(\n",
    "        base_url=os.environ.get(\"GITHUB_BASE_URL\"),\n",
    "        api_key=os.environ.get(\"GITHUB_TOKEN\")\n",
    "    )\n",
    "    \n",
    "    # Verificar configuración (sin mostrar la API key completa por seguridad)\n",
    "    print(\"Base URL configurada:\", client.base_url)\n",
    "    print(\"API Key configurada:\", \"✓\" if client.api_key else \"✗\")\n",
    "    \n",
    "    if client.api_key:\n",
    "        print(\"API Key preview:\", client.api_key[:10] + \"...\" + client.api_key[-4:])\n",
    "    else:\n",
    "        print(\"⚠️  API Key no encontrada. Asegúrate de configurar GITHUB_TOKEN\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error en configuración: {e}\")\n",
    "    print(\"Verifica que las variables de entorno estén configuradas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a49b487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Respuesta del Modelo ===\n",
      "¡Hola! Estoy aquí y lista para ayudarte, ¿en qué puedo asistirte? 😊\n",
      "\n",
      "=== Información Técnica ===\n",
      "Modelo usado: gpt-4o-2024-11-20\n",
      "Tokens usados: 38\n",
      "Tokens de entrada: 19\n",
      "Tokens de salida: 19\n"
     ]
    }
   ],
   "source": [
    "# Primera llamada básica al modelo\n",
    "def llamada_basica():\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": \"Hola, ¿cómo estás? Responde en una oración.\"}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=150\n",
    "        )\n",
    "        \n",
    "        print(\"=== Respuesta del Modelo ===\")\n",
    "        print(response.choices[0].message.content)\n",
    "        print(\"\\n=== Información Técnica ===\")\n",
    "        print(f\"Modelo usado: {response.model}\")\n",
    "        print(f\"Tokens usados: {response.usage.total_tokens}\")\n",
    "        print(f\"Tokens de entrada: {response.usage.prompt_tokens}\")\n",
    "        print(f\"Tokens de salida: {response.usage.completion_tokens}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error en la llamada: {e}\")\n",
    "        print(\"Verifica tu configuración y conexión a internet\")\n",
    "\n",
    "# Ejecutar la función\n",
    "llamada_basica()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cccf4a",
   "metadata": {},
   "source": [
    "## Usando Roles del Sistema\n",
    "\n",
    "El rol \"system\" permite establecer el comportamiento y contexto del asistente antes de la conversación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "j1b57r1oeo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Respuesta con Mensaje de Sistema ===\n",
      "¡Claro! Una API es como un **puente de comunicación** entre dos sistemas o aplicaciones que necesitan \"hablar\" entre sí para compartir datos o realizar acciones.\n",
      "\n",
      "Para explicarlo de una manera más sencilla: imagina que estás en un restaurante. Tú (el cliente) te sientas en la mesa y quieres pedir comida. La cocina (el sistema que prepara la comida) no interactúa directamente contigo. En su lugar, tienes un mesero (la API), que se encarga de llevar tu pedido a la cocina y luego traerte la comida.\n",
      "\n",
      "En el mundo de la tecnología, una **API (Interfaz de Programación de Aplicaciones)** es como ese mesero. Permite que diferentes programas o aplicaciones trabajen juntos, aunque estén construidos de manera diferente. Lo hace proporcionando un conjunto de reglas y herramientas para que puedan comunicarse.\n",
      "\n",
      "---\n",
      "\n",
      "### Ejemplo práctico\n",
      "Pensemos en una **aplicación de clima** en tu teléfono. Esa app no tiene\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo con mensaje de sistema\n",
    "def usar_mensaje_sistema():\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"Eres un experto en tecnología que explica conceptos complejos de manera simple y amigable. Siempre incluyes ejemplos prácticos.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": \"¿Qué es una API?\"\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        \n",
    "        print(\"=== Respuesta con Mensaje de Sistema ===\")\n",
    "        print(response.choices[0].message.content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Ejecutar función\n",
    "usar_mensaje_sistema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3a290e",
   "metadata": {},
   "source": [
    "## Explorando Parámetros de Configuración\n",
    "\n",
    "Los parámetros más importantes al hacer llamadas a LLMs son:\n",
    "\n",
    "- **temperature**: Controla la creatividad (0.0 = determinístico, 1.0 = muy creativo)\n",
    "- **max_tokens**: Límite de tokens en la respuesta\n",
    "- **model**: El modelo específico a usar (gpt-4o, gpt-3.5-turbo, etc.)\n",
    "- **messages**: Array de mensajes con roles (system, user, assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "j48bg48xqs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TEMPERATURE: 0.1\n",
      "==================================================\n",
      "En un pequeño taller lleno de herramientas y chispas, un robot llamado C1-B0, diseñado para tareas domésticas básicas, descubrió un viejo libro de recetas olvidado en un rincón polvoriento. Intrigado por las imágenes coloridas de pasteles y guisos, decidió intentarlo.\n",
      "\n",
      "Al principio, sus movimientos eran torpes. Derramó harina por todo el suelo y confundió el azúcar con la sal. Pero C1-B0 tenía algo especial: un módulo\n",
      "\n",
      "Tokens usados: 121\n",
      "\n",
      "==================================================\n",
      "TEMPERATURE: 0.5\n",
      "==================================================\n",
      "En un pequeño taller lleno de herramientas y chispas, un robot llamado C1-B0 despertó a la vida. Su creador, un chef jubilado llamado Don Emilio, lo había diseñado para ayudar en la cocina, pero había olvidado un detalle crucial: C1-B0 no sabía nada de cocinar.\n",
      "\n",
      "La primera vez que intentó hacer una sopa, el robot confundió sal con azúcar. El resultado fue tan desastroso que incluso el gato de Don Emilio se negó a probar\n",
      "\n",
      "Tokens usados: 121\n",
      "\n",
      "==================================================\n",
      "TEMPERATURE: 0.9\n",
      "==================================================\n",
      "En un pequeño taller lleno de herramientas y cables, vivía un robot llamado P-42. Su creador, el señor Méndez, le había construido para tareas simples del hogar. Pero un día, mientras ordenaba una estantería, P-42 encontró un viejo libro de cocina con páginas amarillentas y dibujos de platillos que parecían obras de arte.\n",
      "\n",
      "Curioso, P-42 escaneó las recetas. Decidió probar. La primera creación fue, por decir\n",
      "\n",
      "Tokens usados: 121\n"
     ]
    }
   ],
   "source": [
    "# Comparando diferentes valores de temperature\n",
    "def comparar_temperature():\n",
    "    prompt = \"Escribe una historia muy corta sobre un robot que aprende a cocinar.\"\n",
    "    \n",
    "    temperatures = [0.1, 0.5, 0.9]\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"TEMPERATURE: {temp}\")\n",
    "        print('='*50)\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temp,\n",
    "                max_tokens=100\n",
    "            )\n",
    "            \n",
    "            print(response.choices[0].message.content)\n",
    "            print(f\"\\nTokens usados: {response.usage.total_tokens}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "# Ejecutar comparación\n",
    "comparar_temperature()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6fc272",
   "metadata": {},
   "source": [
    "## Ejercicios Prácticos\n",
    "\n",
    "### Ejercicio 1: Experimentar con Diferentes Modelos\n",
    "Modifica el código para probar diferentes modelos disponibles (si tienes acceso):\n",
    "- gpt-4o\n",
    "- gpt-4o-mini\n",
    "- DeepSeek-R1-0528\n",
    "\n",
    "Revisa todos los modelos diponibles en la [documentación de Github Marketplace](https://github.com/marketplace?type=models)\n",
    "\n",
    "### Ejercicio 2: Crear un Asistente Especializado\n",
    "Diseña un mensaje de sistema para crear un asistente especializado en un tema específico (ejemplo: finanzas, salud, educación).\n",
    "\n",
    "### Ejercicio 3: Optimización de Tokens\n",
    "Experimenta con diferentes valores de max_tokens para encontrar el equilibrio entre respuesta completa y eficiencia de costos.\n",
    "\n",
    "## Conceptos Clave\n",
    "\n",
    "1. **Configuración segura** de APIs usando variables de entorno\n",
    "2. **Parámetros básicos** para controlar el comportamiento del modelo\n",
    "3. **Manejo de errores** en llamadas a APIs\n",
    "4. **Roles de mensajes** (system, user, assistant)\n",
    "5. **Monitoreo de uso** de tokens y costos\n",
    "\n",
    "## Próximos Pasos\n",
    "\n",
    "En el siguiente notebook exploraremos cómo LangChain simplifica y abstrae estas operaciones, proporcionando herramientas más poderosas para el desarrollo de aplicaciones con LLMs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
